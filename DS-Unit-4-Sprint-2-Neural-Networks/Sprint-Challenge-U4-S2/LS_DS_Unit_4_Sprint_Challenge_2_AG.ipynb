{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "LS_DS_Unit_4_Sprint_Challenge_2_AG.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M9B41y6d7mY"
      },
      "source": [
        "\n",
        "## Autograded Notebook (Canvas & CodeGrade)\n",
        "\n",
        "This notebook will be automatically graded. It is designed to test your answers and award points for the correct answers. Following the instructions for each Task carefully.\n",
        "Instructions\n",
        "\n",
        "- **Download** this notebook as you would any other ipynb file \n",
        "- **Upload** to Google Colab or work locally (if you have that set-up)\n",
        "- **Delete** `raise NotImplementedError()`\n",
        "\n",
        "- **Write** your code in the `# YOUR CODE HERE` space\n",
        "\n",
        "\n",
        "- **Execute** the Test cells that contain assert statements - these help you check your work (others contain hidden tests that will be checked when you submit through Canvas)\n",
        "\n",
        "- **Save** your notebook when you are finished\n",
        "- **Download** as a ipynb file (if working in Colab)\n",
        "- **Upload** your complete notebook to Canvas (there will be additional instructions in Slack and/or Canvas)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJo74pmed7md"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2*\n",
        "\n",
        "# Sprint Challenge - Neural Network Foundations\n",
        "\n",
        "Table of Problems\n",
        "\n",
        "1. [Defining Neural Networks](#Q1)\n",
        "2. [Simple Perceptron](#Q2)\n",
        "    - Perceptron\n",
        "    - Multilayer Perceptron\n",
        "    - Analyze and Compare\n",
        "4. [Keras MMP](#Q3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a2d017ba3200be3890c0b67eda283c48",
          "grade": false,
          "grade_id": "cell-621a8b86bacf295a",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": true
        },
        "id": "HVlR_3VRd7me"
      },
      "source": [
        "<a id=\"Q1\"></a>\n",
        "## 1. Defining Neural Networks \n",
        "\n",
        "Write *your own* definitions for the following terms:\n",
        "\n",
        "- **Neuron:**\n",
        "\n",
        "- **Input Layer:** \n",
        "\n",
        "- **Hidden Layer:** \n",
        "\n",
        "- **Output Layer:**\n",
        "\n",
        "- **Activation:** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "7233c31461609b21a7fc2651afb12632",
          "grade": true,
          "grade_id": "cell-6adae65226f09553",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "aOIEq0uZd7me"
      },
      "source": [
        "**Neuron:**\r\n",
        "- The \"units entered for each layer within the neural network. Each neuron takes a group of weights and uses those weights to get a weighted sum of the inputs plus the bias. Then runs that value through an activation function and returns an output between 0 and 1 depending on the activation function used.\r\n",
        "\r\n",
        "**Input Layer:**\r\n",
        "- The initial data for training the neural network gets passed into this layer. This layer is assigned a random weight to start the process of updating the weights since it has no other initial weight to update.\r\n",
        "\r\n",
        "**Hidden Layer:**\r\n",
        "- There can be multiple hidden layers within a neural network. These layers are where the calulations are done and where the weights continue to get updated based on the prior layer. All of these layers wull be inbetween the input and output layers.\r\n",
        "\r\n",
        "**Output Layer:**\r\n",
        "- This is always the final layer in the neural network. This is the layer that will produce the results for the given inputs.\r\n",
        "\r\n",
        "**Activation:**\r\n",
        "- A function that works like a filter. It will take a group of weights for the input data and \"filter\" the data based on the weight average from the prior neurons to pull out and keep only the useful information that is best for making the prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "bb49a3d32ff92032e5bc641764d86ca4",
          "grade": false,
          "grade_id": "cell-d64f1de9e9458dc7",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": true
        },
        "id": "GypBpUoOd7mf"
      },
      "source": [
        "Explain how back propagation works as if you were explaining it to a five year-old. Use your own words, but feel free to reference external materials for this question. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "ceb3f64a4b1b18346decf75c8f5567d2",
          "grade": true,
          "grade_id": "cell-cef20b23d4e0b056",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "YZRKPg2pd7mf"
      },
      "source": [
        "- Back propagation takes a group of numbers and uses those numbers to predict, or tell what something might or might not be, the answer to a problem or question, using the data or information we already have available to us. This is a popular way to train a neural network because of the way it works to return an accurate prediction is a short amount of time. The way that is works is that it first runs through all the layers in the network, kind of like going through the layers of an onion, one at time. Then it will go backwards through the network while adjusting the parameters (weights and biases) based on the new information it learned when it peeled the onion the first time, kind of like putting the layers back on the onion with glue to strength the layers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f57b11b197b768f69957260035113f43",
          "grade": false,
          "grade_id": "cell-e013d19857352d79",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": true
        },
        "id": "bVFhEDvCd7mf"
      },
      "source": [
        "Remember our Simple Perceptron Class from Monday. In a simple prediction describe the process of making a prediction. How do you go from inputs to predicted output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d746de6391012340f8548821850a621c",
          "grade": true,
          "grade_id": "cell-53c7cc36db9d7983",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "DmrI1Xrid7mf"
      },
      "source": [
        "- A simple one layer perceptron is used for a simple classification problem where a linear descision with be a good enough result. Being that there is only one layer it can only learn linearly with the help of gradient descent to find the minimal convergence point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkvSFHhtd7mg"
      },
      "source": [
        "<a id=\"Q2\"></a>\n",
        "## 2. Simple Perceptron\n",
        "\n",
        "In this question, you will build two neural networks using Tensorflow Keras. After you build these two models, compare the results of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bw_0whMud7mg",
        "outputId": "91b6d1c4-83ec-456c-e393-0fbf0c82c85a"
      },
      "source": [
        "\"\"\"\n",
        "Our Dataset\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "rng = np.random.RandomState(0)\n",
        "\n",
        "\"Use this X & y in the following 2 models\"\n",
        "X = rng.randn(300, 2)\n",
        "y = np.array(np.logical_xor(X[:, 0] > 0, X[:, 1] > 0), \n",
        "             dtype=int)\n",
        "\n",
        "# Look at the shape of data I am using\n",
        "X.shape, y.shape"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((300, 2), (300,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WSkLfpld7mh"
      },
      "source": [
        "### Simple Perceptron\n",
        "Construct a simple perceptron using Keras. \n",
        "\n",
        "You model should have `1 dense layer` with a `single neuron` and a `sigmoid activation function`. \n",
        "\n",
        "\n",
        "Your model should be called `model1` and make sure to save the results of your fit statement to a variable called `h1`. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ed5db44c2ae01d03f766beeeca1a7f0a",
          "grade": false,
          "grade_id": "cell-427690628f9c900b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwXtpCiXd7mh",
        "outputId": "c8afb619-30ae-4b23-b58f-93c7f3098536"
      },
      "source": [
        "#import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# build and fit model\n",
        "\n",
        "# Instantiate my simple perceptron\n",
        "model1 = Sequential([Dense(1, input_dim=2, activation='relu')])\n",
        "\n",
        "# Look at the summary of my simple perceptron\n",
        "model1.summary()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 1)                 3         \n",
            "=================================================================\n",
            "Total params: 3\n",
            "Trainable params: 3\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iP6tiDJKetZ0",
        "outputId": "f6c73e8a-3c41-4756-802b-67cd50d67fae"
      },
      "source": [
        "# Compile my simple perceptron\r\n",
        "model1.compile(loss='binary_crossentropy',\r\n",
        "               optimizer='sgd',\r\n",
        "               metrics=['accuracy'])\r\n",
        "\r\n",
        "# Fit my simple perceptron on X and y with \r\n",
        "#   only 9 epochs and validate on 10% of the data\r\n",
        "h1 = model1.fit(X, y, epochs=9, validation_split=0.1)\r\n",
        "\r\n",
        "# Get the scores for my simple perceptron\r\n",
        "score1 = model1.evaluate(X, y)\r\n",
        "\r\n",
        "# Print my scores\r\n",
        "print(f'Accuracy Score: {(score1[1] * 100):.2f}%')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/9\n",
            "9/9 [==============================] - 1s 56ms/step - loss: 4.5702 - accuracy: 0.4854 - val_loss: 8.1974 - val_accuracy: 0.4667\n",
            "Epoch 2/9\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 8.7878 - accuracy: 0.4147 - val_loss: 8.1974 - val_accuracy: 0.4667\n",
            "Epoch 3/9\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 8.2542 - accuracy: 0.4571 - val_loss: 8.1974 - val_accuracy: 0.4667\n",
            "Epoch 4/9\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 8.3057 - accuracy: 0.4453 - val_loss: 8.1974 - val_accuracy: 0.4667\n",
            "Epoch 5/9\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 8.9238 - accuracy: 0.4120 - val_loss: 8.1974 - val_accuracy: 0.4667\n",
            "Epoch 6/9\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 8.6428 - accuracy: 0.4343 - val_loss: 8.1974 - val_accuracy: 0.4667\n",
            "Epoch 7/9\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 9.1343 - accuracy: 0.3966 - val_loss: 8.1974 - val_accuracy: 0.4667\n",
            "Epoch 8/9\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 8.8564 - accuracy: 0.4201 - val_loss: 8.1974 - val_accuracy: 0.4667\n",
            "Epoch 9/9\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 8.8565 - accuracy: 0.4167 - val_loss: 8.1974 - val_accuracy: 0.4667\n",
            "10/10 [==============================] - 0s 1ms/step - loss: 8.7318 - accuracy: 0.4233\n",
            "Accuracy Score: 42.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "36f7f830036d0443ca8e8ba0f17b2a4e",
          "grade": true,
          "grade_id": "cell-bf2ae566afacde8c",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "EApr93oid7mh"
      },
      "source": [
        "# Visible test\n",
        "assert len(model1.get_config()[\"layers\"]) == 2, \"Make sure you only create 1 Dense layer.\"\n",
        "assert len(h1.epoch) <=10, \"Did you make sure to set epochs to 10 or less?\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "95d3ee2935a0de64f2a5a22460520e69",
          "grade": true,
          "grade_id": "cell-a957e14380b2f508",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "w1gfyoL-d7mh"
      },
      "source": [
        "# Hidden tests - you will see the results when you submit to Canvas"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pX_DmDzbd7mh"
      },
      "source": [
        "### Multi-Layer Perceptron\n",
        "Now construct a multi-layer perceptron using. Here are some architecture suggestions: \n",
        "- `2` Hidden Layers\n",
        "- `5-32` Neurons in the Hidden Layers\n",
        "- Your pick of activation function and optimizer\n",
        "- Incorporate the `Callback function` below into your model\n",
        "- Set epochs to `100`\n",
        "\n",
        "Your model should be called `model2` and make sure to save the results of your fit statement to a variable called `h2`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_7uy-hRd7mi"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback): \n",
        "    def on_epoch_end(self, epoch, logs={}): \n",
        "        if(logs.get('accuracy') > .99999):   \n",
        "            self.model.stop_training = True"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "314337f29c8cd7f38224a31687a86b12",
          "grade": false,
          "grade_id": "cell-77523c4c64743f16",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktYTKfpcd7mi",
        "outputId": "d087c8e8-6dc3-44f1-e087-7f2d905236ba"
      },
      "source": [
        "# build and fit model\n",
        "\n",
        "# Instantiate my multi-layer perceptron with 2 hidden layers, with 32 or less neurons\n",
        "model2 = Sequential([\n",
        "            Dense(32, input_dim=2, activation='relu'),\n",
        "            Dense(24, activation='relu'),\n",
        "            Dense(16, activation='relu'),\n",
        "            Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile my multi-layer perceptron\n",
        "model2.compile(loss='binary_crossentropy',\n",
        "               optimizer='sgd',\n",
        "               metrics=['accuracy'])\n",
        "\n",
        "# Look at the summary of my multi-layer perceptron\n",
        "model2.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 32)                96        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 24)                792       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 16)                400       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 1,305\n",
            "Trainable params: 1,305\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vK_vMzPiace",
        "outputId": "a49014d0-9dae-4612-cc62-ad0717f14fea"
      },
      "source": [
        "# Look at the config of my model to find out why it is failing \r\n",
        "#   the visable test below when it should not be failing.\r\n",
        "model2.get_config()\r\n",
        "\r\n",
        "############### POTENTIAL BUG IN TEST ############### PLEASE READ ###############\r\n",
        "# According to the results here, for some reason there is an additional \r\n",
        "#   \"hidden\" input layer prior to my input layer which is throwing the indexing\r\n",
        "#   off by 1 for the test. I changed the test to compensate for this but I am \r\n",
        "#   not sure how this is going to affect the hidden test when I run my sprint \r\n",
        "#   challenge through the autograder. If this part fails the hidden test because\r\n",
        "#   of this bug, please correct this error for me."
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'layers': [{'class_name': 'InputLayer',\n",
              "   'config': {'batch_input_shape': (None, 2),\n",
              "    'dtype': 'float32',\n",
              "    'name': 'dense_1_input',\n",
              "    'ragged': False,\n",
              "    'sparse': False}},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'activation': 'relu',\n",
              "    'activity_regularizer': None,\n",
              "    'batch_input_shape': (None, 2),\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'dtype': 'float32',\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "     'config': {'seed': None}},\n",
              "    'kernel_regularizer': None,\n",
              "    'name': 'dense_1',\n",
              "    'trainable': True,\n",
              "    'units': 32,\n",
              "    'use_bias': True}},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'activation': 'relu',\n",
              "    'activity_regularizer': None,\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'dtype': 'float32',\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "     'config': {'seed': None}},\n",
              "    'kernel_regularizer': None,\n",
              "    'name': 'dense_2',\n",
              "    'trainable': True,\n",
              "    'units': 24,\n",
              "    'use_bias': True}},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'activation': 'relu',\n",
              "    'activity_regularizer': None,\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'dtype': 'float32',\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "     'config': {'seed': None}},\n",
              "    'kernel_regularizer': None,\n",
              "    'name': 'dense_3',\n",
              "    'trainable': True,\n",
              "    'units': 16,\n",
              "    'use_bias': True}},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'activation': 'sigmoid',\n",
              "    'activity_regularizer': None,\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'dtype': 'float32',\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "     'config': {'seed': None}},\n",
              "    'kernel_regularizer': None,\n",
              "    'name': 'dense_4',\n",
              "    'trainable': True,\n",
              "    'units': 1,\n",
              "    'use_bias': True}}],\n",
              " 'name': 'sequential_1'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLOhGaVZkKX3",
        "outputId": "4477ee81-b102-4bb8-8a7a-eca3b1ebf7cc"
      },
      "source": [
        "# Fit my multi-layer perceptron\r\n",
        "h2 = model2.fit(X, y, \r\n",
        "                epochs=100,\r\n",
        "                validation_split=0.1,\r\n",
        "                callbacks=[myCallback()])\r\n",
        "\r\n",
        "# Get the scores for my simple perceptron\r\n",
        "score1 = model1.evaluate(X, y)\r\n",
        "\r\n",
        "# Print my scores\r\n",
        "print(f'Accuracy Score: {(score1[1] * 100):.2f}%')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "9/9 [==============================] - 1s 24ms/step - loss: 0.7395 - accuracy: 0.4486 - val_loss: 0.6770 - val_accuracy: 0.5000\n",
            "Epoch 2/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.7254 - accuracy: 0.4551 - val_loss: 0.6734 - val_accuracy: 0.5333\n",
            "Epoch 3/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.7160 - accuracy: 0.4921 - val_loss: 0.6704 - val_accuracy: 0.5333\n",
            "Epoch 4/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.7062 - accuracy: 0.4826 - val_loss: 0.6678 - val_accuracy: 0.5667\n",
            "Epoch 5/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.6893 - accuracy: 0.5587 - val_loss: 0.6655 - val_accuracy: 0.6667\n",
            "Epoch 6/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.6940 - accuracy: 0.5947 - val_loss: 0.6633 - val_accuracy: 0.6667\n",
            "Epoch 7/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.6811 - accuracy: 0.5642 - val_loss: 0.6609 - val_accuracy: 0.7333\n",
            "Epoch 8/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.6818 - accuracy: 0.6371 - val_loss: 0.6585 - val_accuracy: 0.7333\n",
            "Epoch 9/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.6809 - accuracy: 0.6055 - val_loss: 0.6558 - val_accuracy: 0.7667\n",
            "Epoch 10/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.6731 - accuracy: 0.6297 - val_loss: 0.6535 - val_accuracy: 0.8000\n",
            "Epoch 11/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.6628 - accuracy: 0.6970 - val_loss: 0.6517 - val_accuracy: 0.8667\n",
            "Epoch 12/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.6625 - accuracy: 0.6558 - val_loss: 0.6498 - val_accuracy: 0.8667\n",
            "Epoch 13/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.6483 - accuracy: 0.7000 - val_loss: 0.6479 - val_accuracy: 0.8667\n",
            "Epoch 14/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.6518 - accuracy: 0.6990 - val_loss: 0.6461 - val_accuracy: 0.8667\n",
            "Epoch 15/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.6468 - accuracy: 0.7230 - val_loss: 0.6442 - val_accuracy: 0.8667\n",
            "Epoch 16/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.6409 - accuracy: 0.7356 - val_loss: 0.6425 - val_accuracy: 0.8333\n",
            "Epoch 17/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.6378 - accuracy: 0.7650 - val_loss: 0.6406 - val_accuracy: 0.8000\n",
            "Epoch 18/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.6433 - accuracy: 0.7359 - val_loss: 0.6387 - val_accuracy: 0.7667\n",
            "Epoch 19/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.6293 - accuracy: 0.7548 - val_loss: 0.6370 - val_accuracy: 0.8000\n",
            "Epoch 20/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.6297 - accuracy: 0.7716 - val_loss: 0.6355 - val_accuracy: 0.8000\n",
            "Epoch 21/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.6314 - accuracy: 0.7582 - val_loss: 0.6336 - val_accuracy: 0.7667\n",
            "Epoch 22/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.6167 - accuracy: 0.7994 - val_loss: 0.6320 - val_accuracy: 0.7667\n",
            "Epoch 23/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.6264 - accuracy: 0.7782 - val_loss: 0.6300 - val_accuracy: 0.7667\n",
            "Epoch 24/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.6198 - accuracy: 0.7923 - val_loss: 0.6281 - val_accuracy: 0.7667\n",
            "Epoch 25/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.6084 - accuracy: 0.8198 - val_loss: 0.6263 - val_accuracy: 0.7333\n",
            "Epoch 26/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.6138 - accuracy: 0.7732 - val_loss: 0.6246 - val_accuracy: 0.7333\n",
            "Epoch 27/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.6134 - accuracy: 0.8119 - val_loss: 0.6227 - val_accuracy: 0.7333\n",
            "Epoch 28/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.6054 - accuracy: 0.7831 - val_loss: 0.6209 - val_accuracy: 0.7333\n",
            "Epoch 29/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.6004 - accuracy: 0.7895 - val_loss: 0.6188 - val_accuracy: 0.7333\n",
            "Epoch 30/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.6031 - accuracy: 0.8181 - val_loss: 0.6169 - val_accuracy: 0.7333\n",
            "Epoch 31/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.5950 - accuracy: 0.8079 - val_loss: 0.6147 - val_accuracy: 0.7333\n",
            "Epoch 32/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.6021 - accuracy: 0.8032 - val_loss: 0.6128 - val_accuracy: 0.7667\n",
            "Epoch 33/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.5933 - accuracy: 0.8109 - val_loss: 0.6105 - val_accuracy: 0.7333\n",
            "Epoch 34/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5835 - accuracy: 0.8370 - val_loss: 0.6081 - val_accuracy: 0.7000\n",
            "Epoch 35/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.5817 - accuracy: 0.8378 - val_loss: 0.6059 - val_accuracy: 0.7000\n",
            "Epoch 36/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.5835 - accuracy: 0.8208 - val_loss: 0.6036 - val_accuracy: 0.7333\n",
            "Epoch 37/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.5798 - accuracy: 0.8286 - val_loss: 0.6013 - val_accuracy: 0.7333\n",
            "Epoch 38/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5788 - accuracy: 0.8390 - val_loss: 0.5992 - val_accuracy: 0.7333\n",
            "Epoch 39/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5762 - accuracy: 0.8244 - val_loss: 0.5967 - val_accuracy: 0.7333\n",
            "Epoch 40/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5723 - accuracy: 0.8201 - val_loss: 0.5945 - val_accuracy: 0.7333\n",
            "Epoch 41/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5745 - accuracy: 0.7883 - val_loss: 0.5922 - val_accuracy: 0.7333\n",
            "Epoch 42/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5602 - accuracy: 0.8374 - val_loss: 0.5896 - val_accuracy: 0.7333\n",
            "Epoch 43/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5598 - accuracy: 0.8413 - val_loss: 0.5872 - val_accuracy: 0.7333\n",
            "Epoch 44/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.5693 - accuracy: 0.8001 - val_loss: 0.5848 - val_accuracy: 0.7333\n",
            "Epoch 45/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5434 - accuracy: 0.8660 - val_loss: 0.5823 - val_accuracy: 0.7333\n",
            "Epoch 46/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.5506 - accuracy: 0.8099 - val_loss: 0.5798 - val_accuracy: 0.7333\n",
            "Epoch 47/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5596 - accuracy: 0.8057 - val_loss: 0.5773 - val_accuracy: 0.7333\n",
            "Epoch 48/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5376 - accuracy: 0.8370 - val_loss: 0.5745 - val_accuracy: 0.7333\n",
            "Epoch 49/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5484 - accuracy: 0.8176 - val_loss: 0.5720 - val_accuracy: 0.7667\n",
            "Epoch 50/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.5370 - accuracy: 0.8354 - val_loss: 0.5694 - val_accuracy: 0.7667\n",
            "Epoch 51/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5346 - accuracy: 0.8275 - val_loss: 0.5666 - val_accuracy: 0.7667\n",
            "Epoch 52/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.5224 - accuracy: 0.8327 - val_loss: 0.5638 - val_accuracy: 0.7667\n",
            "Epoch 53/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5164 - accuracy: 0.8328 - val_loss: 0.5609 - val_accuracy: 0.7667\n",
            "Epoch 54/100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.5313 - accuracy: 0.8237 - val_loss: 0.5579 - val_accuracy: 0.7667\n",
            "Epoch 55/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5221 - accuracy: 0.8099 - val_loss: 0.5550 - val_accuracy: 0.7667\n",
            "Epoch 56/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.5318 - accuracy: 0.8112 - val_loss: 0.5517 - val_accuracy: 0.7667\n",
            "Epoch 57/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5025 - accuracy: 0.8728 - val_loss: 0.5482 - val_accuracy: 0.7667\n",
            "Epoch 58/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.5221 - accuracy: 0.8439 - val_loss: 0.5451 - val_accuracy: 0.7667\n",
            "Epoch 59/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4953 - accuracy: 0.8788 - val_loss: 0.5418 - val_accuracy: 0.7667\n",
            "Epoch 60/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5061 - accuracy: 0.8612 - val_loss: 0.5388 - val_accuracy: 0.7667\n",
            "Epoch 61/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4933 - accuracy: 0.8696 - val_loss: 0.5352 - val_accuracy: 0.8000\n",
            "Epoch 62/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.5018 - accuracy: 0.8626 - val_loss: 0.5317 - val_accuracy: 0.8000\n",
            "Epoch 63/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5001 - accuracy: 0.8666 - val_loss: 0.5282 - val_accuracy: 0.8000\n",
            "Epoch 64/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.4885 - accuracy: 0.8659 - val_loss: 0.5246 - val_accuracy: 0.8000\n",
            "Epoch 65/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4888 - accuracy: 0.8372 - val_loss: 0.5215 - val_accuracy: 0.8000\n",
            "Epoch 66/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.4960 - accuracy: 0.8489 - val_loss: 0.5178 - val_accuracy: 0.8000\n",
            "Epoch 67/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4727 - accuracy: 0.8738 - val_loss: 0.5133 - val_accuracy: 0.8333\n",
            "Epoch 68/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4851 - accuracy: 0.8584 - val_loss: 0.5093 - val_accuracy: 0.8333\n",
            "Epoch 69/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4663 - accuracy: 0.8923 - val_loss: 0.5048 - val_accuracy: 0.8333\n",
            "Epoch 70/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4685 - accuracy: 0.8662 - val_loss: 0.5009 - val_accuracy: 0.8333\n",
            "Epoch 71/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4563 - accuracy: 0.8864 - val_loss: 0.4968 - val_accuracy: 0.8333\n",
            "Epoch 72/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.4522 - accuracy: 0.8842 - val_loss: 0.4927 - val_accuracy: 0.8333\n",
            "Epoch 73/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4613 - accuracy: 0.9046 - val_loss: 0.4886 - val_accuracy: 0.8333\n",
            "Epoch 74/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.4516 - accuracy: 0.8857 - val_loss: 0.4840 - val_accuracy: 0.8333\n",
            "Epoch 75/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4512 - accuracy: 0.8873 - val_loss: 0.4793 - val_accuracy: 0.8333\n",
            "Epoch 76/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.4459 - accuracy: 0.9076 - val_loss: 0.4747 - val_accuracy: 0.8333\n",
            "Epoch 77/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4304 - accuracy: 0.9102 - val_loss: 0.4698 - val_accuracy: 0.8333\n",
            "Epoch 78/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.4384 - accuracy: 0.8986 - val_loss: 0.4657 - val_accuracy: 0.8333\n",
            "Epoch 79/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4209 - accuracy: 0.9234 - val_loss: 0.4610 - val_accuracy: 0.8333\n",
            "Epoch 80/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4189 - accuracy: 0.9167 - val_loss: 0.4566 - val_accuracy: 0.8333\n",
            "Epoch 81/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4181 - accuracy: 0.9222 - val_loss: 0.4521 - val_accuracy: 0.8667\n",
            "Epoch 82/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3988 - accuracy: 0.9238 - val_loss: 0.4479 - val_accuracy: 0.8667\n",
            "Epoch 83/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4131 - accuracy: 0.9058 - val_loss: 0.4430 - val_accuracy: 0.9000\n",
            "Epoch 84/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4073 - accuracy: 0.9329 - val_loss: 0.4385 - val_accuracy: 0.9000\n",
            "Epoch 85/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.3969 - accuracy: 0.9221 - val_loss: 0.4334 - val_accuracy: 0.9000\n",
            "Epoch 86/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4063 - accuracy: 0.9350 - val_loss: 0.4289 - val_accuracy: 0.9000\n",
            "Epoch 87/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3926 - accuracy: 0.9448 - val_loss: 0.4239 - val_accuracy: 0.9333\n",
            "Epoch 88/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.3816 - accuracy: 0.9618 - val_loss: 0.4196 - val_accuracy: 0.9000\n",
            "Epoch 89/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3773 - accuracy: 0.9612 - val_loss: 0.4150 - val_accuracy: 0.9000\n",
            "Epoch 90/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.3854 - accuracy: 0.9606 - val_loss: 0.4105 - val_accuracy: 0.9333\n",
            "Epoch 91/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3752 - accuracy: 0.9592 - val_loss: 0.4056 - val_accuracy: 0.9333\n",
            "Epoch 92/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.3676 - accuracy: 0.9405 - val_loss: 0.4011 - val_accuracy: 0.9333\n",
            "Epoch 93/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3652 - accuracy: 0.9619 - val_loss: 0.3963 - val_accuracy: 0.9333\n",
            "Epoch 94/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.3451 - accuracy: 0.9684 - val_loss: 0.3917 - val_accuracy: 0.9333\n",
            "Epoch 95/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.3634 - accuracy: 0.9644 - val_loss: 0.3868 - val_accuracy: 0.9333\n",
            "Epoch 96/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3429 - accuracy: 0.9708 - val_loss: 0.3825 - val_accuracy: 0.9333\n",
            "Epoch 97/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3475 - accuracy: 0.9660 - val_loss: 0.3779 - val_accuracy: 0.9333\n",
            "Epoch 98/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3394 - accuracy: 0.9721 - val_loss: 0.3735 - val_accuracy: 0.9333\n",
            "Epoch 99/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3232 - accuracy: 0.9793 - val_loss: 0.3692 - val_accuracy: 0.9333\n",
            "Epoch 100/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.3314 - accuracy: 0.9741 - val_loss: 0.3649 - val_accuracy: 0.9333\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.7318 - accuracy: 0.4233\n",
            "Accuracy Score: 42.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "612b9128f3a57d18c3019b2a12ace3c8",
          "grade": true,
          "grade_id": "cell-770612ca24334d8a",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "9Yl8FTRhd7mi"
      },
      "source": [
        "# Visible test\n",
        "assert len(model2.get_config()[\"layers\"]) == 5, \"You should have 4 layers: Input, hidden 1, hidden 2, output.\"\n",
        "assert 5 <= model2.get_config()[\"layers\"][2][\"config\"][\"units\"] <= 32, \"You should have 5 - 32 units in hidden layer 1, but don't.\"\n",
        "assert 5 <= model2.get_config()[\"layers\"][3][\"config\"][\"units\"] <= 32, \"You should have 5 - 32 units in hidden layer 2, but don't.\"\n",
        "assert h2.params[\"epochs\"] == 100, \"You didn't set epochs to 100.\"\n",
        "\n",
        "# Hidden tests - you will see the results when you submit to Canvas"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xkxn-YJwd7mi"
      },
      "source": [
        "### Analyze and Compare\n",
        "\n",
        "**Before you Start**: You will need to install an additional library for this next segment. Install the package `mlxtend` into the environment you are using for the sprint challenge.\n",
        "\n",
        "\n",
        "The cells below generate decision boundary plots of your models (`model1` & `model2`). Review the plots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "UivoUu3_d7mi",
        "outputId": "38f17411-78b9-4985-abf0-0eaa8ab19511"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "\n",
        "for clf, hist, name, grd in zip([model1,model2], [h1, h2],['Perceptron', 'Multi-Layer Perceptron'],[1,2]):\n",
        "\n",
        "    ax = plt.subplot(1,2, grd)\n",
        "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
        "    title = f\"{name} with {hist.history['accuracy'][-1]:,.2f} Accuracy\"\n",
        "    plt.title(title)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/mlxtend/plotting/decision_regions.py:244: MatplotlibDeprecationWarning: Passing unsupported keyword arguments to axis() will raise a TypeError in 3.3.\n",
            "  ax.axis(xmin=xx.min(), xmax=xx.max(), y_min=yy.min(), y_max=yy.max())\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "dfefeea12a0993da5cf2b70ee858f890",
          "grade": false,
          "grade_id": "cell-b1bde9222e35b3fc",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": true
        },
        "id": "_UAuD6wWd7mj"
      },
      "source": [
        "Why does the Perceptron (`model1`) only achieve ~70% accuracy? What is the architectural property of the Multi-Layer Perceptron that allows it more accurately learn the relationship between X and y? \n",
        "\n",
        "Why might this property be useful in more complex data such as images?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e01b50ff508342b905c5a4cdbd7d2dc4",
          "grade": true,
          "grade_id": "cell-302694c508c8da0e",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "EWAL6fDsd7mj"
      },
      "source": [
        "- Why does the Perceptron (model1) only achieve ~70% accuracy?  \r\n",
        "    - It is a binary classification and only has one layer meaning that it can only learn a linear decision. Therefore the model is not able to train the model to accurately predict the outcome any better then just guessing. This is a very \"vanilla\" model with only one reduction to the noise within the data.\r\n",
        "\r\n",
        "- What is the architectural property of the Multi-Layer Perceptron that allows it more accurately learn the relationship between X and y?\r\n",
        "    - The multiple hidden layers give this model the ability to learn the training data better because it is going through multiple \"filters\" to remove the noise from the data that is not useful in helping to predict the outcome of the values. Therefore this is a much better model when you need it to learn non-linear relationships.\r\n",
        "\r\n",
        "- Why might this property be useful in more complex data such as images?\r\n",
        "    - The multi-layer perceptron will be able to get rid of the noise within the image that is not useful and just pull out the data that makes the training image unique in comparison to the other images within the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDbKbRwFd7mj"
      },
      "source": [
        "## 3. Keras MMP <a id=\"Q3\"></a>\n",
        "\n",
        "- Implement a Multilayer Perceptron architecture of your choosing using the Keras library. \n",
        "- Train your model and report its baseline accuracy. \n",
        "- Then `hyperparameter tune two parameters each with no more than 3 values each`\n",
        "    - Due to limited computational resources on CodeGrade `DO NOT INCLUDE ADDITIONAL PARAMETERS OR VALUES PLEASE`\n",
        "- Report your optimized model's accuracy\n",
        "- Use the Heart Disease Dataset (binary classification)\n",
        "- Use an appropriate loss function for a binary classification task\n",
        "- Use an appropriate activation function on the final layer of your network.\n",
        "- Train your model using verbose output for ease of grading.\n",
        "- Use GridSearchCV to hyperparameter tune your model. \n",
        "    - **Use `n_jobs` = 1**\n",
        "- When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
        "- Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "outputHidden": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "mzHSI1bgd7mj",
        "outputId": "f57f8332-d3da-4c4a-d50c-0a78ac0f8f09"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# load data\n",
        "data_path = 'https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "df = df.sample(frac=1)\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(303, 14)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>46</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>105</td>\n",
              "      <td>204</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278</th>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>136</td>\n",
              "      <td>319</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>152</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>256</th>\n",
              "      <td>58</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>128</td>\n",
              "      <td>259</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>130</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>301</th>\n",
              "      <td>57</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>130</td>\n",
              "      <td>131</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>115</td>\n",
              "      <td>1</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>152</td>\n",
              "      <td>298</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     age  sex  cp  trestbps  chol  fbs  ...  exang  oldpeak  slope  ca  thal  target\n",
              "118   46    0   1       105   204    0  ...      0      0.0      2   0     2       1\n",
              "278   58    0   1       136   319    1  ...      0      0.0      2   2     2       0\n",
              "256   58    1   0       128   259    0  ...      1      3.0      1   2     3       0\n",
              "301   57    1   0       130   131    0  ...      1      1.2      1   1     3       0\n",
              "83    52    1   3       152   298    1  ...      0      1.2      1   0     3       1\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "22de1dc5d17d7a0bc674d082c33e8b65",
          "grade": false,
          "grade_id": "cell-85dc40f19f5a1d6b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEgro6NFd7mk",
        "outputId": "2cc25d30-7c87-4a8e-9cff-2e09024f8807"
      },
      "source": [
        "# Create an input matrix named 'X' store it in a 2D numpy array\n",
        "X = df.drop(columns='target')\n",
        "\n",
        "# Create an output vector for the labels named 'Y', store it in 1D numpy array\n",
        "Y = df['target']\n",
        "\n",
        "# Look at the shape of X and Y\n",
        "X.shape, Y.shape\n",
        "\n",
        "######### NOTE TO DESIGNER OF THESE TEST #########\n",
        "# Y should actually be named y to follow naming conventions that I have been \n",
        "#   taught through multiple instructors here at Lambda. The features should be\n",
        "#   capitalized but the target should be lowercase. At least this is what I \n",
        "#   have learned here at Lambda."
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((303, 13), (303,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "825d4f808810a2a8d6301d7453afe478",
          "grade": true,
          "grade_id": "cell-c17c686c974edc2e",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "H-yvthIzd7mk"
      },
      "source": [
        "# Visible Testing\n",
        "assert X.shape[0] == 303, \"Did you drop/lose some rows in X? Did you properly load and split the data?\"\n",
        "assert X.shape[1] == 13, \"Did you drop/lose some columns in X? Did you properly load and split the data?\"\n",
        "assert len(Y)== 303, \"Did you drop/lose some rows in Y? Did you properly load and split the data?\""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZkIqLaxd7mk"
      },
      "source": [
        "# Imports to add GridSearch and the classifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "475835631ff6a34028443dbf604bd922",
          "grade": false,
          "grade_id": "cell-cfc5517cd0b6fa64",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "zztjJA8Nd7ml"
      },
      "source": [
        "# Create a function named 'create_model' that returns a complied keras model - required for KerasClassifier\n",
        "\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Function to create a baseline model\n",
        "def create_model(input_dim=13, layers=1, units=32, lr=0.01):\n",
        "    # Instantiate a keras model\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add the input layer\n",
        "    model.add(Dense(units=units, input_dim=input_dim, activation='relu'))\n",
        "\n",
        "    # Add hidden layer(s)\n",
        "    for i in range(layers):\n",
        "        model.add(Dense(units=(units / 2), activation='relu'))\n",
        "        units = (units / 2)\n",
        "\n",
        "    # Add the output layer\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compile the model\n",
        "    opt = SGD(learning_rate=lr)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=opt,\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    # Return the model\n",
        "    return model"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLUVvxSud7ml",
        "outputId": "b2f7c906-71a3-45cb-ddc9-e730734b9678"
      },
      "source": [
        "type(create_model())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.keras.engine.sequential.Sequential"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7b906697afb0a3b52cd19e9548eae6a7",
          "grade": true,
          "grade_id": "cell-fac25126eaf1eee4",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "sMsckVE-d7ml"
      },
      "source": [
        "# Visible Testing\n",
        "assert create_model().__module__ == 'tensorflow.python.keras.engine.sequential', \"create_model should return a keras model that was created using the Sequential class.\""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0412c74b7803790452d4914d99995dd2",
          "grade": false,
          "grade_id": "cell-fbc3d0a07230078c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "6i1dfY7ud7ml"
      },
      "source": [
        "# Pass 'create_model' into KerasClassifier, store KerasClassifier to a variable named 'model'\n",
        "model = KerasClassifier(build_fn=create_model, verbose=1)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0442c29a94065e922c5ae929976a52ab",
          "grade": true,
          "grade_id": "cell-464e7506993775f2",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "JrrvfdfOd7ml"
      },
      "source": [
        "# Visible Testing\n",
        "assert model.__module__ == 'tensorflow.python.keras.wrappers.scikit_learn', \"model should be a instance of KerasClassifier.\""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f522d3a9a4bb90f7231d1be98d067c62",
          "grade": false,
          "grade_id": "cell-985c0425f3b1304d",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "_gOFE0R-d7ml"
      },
      "source": [
        "# Define the grid search parameters inside a dictionary named 'param_grid' \n",
        "# Use 2 hyper-parameters with no more than 3 possible values for each \n",
        "\n",
        "# Define my search parameters\n",
        "param_grid = {\n",
        "    'batch_size': [32, 64],\n",
        "    'epochs': [10, 50, 90]\n",
        "}"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a551fd8278b30c1318c036f6ad43b503",
          "grade": true,
          "grade_id": "cell-c765b5db5489d7a2",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "FCQOz0hZd7mm"
      },
      "source": [
        "assert len(param_grid.keys()) == 2, \"Did you create a param dict with 2 hyper-parameters as keys?\""
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2ea6312f4bc1f42809196b696037dd52",
          "grade": false,
          "grade_id": "cell-7cfb4315eab5031c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEiQmV6Xd7mm",
        "outputId": "43ceb9e4-7953-404d-824e-c8a9b77685d3"
      },
      "source": [
        "# Create Grid Search object and name it 'gs'\n",
        "# Run Grid Search \n",
        "gs = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
        "grid_result = gs.fit(X, Y)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 18.7893 - accuracy: 0.5503\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.7205 - accuracy: 0.4824\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6881 - accuracy: 0.4954\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6782 - accuracy: 0.5382\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6816 - accuracy: 0.5403\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6728 - accuracy: 0.5245\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6594 - accuracy: 0.5675\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6620 - accuracy: 0.5589\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6694 - accuracy: 0.5654\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6382 - accuracy: 0.6030\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6650 - accuracy: 0.5246\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 50.3165 - accuracy: 0.4505\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6847 - accuracy: 0.5376\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6692 - accuracy: 0.5723\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6791 - accuracy: 0.6036\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6586 - accuracy: 0.6052\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6665 - accuracy: 0.5492\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6573 - accuracy: 0.5775\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6690 - accuracy: 0.5424\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.7047 - accuracy: 0.5649\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6515 - accuracy: 0.5731\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.6799 - accuracy: 0.4590\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 46.6788 - accuracy: 0.4278\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.9624 - accuracy: 0.5316\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6610 - accuracy: 0.5432\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6642 - accuracy: 0.5507\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6861 - accuracy: 0.5324\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6898 - accuracy: 0.4445\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6767 - accuracy: 0.6736\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6489 - accuracy: 0.6219\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6659 - accuracy: 0.5847\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6769 - accuracy: 0.5843\n",
            "2/2 [==============================] - 0s 3ms/step - loss: 0.7242 - accuracy: 0.5902\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 100.7460 - accuracy: 0.4280\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6881 - accuracy: 0.4636\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6773 - accuracy: 0.5351\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6982 - accuracy: 0.5344\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5102\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6927 - accuracy: 0.5438\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6927 - accuracy: 0.5318\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6925 - accuracy: 0.5366\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6911 - accuracy: 0.5880\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.5532\n",
            "WARNING:tensorflow:5 out of the last 117 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0ff26b7950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 3ms/step - loss: 0.7063 - accuracy: 0.5167\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 65.3006 - accuracy: 0.5606\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6724 - accuracy: 0.4742\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6595 - accuracy: 0.6394\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6644 - accuracy: 0.6571\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6839 - accuracy: 0.5588\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6840 - accuracy: 0.5220\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6735 - accuracy: 0.6234\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6830 - accuracy: 0.5221\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6770 - accuracy: 0.5248\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6678 - accuracy: 0.5498\n",
            "WARNING:tensorflow:6 out of the last 119 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fedd9a510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6820 - accuracy: 0.5667\n",
            "Epoch 1/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 34.6854 - accuracy: 0.5846\n",
            "Epoch 2/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6958 - accuracy: 0.5326\n",
            "Epoch 3/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.7022 - accuracy: 0.4803\n",
            "Epoch 4/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6927 - accuracy: 0.5375\n",
            "Epoch 5/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6897 - accuracy: 0.5566\n",
            "Epoch 6/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6875 - accuracy: 0.5392\n",
            "Epoch 7/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6889 - accuracy: 0.5300\n",
            "Epoch 8/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6914 - accuracy: 0.5003\n",
            "Epoch 9/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6865 - accuracy: 0.5361\n",
            "Epoch 10/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6748 - accuracy: 0.5473\n",
            "Epoch 11/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6742 - accuracy: 0.5477\n",
            "Epoch 12/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6716 - accuracy: 0.5559\n",
            "Epoch 13/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6639 - accuracy: 0.6411\n",
            "Epoch 14/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6636 - accuracy: 0.6016\n",
            "Epoch 15/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6690 - accuracy: 0.6063\n",
            "Epoch 16/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6617 - accuracy: 0.6170\n",
            "Epoch 17/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6676 - accuracy: 0.5610\n",
            "Epoch 18/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6593 - accuracy: 0.5799\n",
            "Epoch 19/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6599 - accuracy: 0.6295\n",
            "Epoch 20/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6601 - accuracy: 0.6154\n",
            "Epoch 21/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6791 - accuracy: 0.6089\n",
            "Epoch 22/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6625 - accuracy: 0.6106\n",
            "Epoch 23/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6613 - accuracy: 0.5928\n",
            "Epoch 24/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6447 - accuracy: 0.6417\n",
            "Epoch 25/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6810 - accuracy: 0.6065\n",
            "Epoch 26/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6850 - accuracy: 0.5710\n",
            "Epoch 27/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6473 - accuracy: 0.6170\n",
            "Epoch 28/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6583 - accuracy: 0.6089\n",
            "Epoch 29/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6989 - accuracy: 0.5583\n",
            "Epoch 30/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6526 - accuracy: 0.5921\n",
            "Epoch 31/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6663 - accuracy: 0.5729\n",
            "Epoch 32/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6418 - accuracy: 0.6473\n",
            "Epoch 33/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6678 - accuracy: 0.5884\n",
            "Epoch 34/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5123\n",
            "Epoch 35/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6449 - accuracy: 0.6250\n",
            "Epoch 36/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6368 - accuracy: 0.6159\n",
            "Epoch 37/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6640 - accuracy: 0.6060\n",
            "Epoch 38/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6627 - accuracy: 0.5550\n",
            "Epoch 39/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6475 - accuracy: 0.6170\n",
            "Epoch 40/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6500 - accuracy: 0.6537\n",
            "Epoch 41/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6277 - accuracy: 0.6187\n",
            "Epoch 42/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6679 - accuracy: 0.5972\n",
            "Epoch 43/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6883 - accuracy: 0.6022\n",
            "Epoch 44/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6171 - accuracy: 0.6483\n",
            "Epoch 45/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6442 - accuracy: 0.6509\n",
            "Epoch 46/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6322 - accuracy: 0.6204\n",
            "Epoch 47/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6608 - accuracy: 0.5865\n",
            "Epoch 48/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6447 - accuracy: 0.6115\n",
            "Epoch 49/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6385 - accuracy: 0.6534\n",
            "Epoch 50/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6159 - accuracy: 0.6634\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fed539598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.6014 - accuracy: 0.6557\n",
            "Epoch 1/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 41.5159 - accuracy: 0.4720\n",
            "Epoch 2/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6996 - accuracy: 0.5936\n",
            "Epoch 3/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6808 - accuracy: 0.5241\n",
            "Epoch 4/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6791 - accuracy: 0.5299\n",
            "Epoch 5/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6927 - accuracy: 0.5354\n",
            "Epoch 6/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6677 - accuracy: 0.5893\n",
            "Epoch 7/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1.0562 - accuracy: 0.5029\n",
            "Epoch 8/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6958 - accuracy: 0.5731\n",
            "Epoch 9/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6962 - accuracy: 0.5435\n",
            "Epoch 10/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6905 - accuracy: 0.5419\n",
            "Epoch 11/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6883 - accuracy: 0.5526\n",
            "Epoch 12/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6875 - accuracy: 0.5352\n",
            "Epoch 13/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6787 - accuracy: 0.6177\n",
            "Epoch 14/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6723 - accuracy: 0.6131\n",
            "Epoch 15/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6751 - accuracy: 0.6359\n",
            "Epoch 16/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6612 - accuracy: 0.6783\n",
            "Epoch 17/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6748 - accuracy: 0.6572\n",
            "Epoch 18/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6706 - accuracy: 0.6584\n",
            "Epoch 19/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6637 - accuracy: 0.6139\n",
            "Epoch 20/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6672 - accuracy: 0.6525\n",
            "Epoch 21/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6576 - accuracy: 0.6308\n",
            "Epoch 22/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6810 - accuracy: 0.6508\n",
            "Epoch 23/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6803 - accuracy: 0.5854\n",
            "Epoch 24/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6691 - accuracy: 0.6375\n",
            "Epoch 25/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6642 - accuracy: 0.6630\n",
            "Epoch 26/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6743 - accuracy: 0.6260\n",
            "Epoch 27/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6631 - accuracy: 0.6606\n",
            "Epoch 28/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6545 - accuracy: 0.6477\n",
            "Epoch 29/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6616 - accuracy: 0.6110\n",
            "Epoch 30/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6713 - accuracy: 0.5895\n",
            "Epoch 31/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6474 - accuracy: 0.6763\n",
            "Epoch 32/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6523 - accuracy: 0.6617\n",
            "Epoch 33/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6618 - accuracy: 0.6061\n",
            "Epoch 34/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6536 - accuracy: 0.6505\n",
            "Epoch 35/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6492 - accuracy: 0.6599\n",
            "Epoch 36/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6500 - accuracy: 0.6855\n",
            "Epoch 37/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6414 - accuracy: 0.6886\n",
            "Epoch 38/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6559 - accuracy: 0.6482\n",
            "Epoch 39/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6390 - accuracy: 0.7055\n",
            "Epoch 40/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6554 - accuracy: 0.6483\n",
            "Epoch 41/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6516 - accuracy: 0.6370\n",
            "Epoch 42/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6594 - accuracy: 0.6080\n",
            "Epoch 43/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6601 - accuracy: 0.6291\n",
            "Epoch 44/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6539 - accuracy: 0.6643\n",
            "Epoch 45/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6520 - accuracy: 0.6376\n",
            "Epoch 46/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6501 - accuracy: 0.6721\n",
            "Epoch 47/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6847 - accuracy: 0.6436\n",
            "Epoch 48/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6622 - accuracy: 0.6321\n",
            "Epoch 49/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6531 - accuracy: 0.6444\n",
            "Epoch 50/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6438 - accuracy: 0.6664\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0ff0f8e268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.6833 - accuracy: 0.5902\n",
            "Epoch 1/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 50.4673 - accuracy: 0.4966\n",
            "Epoch 2/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.7021 - accuracy: 0.4641\n",
            "Epoch 3/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6876 - accuracy: 0.5275\n",
            "Epoch 4/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6856 - accuracy: 0.5105\n",
            "Epoch 5/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6830 - accuracy: 0.5795\n",
            "Epoch 6/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6905 - accuracy: 0.5226\n",
            "Epoch 7/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6834 - accuracy: 0.5338\n",
            "Epoch 8/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6701\n",
            "Epoch 9/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6632 - accuracy: 0.6225\n",
            "Epoch 10/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6578 - accuracy: 0.5694\n",
            "Epoch 11/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6598 - accuracy: 0.6301\n",
            "Epoch 12/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6673 - accuracy: 0.6017\n",
            "Epoch 13/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6810 - accuracy: 0.5601\n",
            "Epoch 14/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6537 - accuracy: 0.6519\n",
            "Epoch 15/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6622 - accuracy: 0.5883\n",
            "Epoch 16/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6344 - accuracy: 0.6567\n",
            "Epoch 17/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6224 - accuracy: 0.6701\n",
            "Epoch 18/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6295 - accuracy: 0.6592\n",
            "Epoch 19/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6308 - accuracy: 0.6791\n",
            "Epoch 20/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6394 - accuracy: 0.6612\n",
            "Epoch 21/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6589 - accuracy: 0.6219\n",
            "Epoch 22/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6848 - accuracy: 0.5120\n",
            "Epoch 23/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6245 - accuracy: 0.6545\n",
            "Epoch 24/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6512 - accuracy: 0.6046\n",
            "Epoch 25/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6242 - accuracy: 0.6932\n",
            "Epoch 26/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6625 - accuracy: 0.5903\n",
            "Epoch 27/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6533 - accuracy: 0.6203\n",
            "Epoch 28/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6676 - accuracy: 0.6052\n",
            "Epoch 29/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6152 - accuracy: 0.6574\n",
            "Epoch 30/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6805 - accuracy: 0.6167\n",
            "Epoch 31/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5953 - accuracy: 0.6660\n",
            "Epoch 32/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6101 - accuracy: 0.6793\n",
            "Epoch 33/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5924 - accuracy: 0.6973\n",
            "Epoch 34/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5920 - accuracy: 0.6947\n",
            "Epoch 35/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6657 - accuracy: 0.6444\n",
            "Epoch 36/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6144 - accuracy: 0.6375\n",
            "Epoch 37/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6173 - accuracy: 0.6516\n",
            "Epoch 38/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6176 - accuracy: 0.6274\n",
            "Epoch 39/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6120 - accuracy: 0.6961\n",
            "Epoch 40/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.7004 - accuracy: 0.5834\n",
            "Epoch 41/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6448 - accuracy: 0.6177\n",
            "Epoch 42/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6392 - accuracy: 0.6646\n",
            "Epoch 43/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5906 - accuracy: 0.7013\n",
            "Epoch 44/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6318 - accuracy: 0.6029\n",
            "Epoch 45/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6479 - accuracy: 0.6123\n",
            "Epoch 46/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5958 - accuracy: 0.6676\n",
            "Epoch 47/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5796 - accuracy: 0.7263\n",
            "Epoch 48/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6382 - accuracy: 0.6016\n",
            "Epoch 49/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5819 - accuracy: 0.6667\n",
            "Epoch 50/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6605 - accuracy: 0.6115\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0ff0f8e1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 0.7772 - accuracy: 0.4918\n",
            "Epoch 1/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 30.3839 - accuracy: 0.4915\n",
            "Epoch 2/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7578 - accuracy: 0.4048\n",
            "Epoch 3/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.7099 - accuracy: 0.5688\n",
            "Epoch 4/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6806 - accuracy: 0.5875\n",
            "Epoch 5/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6796 - accuracy: 0.6270\n",
            "Epoch 6/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6622 - accuracy: 0.6816\n",
            "Epoch 7/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6701 - accuracy: 0.6343\n",
            "Epoch 8/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6793 - accuracy: 0.6340\n",
            "Epoch 9/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6648 - accuracy: 0.6138\n",
            "Epoch 10/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6586 - accuracy: 0.6703\n",
            "Epoch 11/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6594 - accuracy: 0.7093\n",
            "Epoch 12/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6524 - accuracy: 0.6311\n",
            "Epoch 13/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6591 - accuracy: 0.6672\n",
            "Epoch 14/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6700 - accuracy: 0.6381\n",
            "Epoch 15/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6632 - accuracy: 0.6453\n",
            "Epoch 16/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6684 - accuracy: 0.6383\n",
            "Epoch 17/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6520 - accuracy: 0.6543\n",
            "Epoch 18/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6545 - accuracy: 0.6516\n",
            "Epoch 19/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6400 - accuracy: 0.6727\n",
            "Epoch 20/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6617 - accuracy: 0.6257\n",
            "Epoch 21/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6292 - accuracy: 0.6974\n",
            "Epoch 22/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6466 - accuracy: 0.6283\n",
            "Epoch 23/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6327 - accuracy: 0.6347\n",
            "Epoch 24/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6468 - accuracy: 0.6896\n",
            "Epoch 25/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6424 - accuracy: 0.6421\n",
            "Epoch 26/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6428 - accuracy: 0.6718\n",
            "Epoch 27/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6295 - accuracy: 0.6740\n",
            "Epoch 28/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6779 - accuracy: 0.5620\n",
            "Epoch 29/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6621 - accuracy: 0.6117\n",
            "Epoch 30/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6869 - accuracy: 0.5616\n",
            "Epoch 31/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6572 - accuracy: 0.6550\n",
            "Epoch 32/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6059 - accuracy: 0.7163\n",
            "Epoch 33/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6387 - accuracy: 0.6453\n",
            "Epoch 34/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6553 - accuracy: 0.6258\n",
            "Epoch 35/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6428 - accuracy: 0.6231\n",
            "Epoch 36/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6670 - accuracy: 0.6045\n",
            "Epoch 37/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6306 - accuracy: 0.6859\n",
            "Epoch 38/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6242 - accuracy: 0.6676\n",
            "Epoch 39/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6559 - accuracy: 0.6215\n",
            "Epoch 40/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6253 - accuracy: 0.6948\n",
            "Epoch 41/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6588 - accuracy: 0.5872\n",
            "Epoch 42/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6473 - accuracy: 0.6716\n",
            "Epoch 43/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6623 - accuracy: 0.6630\n",
            "Epoch 44/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6530 - accuracy: 0.6362\n",
            "Epoch 45/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6328 - accuracy: 0.6676\n",
            "Epoch 46/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6311 - accuracy: 0.6624\n",
            "Epoch 47/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.7366 - accuracy: 0.5969\n",
            "Epoch 48/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6430 - accuracy: 0.6379\n",
            "Epoch 49/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6387 - accuracy: 0.6503\n",
            "Epoch 50/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6592 - accuracy: 0.6851\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fea1eb2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.7277 - accuracy: 0.5667\n",
            "Epoch 1/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 6.9847 - accuracy: 0.4841\n",
            "Epoch 2/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6401 - accuracy: 0.6415\n",
            "Epoch 3/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6788 - accuracy: 0.6919\n",
            "Epoch 4/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6788 - accuracy: 0.6013\n",
            "Epoch 5/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6717 - accuracy: 0.6169\n",
            "Epoch 6/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6729 - accuracy: 0.6191\n",
            "Epoch 7/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6575 - accuracy: 0.6248\n",
            "Epoch 8/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6921 - accuracy: 0.5708\n",
            "Epoch 9/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6871 - accuracy: 0.5957\n",
            "Epoch 10/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6653 - accuracy: 0.6629\n",
            "Epoch 11/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.7186 - accuracy: 0.5939\n",
            "Epoch 12/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6807 - accuracy: 0.5962\n",
            "Epoch 13/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6784 - accuracy: 0.6169\n",
            "Epoch 14/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6476 - accuracy: 0.6532\n",
            "Epoch 15/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6650 - accuracy: 0.6283\n",
            "Epoch 16/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6502 - accuracy: 0.6346\n",
            "Epoch 17/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6738 - accuracy: 0.6531\n",
            "Epoch 18/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7801 - accuracy: 0.4923\n",
            "Epoch 19/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6907 - accuracy: 0.5398\n",
            "Epoch 20/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6915 - accuracy: 0.5216\n",
            "Epoch 21/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6835 - accuracy: 0.5971\n",
            "Epoch 22/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6673 - accuracy: 0.5872\n",
            "Epoch 23/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6711 - accuracy: 0.6004\n",
            "Epoch 24/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6375 - accuracy: 0.6469\n",
            "Epoch 25/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6766 - accuracy: 0.6010\n",
            "Epoch 26/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6549 - accuracy: 0.6316\n",
            "Epoch 27/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6603 - accuracy: 0.6707\n",
            "Epoch 28/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6718 - accuracy: 0.6048\n",
            "Epoch 29/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6598 - accuracy: 0.6299\n",
            "Epoch 30/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6604 - accuracy: 0.6193\n",
            "Epoch 31/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6520 - accuracy: 0.6371\n",
            "Epoch 32/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6466 - accuracy: 0.6448\n",
            "Epoch 33/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6409 - accuracy: 0.6325\n",
            "Epoch 34/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6687 - accuracy: 0.6629\n",
            "Epoch 35/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6852 - accuracy: 0.5573\n",
            "Epoch 36/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6489 - accuracy: 0.6396\n",
            "Epoch 37/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6604 - accuracy: 0.6072\n",
            "Epoch 38/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6594 - accuracy: 0.6681\n",
            "Epoch 39/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6679 - accuracy: 0.6326\n",
            "Epoch 40/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.7020 - accuracy: 0.6445\n",
            "Epoch 41/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6516 - accuracy: 0.6782\n",
            "Epoch 42/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6581 - accuracy: 0.6647\n",
            "Epoch 43/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6392 - accuracy: 0.6640\n",
            "Epoch 44/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6554 - accuracy: 0.6481\n",
            "Epoch 45/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6417 - accuracy: 0.6745\n",
            "Epoch 46/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6834 - accuracy: 0.6155\n",
            "Epoch 47/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6483 - accuracy: 0.6466\n",
            "Epoch 48/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6508 - accuracy: 0.6524\n",
            "Epoch 49/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6604 - accuracy: 0.6148\n",
            "Epoch 50/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6603 - accuracy: 0.6241\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe902bea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6853 - accuracy: 0.6667\n",
            "Epoch 1/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 14.1554 - accuracy: 0.5054\n",
            "Epoch 2/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6933 - accuracy: 0.5135\n",
            "Epoch 3/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6927 - accuracy: 0.5835\n",
            "Epoch 4/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6927 - accuracy: 0.5675\n",
            "Epoch 5/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4958\n",
            "Epoch 6/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6926 - accuracy: 0.5440\n",
            "Epoch 7/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6926 - accuracy: 0.5397\n",
            "Epoch 8/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6925 - accuracy: 0.5397\n",
            "Epoch 9/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6924 - accuracy: 0.5426\n",
            "Epoch 10/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6928 - accuracy: 0.5199\n",
            "Epoch 11/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6924 - accuracy: 0.5323\n",
            "Epoch 12/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6922 - accuracy: 0.5370\n",
            "Epoch 13/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6928 - accuracy: 0.5170\n",
            "Epoch 14/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6926 - accuracy: 0.5211\n",
            "Epoch 15/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6905 - accuracy: 0.5790\n",
            "Epoch 16/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.5397\n",
            "Epoch 17/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6916 - accuracy: 0.5459\n",
            "Epoch 18/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6897 - accuracy: 0.5888\n",
            "Epoch 19/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6922 - accuracy: 0.5295\n",
            "Epoch 20/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6936 - accuracy: 0.4961\n",
            "Epoch 21/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6921 - accuracy: 0.5286\n",
            "Epoch 22/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6913 - accuracy: 0.5439\n",
            "Epoch 23/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6898 - accuracy: 0.5731\n",
            "Epoch 24/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6915 - accuracy: 0.5381\n",
            "Epoch 25/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6936 - accuracy: 0.4991\n",
            "Epoch 26/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6919 - accuracy: 0.5299\n",
            "Epoch 27/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6908 - accuracy: 0.5472\n",
            "Epoch 28/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6907 - accuracy: 0.5477\n",
            "Epoch 29/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6905 - accuracy: 0.5502\n",
            "Epoch 30/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.5296\n",
            "Epoch 31/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6916 - accuracy: 0.5322\n",
            "Epoch 32/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6879 - accuracy: 0.5881\n",
            "Epoch 33/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6897 - accuracy: 0.5592\n",
            "Epoch 34/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6915 - accuracy: 0.5331\n",
            "Epoch 35/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6917 - accuracy: 0.5290\n",
            "Epoch 36/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6938 - accuracy: 0.5001\n",
            "Epoch 37/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6920 - accuracy: 0.5255\n",
            "Epoch 38/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6936 - accuracy: 0.5030\n",
            "Epoch 39/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6898 - accuracy: 0.5523\n",
            "Epoch 40/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6887 - accuracy: 0.5669\n",
            "Epoch 41/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6909 - accuracy: 0.5380\n",
            "Epoch 42/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6918 - accuracy: 0.5265\n",
            "Epoch 43/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5104\n",
            "Epoch 44/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6905 - accuracy: 0.5425\n",
            "Epoch 45/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6922 - accuracy: 0.5214\n",
            "Epoch 46/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6917 - accuracy: 0.5280\n",
            "Epoch 47/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6926 - accuracy: 0.5177\n",
            "Epoch 48/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6884 - accuracy: 0.5648\n",
            "Epoch 49/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6899 - accuracy: 0.5472\n",
            "Epoch 50/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6944 - accuracy: 0.4981\n",
            "Epoch 51/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6916 - accuracy: 0.5289\n",
            "Epoch 52/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6883 - accuracy: 0.5639\n",
            "Epoch 53/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6917 - accuracy: 0.5273\n",
            "Epoch 54/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6956 - accuracy: 0.4863\n",
            "Epoch 55/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.5266\n",
            "Epoch 56/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6925 - accuracy: 0.5193\n",
            "Epoch 57/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6899 - accuracy: 0.5453\n",
            "Epoch 58/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6899 - accuracy: 0.5446\n",
            "Epoch 59/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6879 - accuracy: 0.5646\n",
            "Epoch 60/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6928 - accuracy: 0.5167\n",
            "Epoch 61/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6916 - accuracy: 0.5277\n",
            "Epoch 62/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6913 - accuracy: 0.5307\n",
            "Epoch 63/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6901 - accuracy: 0.5420\n",
            "Epoch 64/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6939 - accuracy: 0.5060\n",
            "Epoch 65/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6923 - accuracy: 0.5217\n",
            "Epoch 66/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6908 - accuracy: 0.5351\n",
            "Epoch 67/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6921 - accuracy: 0.5233\n",
            "Epoch 68/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6920 - accuracy: 0.5248\n",
            "Epoch 69/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6908 - accuracy: 0.5355\n",
            "Epoch 70/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6905 - accuracy: 0.5377\n",
            "Epoch 71/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6969 - accuracy: 0.4808\n",
            "Epoch 72/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6827 - accuracy: 0.6064\n",
            "Epoch 73/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6889 - accuracy: 0.5516\n",
            "Epoch 74/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6868 - accuracy: 0.5704\n",
            "Epoch 75/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.5114\n",
            "Epoch 76/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6889 - accuracy: 0.5511\n",
            "Epoch 77/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6907 - accuracy: 0.5355\n",
            "Epoch 78/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6946 - accuracy: 0.5020\n",
            "Epoch 79/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6885 - accuracy: 0.5549\n",
            "Epoch 80/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6923 - accuracy: 0.5220\n",
            "Epoch 81/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6897 - accuracy: 0.5445\n",
            "Epoch 82/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6914 - accuracy: 0.5299\n",
            "Epoch 83/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6908 - accuracy: 0.5346\n",
            "Epoch 84/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6898 - accuracy: 0.5429\n",
            "Epoch 85/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6898 - accuracy: 0.5434\n",
            "Epoch 86/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6940 - accuracy: 0.5079\n",
            "Epoch 87/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6908 - accuracy: 0.5344\n",
            "Epoch 88/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5148\n",
            "Epoch 89/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6937 - accuracy: 0.5108\n",
            "Epoch 90/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6889 - accuracy: 0.5502\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe902b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.6866 - accuracy: 0.5738\n",
            "Epoch 1/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 38.8564 - accuracy: 0.4454\n",
            "Epoch 2/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6867 - accuracy: 0.5617\n",
            "Epoch 3/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6743 - accuracy: 0.5529\n",
            "Epoch 4/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6586 - accuracy: 0.5683\n",
            "Epoch 5/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.5414\n",
            "Epoch 6/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6703 - accuracy: 0.5396\n",
            "Epoch 7/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6442 - accuracy: 0.5946\n",
            "Epoch 8/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6532 - accuracy: 0.5754\n",
            "Epoch 9/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6583 - accuracy: 0.5715\n",
            "Epoch 10/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6669 - accuracy: 0.5911\n",
            "Epoch 11/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6356 - accuracy: 0.6285\n",
            "Epoch 12/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6447 - accuracy: 0.5781\n",
            "Epoch 13/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6508 - accuracy: 0.6163\n",
            "Epoch 14/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6207 - accuracy: 0.6212\n",
            "Epoch 15/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6215 - accuracy: 0.6231\n",
            "Epoch 16/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6191 - accuracy: 0.6571\n",
            "Epoch 17/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6486 - accuracy: 0.6489\n",
            "Epoch 18/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6347 - accuracy: 0.5907\n",
            "Epoch 19/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6209 - accuracy: 0.6349\n",
            "Epoch 20/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6016 - accuracy: 0.6679\n",
            "Epoch 21/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6442 - accuracy: 0.6158\n",
            "Epoch 22/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6154 - accuracy: 0.6395\n",
            "Epoch 23/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6027 - accuracy: 0.6662\n",
            "Epoch 24/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5977 - accuracy: 0.6576\n",
            "Epoch 25/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6333 - accuracy: 0.6077\n",
            "Epoch 26/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6049 - accuracy: 0.6671\n",
            "Epoch 27/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6273 - accuracy: 0.6536\n",
            "Epoch 28/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6362 - accuracy: 0.6147\n",
            "Epoch 29/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5995 - accuracy: 0.6513\n",
            "Epoch 30/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6129 - accuracy: 0.6140\n",
            "Epoch 31/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6027 - accuracy: 0.6375\n",
            "Epoch 32/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6149 - accuracy: 0.6493\n",
            "Epoch 33/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6051 - accuracy: 0.6612\n",
            "Epoch 34/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5936 - accuracy: 0.6701\n",
            "Epoch 35/90\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6053 - accuracy: 0.6530\n",
            "Epoch 36/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6091 - accuracy: 0.6690\n",
            "Epoch 37/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6243 - accuracy: 0.6301\n",
            "Epoch 38/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6058 - accuracy: 0.6348\n",
            "Epoch 39/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6093 - accuracy: 0.7120\n",
            "Epoch 40/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6013 - accuracy: 0.6198\n",
            "Epoch 41/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6087 - accuracy: 0.6393\n",
            "Epoch 42/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5850 - accuracy: 0.6666\n",
            "Epoch 43/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6130 - accuracy: 0.6347\n",
            "Epoch 44/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6145 - accuracy: 0.6487\n",
            "Epoch 45/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5789 - accuracy: 0.6987\n",
            "Epoch 46/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6046 - accuracy: 0.6597\n",
            "Epoch 47/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5757 - accuracy: 0.7065\n",
            "Epoch 48/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5978 - accuracy: 0.6558\n",
            "Epoch 49/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5928 - accuracy: 0.6786\n",
            "Epoch 50/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5938 - accuracy: 0.6670\n",
            "Epoch 51/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6101 - accuracy: 0.6528\n",
            "Epoch 52/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5938 - accuracy: 0.6678\n",
            "Epoch 53/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6202 - accuracy: 0.6433\n",
            "Epoch 54/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5712 - accuracy: 0.7071\n",
            "Epoch 55/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6279 - accuracy: 0.6141\n",
            "Epoch 56/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6326 - accuracy: 0.6512\n",
            "Epoch 57/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6171 - accuracy: 0.6491\n",
            "Epoch 58/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5826 - accuracy: 0.6844\n",
            "Epoch 59/90\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6552 - accuracy: 0.6171\n",
            "Epoch 60/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6166 - accuracy: 0.6196\n",
            "Epoch 61/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6253 - accuracy: 0.6492\n",
            "Epoch 62/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6089 - accuracy: 0.6630\n",
            "Epoch 63/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5900 - accuracy: 0.6607\n",
            "Epoch 64/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6061 - accuracy: 0.6675\n",
            "Epoch 65/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5903 - accuracy: 0.6493\n",
            "Epoch 66/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5869 - accuracy: 0.6683\n",
            "Epoch 67/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5933 - accuracy: 0.7075\n",
            "Epoch 68/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6088 - accuracy: 0.6531\n",
            "Epoch 69/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6115 - accuracy: 0.6647\n",
            "Epoch 70/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6463 - accuracy: 0.6686\n",
            "Epoch 71/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6209 - accuracy: 0.6570\n",
            "Epoch 72/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5698 - accuracy: 0.6904\n",
            "Epoch 73/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5816 - accuracy: 0.6983\n",
            "Epoch 74/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6148 - accuracy: 0.6544\n",
            "Epoch 75/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6087 - accuracy: 0.6665\n",
            "Epoch 76/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6118 - accuracy: 0.6746\n",
            "Epoch 77/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5908 - accuracy: 0.6921\n",
            "Epoch 78/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5960 - accuracy: 0.6684\n",
            "Epoch 79/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5768 - accuracy: 0.6899\n",
            "Epoch 80/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6341 - accuracy: 0.6123\n",
            "Epoch 81/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5982 - accuracy: 0.6968\n",
            "Epoch 82/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6128 - accuracy: 0.6633\n",
            "Epoch 83/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6104 - accuracy: 0.6704\n",
            "Epoch 84/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6003 - accuracy: 0.6730\n",
            "Epoch 85/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6119 - accuracy: 0.6623\n",
            "Epoch 86/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5996 - accuracy: 0.6701\n",
            "Epoch 87/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6269 - accuracy: 0.6456\n",
            "Epoch 88/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5874 - accuracy: 0.6680\n",
            "Epoch 89/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5915 - accuracy: 0.6636\n",
            "Epoch 90/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6109 - accuracy: 0.6626\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe6d94ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 0.5939 - accuracy: 0.7213\n",
            "Epoch 1/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 119.9047 - accuracy: 0.4251\n",
            "Epoch 2/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7007 - accuracy: 0.5466\n",
            "Epoch 3/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6866 - accuracy: 0.6354\n",
            "Epoch 4/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6812 - accuracy: 0.6271\n",
            "Epoch 5/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6791 - accuracy: 0.6042\n",
            "Epoch 6/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6694 - accuracy: 0.6061\n",
            "Epoch 7/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6720 - accuracy: 0.6116\n",
            "Epoch 8/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6730 - accuracy: 0.5981\n",
            "Epoch 9/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6590 - accuracy: 0.5795\n",
            "Epoch 10/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6444 - accuracy: 0.6524\n",
            "Epoch 11/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6419 - accuracy: 0.6442\n",
            "Epoch 12/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.9794 - accuracy: 0.5818\n",
            "Epoch 13/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6559 - accuracy: 0.7019\n",
            "Epoch 14/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6445 - accuracy: 0.6764\n",
            "Epoch 15/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6509 - accuracy: 0.6907\n",
            "Epoch 16/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6385 - accuracy: 0.6497\n",
            "Epoch 17/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.7032\n",
            "Epoch 18/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6503 - accuracy: 0.6835\n",
            "Epoch 19/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6364 - accuracy: 0.6574\n",
            "Epoch 20/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6700 - accuracy: 0.6092\n",
            "Epoch 21/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6349 - accuracy: 0.7180\n",
            "Epoch 22/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6431 - accuracy: 0.6732\n",
            "Epoch 23/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6535 - accuracy: 0.6451\n",
            "Epoch 24/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6365 - accuracy: 0.6651\n",
            "Epoch 25/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6435 - accuracy: 0.6591\n",
            "Epoch 26/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6281 - accuracy: 0.7030\n",
            "Epoch 27/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6158 - accuracy: 0.7187\n",
            "Epoch 28/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6139 - accuracy: 0.7220\n",
            "Epoch 29/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6437 - accuracy: 0.6510\n",
            "Epoch 30/90\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6282 - accuracy: 0.6491\n",
            "Epoch 31/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5962 - accuracy: 0.7126\n",
            "Epoch 32/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6152 - accuracy: 0.6994\n",
            "Epoch 33/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6295 - accuracy: 0.6812\n",
            "Epoch 34/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6350 - accuracy: 0.7115\n",
            "Epoch 35/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6236 - accuracy: 0.7012\n",
            "Epoch 36/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6123 - accuracy: 0.7225\n",
            "Epoch 37/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5659 - accuracy: 0.7246\n",
            "Epoch 38/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6078 - accuracy: 0.6988\n",
            "Epoch 39/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6616 - accuracy: 0.6473\n",
            "Epoch 40/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6095 - accuracy: 0.6879\n",
            "Epoch 41/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6452 - accuracy: 0.6468\n",
            "Epoch 42/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6286 - accuracy: 0.6383\n",
            "Epoch 43/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6232 - accuracy: 0.6637\n",
            "Epoch 44/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6397 - accuracy: 0.6700\n",
            "Epoch 45/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6266 - accuracy: 0.6525\n",
            "Epoch 46/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6191 - accuracy: 0.7097\n",
            "Epoch 47/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6264 - accuracy: 0.6463\n",
            "Epoch 48/90\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6213 - accuracy: 0.6249\n",
            "Epoch 49/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6096 - accuracy: 0.6593\n",
            "Epoch 50/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6023 - accuracy: 0.6899\n",
            "Epoch 51/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6338 - accuracy: 0.6610\n",
            "Epoch 52/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5911 - accuracy: 0.7060\n",
            "Epoch 53/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5982 - accuracy: 0.7163\n",
            "Epoch 54/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6364 - accuracy: 0.6632\n",
            "Epoch 55/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6007 - accuracy: 0.6774\n",
            "Epoch 56/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6135 - accuracy: 0.6675\n",
            "Epoch 57/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6222 - accuracy: 0.6539\n",
            "Epoch 58/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5890 - accuracy: 0.6926\n",
            "Epoch 59/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6095 - accuracy: 0.6803\n",
            "Epoch 60/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6049 - accuracy: 0.6825\n",
            "Epoch 61/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5915 - accuracy: 0.6870\n",
            "Epoch 62/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5948 - accuracy: 0.6752\n",
            "Epoch 63/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5945 - accuracy: 0.6758\n",
            "Epoch 64/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5940 - accuracy: 0.6692\n",
            "Epoch 65/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5953 - accuracy: 0.6972\n",
            "Epoch 66/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6203 - accuracy: 0.6381\n",
            "Epoch 67/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5957 - accuracy: 0.7233\n",
            "Epoch 68/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6031 - accuracy: 0.7036\n",
            "Epoch 69/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5916 - accuracy: 0.7071\n",
            "Epoch 70/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6190 - accuracy: 0.6461\n",
            "Epoch 71/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5853 - accuracy: 0.6653\n",
            "Epoch 72/90\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5971 - accuracy: 0.6581\n",
            "Epoch 73/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5883 - accuracy: 0.6970\n",
            "Epoch 74/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6092 - accuracy: 0.6672\n",
            "Epoch 75/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5992 - accuracy: 0.7135\n",
            "Epoch 76/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5917 - accuracy: 0.6984\n",
            "Epoch 77/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6093 - accuracy: 0.6614\n",
            "Epoch 78/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6046 - accuracy: 0.7138\n",
            "Epoch 79/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6112 - accuracy: 0.6609\n",
            "Epoch 80/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6525 - accuracy: 0.6375\n",
            "Epoch 81/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5480 - accuracy: 0.7254\n",
            "Epoch 82/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5998 - accuracy: 0.6898\n",
            "Epoch 83/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6399 - accuracy: 0.6800\n",
            "Epoch 84/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5996 - accuracy: 0.7022\n",
            "Epoch 85/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6266 - accuracy: 0.6573\n",
            "Epoch 86/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5688 - accuracy: 0.6697\n",
            "Epoch 87/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5779 - accuracy: 0.7060\n",
            "Epoch 88/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6686 - accuracy: 0.6726\n",
            "Epoch 89/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6160 - accuracy: 0.6834\n",
            "Epoch 90/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5849 - accuracy: 0.7251\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe6d94730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.6207 - accuracy: 0.6393\n",
            "Epoch 1/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 61.1551 - accuracy: 0.5161\n",
            "Epoch 2/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6803 - accuracy: 0.5609\n",
            "Epoch 3/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6823 - accuracy: 0.6100\n",
            "Epoch 4/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6528 - accuracy: 0.6131\n",
            "Epoch 5/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6412 - accuracy: 0.6472\n",
            "Epoch 6/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6597 - accuracy: 0.6125\n",
            "Epoch 7/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6436 - accuracy: 0.6204\n",
            "Epoch 8/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6424 - accuracy: 0.6925\n",
            "Epoch 9/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6516 - accuracy: 0.6214\n",
            "Epoch 10/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6294 - accuracy: 0.6450\n",
            "Epoch 11/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6403 - accuracy: 0.6528\n",
            "Epoch 12/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6384 - accuracy: 0.6294\n",
            "Epoch 13/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6280 - accuracy: 0.6952\n",
            "Epoch 14/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6379 - accuracy: 0.6456\n",
            "Epoch 15/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6323 - accuracy: 0.6507\n",
            "Epoch 16/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5636\n",
            "Epoch 17/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6130 - accuracy: 0.6634\n",
            "Epoch 18/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6648 - accuracy: 0.5749\n",
            "Epoch 19/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6141 - accuracy: 0.7067\n",
            "Epoch 20/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6491 - accuracy: 0.6374\n",
            "Epoch 21/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6217 - accuracy: 0.6769\n",
            "Epoch 22/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6270 - accuracy: 0.6640\n",
            "Epoch 23/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6149 - accuracy: 0.7068\n",
            "Epoch 24/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6644 - accuracy: 0.5728\n",
            "Epoch 25/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6624 - accuracy: 0.6058\n",
            "Epoch 26/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6435 - accuracy: 0.6410\n",
            "Epoch 27/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6292 - accuracy: 0.6770\n",
            "Epoch 28/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6452 - accuracy: 0.6508\n",
            "Epoch 29/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6155 - accuracy: 0.6996\n",
            "Epoch 30/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6425 - accuracy: 0.6422\n",
            "Epoch 31/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6211 - accuracy: 0.6853\n",
            "Epoch 32/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6398 - accuracy: 0.6827\n",
            "Epoch 33/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6232 - accuracy: 0.6682\n",
            "Epoch 34/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6053 - accuracy: 0.7402\n",
            "Epoch 35/90\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6143 - accuracy: 0.6834\n",
            "Epoch 36/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6564 - accuracy: 0.5965\n",
            "Epoch 37/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6319 - accuracy: 0.7015\n",
            "Epoch 38/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6303 - accuracy: 0.7023\n",
            "Epoch 39/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5892 - accuracy: 0.6887\n",
            "Epoch 40/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6180 - accuracy: 0.6482\n",
            "Epoch 41/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6260 - accuracy: 0.6757\n",
            "Epoch 42/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6082 - accuracy: 0.6818\n",
            "Epoch 43/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6072 - accuracy: 0.6506\n",
            "Epoch 44/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6265 - accuracy: 0.6725\n",
            "Epoch 45/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5794 - accuracy: 0.7073\n",
            "Epoch 46/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5992 - accuracy: 0.6857\n",
            "Epoch 47/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6423 - accuracy: 0.6656\n",
            "Epoch 48/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6470 - accuracy: 0.6416\n",
            "Epoch 49/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6288 - accuracy: 0.6420\n",
            "Epoch 50/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5997 - accuracy: 0.7462\n",
            "Epoch 51/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6276 - accuracy: 0.6478\n",
            "Epoch 52/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6112 - accuracy: 0.6771\n",
            "Epoch 53/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6719 - accuracy: 0.6104\n",
            "Epoch 54/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6026 - accuracy: 0.6634\n",
            "Epoch 55/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6119 - accuracy: 0.6803\n",
            "Epoch 56/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5941 - accuracy: 0.7300\n",
            "Epoch 57/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6051 - accuracy: 0.6790\n",
            "Epoch 58/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6367 - accuracy: 0.6034\n",
            "Epoch 59/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5762 - accuracy: 0.6994\n",
            "Epoch 60/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6312 - accuracy: 0.6416\n",
            "Epoch 61/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6591\n",
            "Epoch 62/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6181 - accuracy: 0.6315\n",
            "Epoch 63/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5839 - accuracy: 0.7100\n",
            "Epoch 64/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6028 - accuracy: 0.6917\n",
            "Epoch 65/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5985 - accuracy: 0.6978\n",
            "Epoch 66/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7077 - accuracy: 0.5844\n",
            "Epoch 67/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5814 - accuracy: 0.6907\n",
            "Epoch 68/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6297 - accuracy: 0.6632\n",
            "Epoch 69/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6458 - accuracy: 0.6821\n",
            "Epoch 70/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5904 - accuracy: 0.7417\n",
            "Epoch 71/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6119 - accuracy: 0.6921\n",
            "Epoch 72/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6451 - accuracy: 0.6110\n",
            "Epoch 73/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5940 - accuracy: 0.6851\n",
            "Epoch 74/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5924 - accuracy: 0.7311\n",
            "Epoch 75/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5981 - accuracy: 0.6812\n",
            "Epoch 76/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6458 - accuracy: 0.6286\n",
            "Epoch 77/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5425 - accuracy: 0.7184\n",
            "Epoch 78/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6114 - accuracy: 0.6808\n",
            "Epoch 79/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6290 - accuracy: 0.6585\n",
            "Epoch 80/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5857 - accuracy: 0.6927\n",
            "Epoch 81/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5986 - accuracy: 0.7064\n",
            "Epoch 82/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6126 - accuracy: 0.6526\n",
            "Epoch 83/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5944 - accuracy: 0.6900\n",
            "Epoch 84/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6171 - accuracy: 0.6547\n",
            "Epoch 85/90\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6086 - accuracy: 0.6940\n",
            "Epoch 86/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6122 - accuracy: 0.7007\n",
            "Epoch 87/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5996 - accuracy: 0.6853\n",
            "Epoch 88/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5987 - accuracy: 0.7097\n",
            "Epoch 89/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6054 - accuracy: 0.6860\n",
            "Epoch 90/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6081 - accuracy: 0.7279\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe4be7510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.6311 - accuracy: 0.6833\n",
            "Epoch 1/90\n",
            "8/8 [==============================] - 1s 3ms/step - loss: 12.1141 - accuracy: 0.5010\n",
            "Epoch 2/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7060 - accuracy: 0.5422\n",
            "Epoch 3/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6930 - accuracy: 0.5391\n",
            "Epoch 4/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6930 - accuracy: 0.5265\n",
            "Epoch 5/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4989\n",
            "Epoch 6/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6930 - accuracy: 0.5106\n",
            "Epoch 7/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6921 - accuracy: 0.5629\n",
            "Epoch 8/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6925 - accuracy: 0.5318\n",
            "Epoch 9/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6922 - accuracy: 0.5425\n",
            "Epoch 10/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6923 - accuracy: 0.5338\n",
            "Epoch 11/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6917 - accuracy: 0.5536\n",
            "Epoch 12/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6916 - accuracy: 0.5524\n",
            "Epoch 13/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6914 - accuracy: 0.5555\n",
            "Epoch 14/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6918 - accuracy: 0.5432\n",
            "Epoch 15/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6915 - accuracy: 0.5474\n",
            "Epoch 16/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6923 - accuracy: 0.5274\n",
            "Epoch 17/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6906 - accuracy: 0.5642\n",
            "Epoch 18/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6915 - accuracy: 0.5430\n",
            "Epoch 19/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6925 - accuracy: 0.5197\n",
            "Epoch 20/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6923 - accuracy: 0.5244\n",
            "Epoch 21/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.5107\n",
            "Epoch 22/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6924 - accuracy: 0.5202\n",
            "Epoch 23/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6922 - accuracy: 0.5245\n",
            "Epoch 24/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6906 - accuracy: 0.5530\n",
            "Epoch 25/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6914 - accuracy: 0.5385\n",
            "Epoch 26/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6909 - accuracy: 0.5455\n",
            "Epoch 27/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6912 - accuracy: 0.5390\n",
            "Epoch 28/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6916 - accuracy: 0.5320\n",
            "Epoch 29/90\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6908 - accuracy: 0.5433\n",
            "Epoch 30/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6917 - accuracy: 0.5304\n",
            "Epoch 31/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6913 - accuracy: 0.5358\n",
            "Epoch 32/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6856 - accuracy: 0.6135\n",
            "Epoch 33/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6898 - accuracy: 0.5555\n",
            "Epoch 34/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6874 - accuracy: 0.5877\n",
            "Epoch 35/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6914 - accuracy: 0.5335\n",
            "Epoch 36/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6894 - accuracy: 0.5589\n",
            "Epoch 37/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6907 - accuracy: 0.5408\n",
            "Epoch 38/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6911 - accuracy: 0.5370\n",
            "Epoch 39/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6909 - accuracy: 0.5377\n",
            "Epoch 40/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6945 - accuracy: 0.4946\n",
            "Epoch 41/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6919 - accuracy: 0.5257\n",
            "Epoch 42/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6921 - accuracy: 0.5231\n",
            "Epoch 43/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6884 - accuracy: 0.5653\n",
            "Epoch 44/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6866 - accuracy: 0.5855\n",
            "Epoch 45/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6934 - accuracy: 0.5083\n",
            "Epoch 46/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6903 - accuracy: 0.5434\n",
            "Epoch 47/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6877 - accuracy: 0.5713\n",
            "Epoch 48/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6926 - accuracy: 0.5177\n",
            "Epoch 49/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6942 - accuracy: 0.5008\n",
            "Epoch 50/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6916 - accuracy: 0.5283\n",
            "Epoch 51/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6903 - accuracy: 0.5422\n",
            "Epoch 52/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6917 - accuracy: 0.5270\n",
            "Epoch 53/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6939 - accuracy: 0.5048\n",
            "Epoch 54/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6924 - accuracy: 0.5205\n",
            "Epoch 55/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6911 - accuracy: 0.5333\n",
            "Epoch 56/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6912 - accuracy: 0.5325\n",
            "Epoch 57/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6905 - accuracy: 0.5387\n",
            "Epoch 58/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6915 - accuracy: 0.5292\n",
            "Epoch 59/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6892 - accuracy: 0.5508\n",
            "Epoch 60/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6889 - accuracy: 0.5525\n",
            "Epoch 61/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6917 - accuracy: 0.5274\n",
            "Epoch 62/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6911 - accuracy: 0.5325\n",
            "Epoch 63/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6849 - accuracy: 0.5878\n",
            "Epoch 64/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6905 - accuracy: 0.5374\n",
            "Epoch 65/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6889 - accuracy: 0.5520\n",
            "Epoch 66/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6910 - accuracy: 0.5329\n",
            "Epoch 67/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6933 - accuracy: 0.5137\n",
            "Epoch 68/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6899 - accuracy: 0.5426\n",
            "Epoch 69/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6888 - accuracy: 0.5523\n",
            "Epoch 70/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6935 - accuracy: 0.5113\n",
            "Epoch 71/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6915 - accuracy: 0.5288\n",
            "Epoch 72/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6885 - accuracy: 0.5545\n",
            "Epoch 73/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6917 - accuracy: 0.5273\n",
            "Epoch 74/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6884 - accuracy: 0.5548\n",
            "Epoch 75/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5152\n",
            "Epoch 76/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6892 - accuracy: 0.5478\n",
            "Epoch 77/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6939 - accuracy: 0.5086\n",
            "Epoch 78/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6862 - accuracy: 0.5726\n",
            "Epoch 79/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6872 - accuracy: 0.5639\n",
            "Epoch 80/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6936 - accuracy: 0.5118\n",
            "Epoch 81/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6869 - accuracy: 0.5667\n",
            "Epoch 82/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6942 - accuracy: 0.5066\n",
            "Epoch 83/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6910 - accuracy: 0.5332\n",
            "Epoch 84/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6874 - accuracy: 0.5621\n",
            "Epoch 85/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6893 - accuracy: 0.5465\n",
            "Epoch 86/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6942 - accuracy: 0.5071\n",
            "Epoch 87/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6885 - accuracy: 0.5527\n",
            "Epoch 88/90\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6873 - accuracy: 0.5632\n",
            "Epoch 89/90\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.5256\n",
            "Epoch 90/90\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6919 - accuracy: 0.5258\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe4bb8730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5667\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 62.7782 - accuracy: 0.5038\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.7155 - accuracy: 0.4700\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6923 - accuracy: 0.5300\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6909 - accuracy: 0.5237\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6917 - accuracy: 0.5185\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6908 - accuracy: 0.5269\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6907 - accuracy: 0.5315\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6890 - accuracy: 0.5362\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6913 - accuracy: 0.5388\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6890 - accuracy: 0.5263\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0ff221d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 0.6923 - accuracy: 0.5738\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 25.7162 - accuracy: 0.5016\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.8673 - accuracy: 0.5151\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6594 - accuracy: 0.5635\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6655 - accuracy: 0.6119\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6352 - accuracy: 0.6246\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6555 - accuracy: 0.6135\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6983 - accuracy: 0.5496\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6481 - accuracy: 0.6313\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6470 - accuracy: 0.6145\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6531 - accuracy: 0.6177\n",
            "WARNING:tensorflow:7 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe4bb8a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 0.6169 - accuracy: 0.5902\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 79.3044 - accuracy: 0.4954\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7020 - accuracy: 0.5315\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6986 - accuracy: 0.5303\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6974 - accuracy: 0.5387\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5476\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6902 - accuracy: 0.5205\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6912 - accuracy: 0.5132\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6903 - accuracy: 0.5200\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6895 - accuracy: 0.5221\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6856 - accuracy: 0.5117\n",
            "WARNING:tensorflow:7 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fea13c400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.6920 - accuracy: 0.5902\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 41.3554 - accuracy: 0.5354\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7425 - accuracy: 0.4245\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6895 - accuracy: 0.5414\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6820 - accuracy: 0.5512\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6703 - accuracy: 0.5695\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6602 - accuracy: 0.6235\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6514 - accuracy: 0.6164\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6556 - accuracy: 0.6388\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6558 - accuracy: 0.6119\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6599 - accuracy: 0.6151\n",
            "WARNING:tensorflow:8 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0ff0f90730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.6515 - accuracy: 0.6167\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 51.7365 - accuracy: 0.4789\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7402 - accuracy: 0.5466\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6917 - accuracy: 0.4709\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6870 - accuracy: 0.5369\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6867 - accuracy: 0.5364\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6619 - accuracy: 0.5594\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7089 - accuracy: 0.5474\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6918 - accuracy: 0.5219\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6892 - accuracy: 0.5396\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6855 - accuracy: 0.5818\n",
            "WARNING:tensorflow:8 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fea13ce18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.6867 - accuracy: 0.5667\n",
            "Epoch 1/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 57.6815 - accuracy: 0.4930\n",
            "Epoch 2/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5185\n",
            "Epoch 3/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6926 - accuracy: 0.5461\n",
            "Epoch 4/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6927 - accuracy: 0.5310\n",
            "Epoch 5/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6927 - accuracy: 0.5305\n",
            "Epoch 6/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5138\n",
            "Epoch 7/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6924 - accuracy: 0.5456\n",
            "Epoch 8/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6925 - accuracy: 0.5373\n",
            "Epoch 9/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5112\n",
            "Epoch 10/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6924 - accuracy: 0.5336\n",
            "Epoch 11/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6916 - accuracy: 0.5680\n",
            "Epoch 12/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6920 - accuracy: 0.5477\n",
            "Epoch 13/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6929 - accuracy: 0.5112\n",
            "Epoch 14/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6917 - accuracy: 0.5545\n",
            "Epoch 15/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5435\n",
            "Epoch 16/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.5451\n",
            "Epoch 17/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6921 - accuracy: 0.5368\n",
            "Epoch 18/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6922 - accuracy: 0.5347\n",
            "Epoch 19/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6912 - accuracy: 0.5618\n",
            "Epoch 20/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.5003\n",
            "Epoch 21/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5086\n",
            "Epoch 22/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5091\n",
            "Epoch 23/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6920 - accuracy: 0.5336\n",
            "Epoch 24/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6912 - accuracy: 0.5539\n",
            "Epoch 25/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6924 - accuracy: 0.5232\n",
            "Epoch 26/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6917 - accuracy: 0.5404\n",
            "Epoch 27/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6914 - accuracy: 0.5466\n",
            "Epoch 28/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6914 - accuracy: 0.5456\n",
            "Epoch 29/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6914 - accuracy: 0.5451\n",
            "Epoch 30/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6916 - accuracy: 0.5383\n",
            "Epoch 31/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6925 - accuracy: 0.5206\n",
            "Epoch 32/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6915 - accuracy: 0.5394\n",
            "Epoch 33/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6914 - accuracy: 0.5414\n",
            "Epoch 34/50\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6924 - accuracy: 0.5222\n",
            "Epoch 35/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6922 - accuracy: 0.5253\n",
            "Epoch 36/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.5310\n",
            "Epoch 37/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6914 - accuracy: 0.5404\n",
            "Epoch 38/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6909 - accuracy: 0.5487\n",
            "Epoch 39/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6916 - accuracy: 0.5357\n",
            "Epoch 40/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6929 - accuracy: 0.5112\n",
            "Epoch 41/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6912 - accuracy: 0.5404\n",
            "Epoch 42/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6910 - accuracy: 0.5440\n",
            "Epoch 43/50\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6922 - accuracy: 0.5243\n",
            "Epoch 44/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6939 - accuracy: 0.4956\n",
            "Epoch 45/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6903 - accuracy: 0.5539\n",
            "Epoch 46/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6912 - accuracy: 0.5399\n",
            "Epoch 47/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6894 - accuracy: 0.5675\n",
            "Epoch 48/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6896 - accuracy: 0.5633\n",
            "Epoch 49/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6902 - accuracy: 0.5539\n",
            "Epoch 50/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6922 - accuracy: 0.5222\n",
            "WARNING:tensorflow:9 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fee6bcd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.6888 - accuracy: 0.5738\n",
            "Epoch 1/50\n",
            "4/4 [==============================] - 1s 3ms/step - loss: 37.3879 - accuracy: 0.5276\n",
            "Epoch 2/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.7745 - accuracy: 0.5944\n",
            "Epoch 3/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6703 - accuracy: 0.5611\n",
            "Epoch 4/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6710 - accuracy: 0.5608\n",
            "Epoch 5/50\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6575 - accuracy: 0.5879\n",
            "Epoch 6/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6586 - accuracy: 0.6069\n",
            "Epoch 7/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6501 - accuracy: 0.6190\n",
            "Epoch 8/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6679 - accuracy: 0.5895\n",
            "Epoch 9/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6670 - accuracy: 0.6217\n",
            "Epoch 10/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6454 - accuracy: 0.6391\n",
            "Epoch 11/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6651 - accuracy: 0.6171\n",
            "Epoch 12/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6453 - accuracy: 0.5983\n",
            "Epoch 13/50\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6703 - accuracy: 0.5707\n",
            "Epoch 14/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6388 - accuracy: 0.5938\n",
            "Epoch 15/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6789 - accuracy: 0.5445\n",
            "Epoch 16/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6645 - accuracy: 0.5537\n",
            "Epoch 17/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6794 - accuracy: 0.6927\n",
            "Epoch 18/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6480 - accuracy: 0.6535\n",
            "Epoch 19/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6196 - accuracy: 0.6593\n",
            "Epoch 20/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6416 - accuracy: 0.6617\n",
            "Epoch 21/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6280 - accuracy: 0.6325\n",
            "Epoch 22/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6550 - accuracy: 0.6057\n",
            "Epoch 23/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6413 - accuracy: 0.6162\n",
            "Epoch 24/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6365 - accuracy: 0.6371\n",
            "Epoch 25/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6912 - accuracy: 0.5259\n",
            "Epoch 26/50\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6396 - accuracy: 0.6429\n",
            "Epoch 27/50\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6306 - accuracy: 0.6296\n",
            "Epoch 28/50\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6397 - accuracy: 0.6810\n",
            "Epoch 29/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6380 - accuracy: 0.6243\n",
            "Epoch 30/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6481 - accuracy: 0.6323\n",
            "Epoch 31/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6536 - accuracy: 0.6221\n",
            "Epoch 32/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6491 - accuracy: 0.5969\n",
            "Epoch 33/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6383 - accuracy: 0.6906\n",
            "Epoch 34/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6389 - accuracy: 0.6691\n",
            "Epoch 35/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6214 - accuracy: 0.6425\n",
            "Epoch 36/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6286 - accuracy: 0.6793\n",
            "Epoch 37/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6300 - accuracy: 0.6576\n",
            "Epoch 38/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6200 - accuracy: 0.6879\n",
            "Epoch 39/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6230 - accuracy: 0.6809\n",
            "Epoch 40/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6235 - accuracy: 0.6545\n",
            "Epoch 41/50\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.6332 - accuracy: 0.6254\n",
            "Epoch 42/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6277 - accuracy: 0.6739\n",
            "Epoch 43/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6629 - accuracy: 0.5850\n",
            "Epoch 44/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6432 - accuracy: 0.6084\n",
            "Epoch 45/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6126 - accuracy: 0.6558\n",
            "Epoch 46/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6329 - accuracy: 0.6125\n",
            "Epoch 47/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6434 - accuracy: 0.6340\n",
            "Epoch 48/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6084 - accuracy: 0.6595\n",
            "Epoch 49/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6294 - accuracy: 0.6452\n",
            "Epoch 50/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6523 - accuracy: 0.6244\n",
            "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fee6bcc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.6547 - accuracy: 0.7049\n",
            "Epoch 1/50\n",
            "4/4 [==============================] - 1s 4ms/step - loss: 33.9837 - accuracy: 0.4350\n",
            "Epoch 2/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 3.2910 - accuracy: 0.4978\n",
            "Epoch 3/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7048 - accuracy: 0.5523\n",
            "Epoch 4/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.5114\n",
            "Epoch 5/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6814 - accuracy: 0.5275\n",
            "Epoch 6/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6644 - accuracy: 0.5601\n",
            "Epoch 7/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6687 - accuracy: 0.5560\n",
            "Epoch 8/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6619 - accuracy: 0.5491\n",
            "Epoch 9/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6612 - accuracy: 0.5429\n",
            "Epoch 10/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.5450\n",
            "Epoch 11/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6681 - accuracy: 0.5301\n",
            "Epoch 12/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6634 - accuracy: 0.5855\n",
            "Epoch 13/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6511 - accuracy: 0.6317\n",
            "Epoch 14/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6691 - accuracy: 0.6038\n",
            "Epoch 15/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6777 - accuracy: 0.6159\n",
            "Epoch 16/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6566 - accuracy: 0.6209\n",
            "Epoch 17/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6482 - accuracy: 0.5996\n",
            "Epoch 18/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6487 - accuracy: 0.6292\n",
            "Epoch 19/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6671 - accuracy: 0.6242\n",
            "Epoch 20/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6533 - accuracy: 0.6414\n",
            "Epoch 21/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6531 - accuracy: 0.6271\n",
            "Epoch 22/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6620 - accuracy: 0.6110\n",
            "Epoch 23/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6498 - accuracy: 0.6677\n",
            "Epoch 24/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6368 - accuracy: 0.6563\n",
            "Epoch 25/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6427 - accuracy: 0.6527\n",
            "Epoch 26/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5848 - accuracy: 0.6761\n",
            "Epoch 27/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6560 - accuracy: 0.6154\n",
            "Epoch 28/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6698 - accuracy: 0.6377\n",
            "Epoch 29/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6158 - accuracy: 0.6550\n",
            "Epoch 30/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6080 - accuracy: 0.6690\n",
            "Epoch 31/50\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.6178 - accuracy: 0.6547\n",
            "Epoch 32/50\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6085 - accuracy: 0.6531\n",
            "Epoch 33/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6740 - accuracy: 0.6319\n",
            "Epoch 34/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6469 - accuracy: 0.6430\n",
            "Epoch 35/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6030 - accuracy: 0.6918\n",
            "Epoch 36/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6069 - accuracy: 0.6566\n",
            "Epoch 37/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6348 - accuracy: 0.6621\n",
            "Epoch 38/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6151 - accuracy: 0.6549\n",
            "Epoch 39/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5883 - accuracy: 0.6794\n",
            "Epoch 40/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6050 - accuracy: 0.6575\n",
            "Epoch 41/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6826 - accuracy: 0.5712\n",
            "Epoch 42/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5858 - accuracy: 0.6775\n",
            "Epoch 43/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6184 - accuracy: 0.6318\n",
            "Epoch 44/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6131 - accuracy: 0.6371\n",
            "Epoch 45/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6075 - accuracy: 0.6512\n",
            "Epoch 46/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6443 - accuracy: 0.5993\n",
            "Epoch 47/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6439 - accuracy: 0.6202\n",
            "Epoch 48/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6038 - accuracy: 0.6779\n",
            "Epoch 49/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6242 - accuracy: 0.6602\n",
            "Epoch 50/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6131 - accuracy: 0.6586\n",
            "WARNING:tensorflow:10 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fed55b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.6110 - accuracy: 0.6557\n",
            "Epoch 1/50\n",
            "4/4 [==============================] - 1s 3ms/step - loss: 33.0613 - accuracy: 0.5851\n",
            "Epoch 2/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 16.3700 - accuracy: 0.4793\n",
            "Epoch 3/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6928 - accuracy: 0.5418\n",
            "Epoch 4/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6926 - accuracy: 0.5580\n",
            "Epoch 5/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6924 - accuracy: 0.5642\n",
            "Epoch 6/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6919 - accuracy: 0.5918\n",
            "Epoch 7/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6924 - accuracy: 0.5486\n",
            "Epoch 8/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6928 - accuracy: 0.5231\n",
            "Epoch 9/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6931 - accuracy: 0.5080\n",
            "Epoch 10/50\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6929 - accuracy: 0.5148\n",
            "Epoch 11/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6918 - accuracy: 0.5601\n",
            "Epoch 12/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6928 - accuracy: 0.5189\n",
            "Epoch 13/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6917 - accuracy: 0.5585\n",
            "Epoch 14/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5465\n",
            "Epoch 15/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6927 - accuracy: 0.5205\n",
            "Epoch 16/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6913 - accuracy: 0.5637\n",
            "Epoch 17/50\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6919 - accuracy: 0.5418\n",
            "Epoch 18/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6915 - accuracy: 0.5533\n",
            "Epoch 19/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6907 - accuracy: 0.5741\n",
            "Epoch 20/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6908 - accuracy: 0.5684\n",
            "Epoch 21/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6912 - accuracy: 0.5559\n",
            "Epoch 22/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6906 - accuracy: 0.5679\n",
            "Epoch 23/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6905 - accuracy: 0.5705\n",
            "Epoch 24/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6911 - accuracy: 0.5528\n",
            "Epoch 25/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6921 - accuracy: 0.5299\n",
            "Epoch 26/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6916 - accuracy: 0.5387\n",
            "Epoch 27/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6914 - accuracy: 0.5429\n",
            "Epoch 28/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6908 - accuracy: 0.5543\n",
            "Epoch 29/50\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.6911 - accuracy: 0.5465\n",
            "Epoch 30/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6919 - accuracy: 0.5309\n",
            "Epoch 31/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6909 - accuracy: 0.5486\n",
            "Epoch 32/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6914 - accuracy: 0.5382\n",
            "Epoch 33/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6891 - accuracy: 0.5804\n",
            "Epoch 34/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6909 - accuracy: 0.5465\n",
            "Epoch 35/50\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6895 - accuracy: 0.5705\n",
            "Epoch 36/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6891 - accuracy: 0.5757\n",
            "Epoch 37/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6895 - accuracy: 0.5674\n",
            "Epoch 38/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6904 - accuracy: 0.5517\n",
            "Epoch 39/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6909 - accuracy: 0.5439\n",
            "Epoch 40/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6903 - accuracy: 0.5528\n",
            "Epoch 41/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6922 - accuracy: 0.5221\n",
            "Epoch 42/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6903 - accuracy: 0.5507\n",
            "Epoch 43/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6916 - accuracy: 0.5320\n",
            "Epoch 44/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6923 - accuracy: 0.5205\n",
            "Epoch 45/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6905 - accuracy: 0.5465\n",
            "Epoch 46/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6909 - accuracy: 0.5398\n",
            "Epoch 47/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6890 - accuracy: 0.5658\n",
            "Epoch 48/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6890 - accuracy: 0.5648\n",
            "Epoch 49/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6919 - accuracy: 0.5257\n",
            "Epoch 50/50\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6901 - accuracy: 0.5497\n",
            "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe902f2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.6913 - accuracy: 0.5333\n",
            "Epoch 1/50\n",
            "4/4 [==============================] - 1s 4ms/step - loss: 70.8397 - accuracy: 0.4717\n",
            "Epoch 2/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.6561 - accuracy: 0.5202\n",
            "Epoch 3/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6945 - accuracy: 0.5375\n",
            "Epoch 4/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6965 - accuracy: 0.5531\n",
            "Epoch 5/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6980 - accuracy: 0.5275\n",
            "Epoch 6/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.5489\n",
            "Epoch 7/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6937 - accuracy: 0.5322\n",
            "Epoch 8/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6931 - accuracy: 0.5531\n",
            "Epoch 9/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6940 - accuracy: 0.5135\n",
            "Epoch 10/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5333\n",
            "Epoch 11/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6927 - accuracy: 0.5614\n",
            "Epoch 12/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6929 - accuracy: 0.5275\n",
            "Epoch 13/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.5624\n",
            "Epoch 14/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6925 - accuracy: 0.5291\n",
            "Epoch 15/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6926 - accuracy: 0.5250\n",
            "Epoch 16/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5412\n",
            "Epoch 17/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5386\n",
            "Epoch 18/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6921 - accuracy: 0.5354\n",
            "Epoch 19/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6921 - accuracy: 0.5333\n",
            "Epoch 20/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6916 - accuracy: 0.5469\n",
            "Epoch 21/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6917 - accuracy: 0.5422\n",
            "Epoch 22/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6926 - accuracy: 0.5198\n",
            "Epoch 23/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6926 - accuracy: 0.5188\n",
            "Epoch 24/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6909 - accuracy: 0.5583\n",
            "Epoch 25/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6919 - accuracy: 0.5344\n",
            "Epoch 26/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6918 - accuracy: 0.5365\n",
            "Epoch 27/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6912 - accuracy: 0.5474\n",
            "Epoch 28/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6927 - accuracy: 0.5151\n",
            "Epoch 29/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6915 - accuracy: 0.5391\n",
            "Epoch 30/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6924 - accuracy: 0.5219\n",
            "Epoch 31/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6906 - accuracy: 0.5557\n",
            "Epoch 32/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6927 - accuracy: 0.5156\n",
            "Epoch 33/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6908 - accuracy: 0.5490\n",
            "Epoch 34/50\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6911 - accuracy: 0.5438\n",
            "Epoch 35/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6914 - accuracy: 0.5375\n",
            "Epoch 36/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6940 - accuracy: 0.4938\n",
            "Epoch 37/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6918 - accuracy: 0.5302\n",
            "Epoch 38/50\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6920 - accuracy: 0.5271\n",
            "Epoch 39/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5089\n",
            "Epoch 40/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6922 - accuracy: 0.5224\n",
            "Epoch 41/50\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6916 - accuracy: 0.5318\n",
            "Epoch 42/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6907 - accuracy: 0.5453\n",
            "Epoch 43/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6916 - accuracy: 0.5323\n",
            "Epoch 44/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6892 - accuracy: 0.5662\n",
            "Epoch 45/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6916 - accuracy: 0.5313\n",
            "Epoch 46/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6913 - accuracy: 0.5349\n",
            "Epoch 47/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6916 - accuracy: 0.5302\n",
            "Epoch 48/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5120\n",
            "Epoch 49/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6910 - accuracy: 0.5380\n",
            "Epoch 50/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6906 - accuracy: 0.5432\n",
            "WARNING:tensorflow:11 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fee64be18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6888 - accuracy: 0.5667\n",
            "Epoch 1/90\n",
            "4/4 [==============================] - 1s 3ms/step - loss: 171.9932 - accuracy: 0.4481\n",
            "Epoch 2/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.7494 - accuracy: 0.4855\n",
            "Epoch 3/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7283 - accuracy: 0.5964\n",
            "Epoch 4/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6919 - accuracy: 0.5580\n",
            "Epoch 5/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6944 - accuracy: 0.5429\n",
            "Epoch 6/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6956 - accuracy: 0.5368\n",
            "Epoch 7/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6916 - accuracy: 0.5414\n",
            "Epoch 8/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6940 - accuracy: 0.5446\n",
            "Epoch 9/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6968 - accuracy: 0.5612\n",
            "Epoch 10/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6949 - accuracy: 0.5404\n",
            "Epoch 11/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6920 - accuracy: 0.5487\n",
            "Epoch 12/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6899 - accuracy: 0.5357\n",
            "Epoch 13/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6911 - accuracy: 0.5399\n",
            "Epoch 14/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6884 - accuracy: 0.5404\n",
            "Epoch 15/90\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6876 - accuracy: 0.5461\n",
            "Epoch 16/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6936 - accuracy: 0.5144\n",
            "Epoch 17/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6906 - accuracy: 0.5107\n",
            "Epoch 18/90\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6898 - accuracy: 0.5373\n",
            "Epoch 19/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6908 - accuracy: 0.5446\n",
            "Epoch 20/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6905 - accuracy: 0.5565\n",
            "Epoch 21/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6907 - accuracy: 0.5529\n",
            "Epoch 22/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6866 - accuracy: 0.5482\n",
            "Epoch 23/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6875 - accuracy: 0.5248\n",
            "Epoch 24/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6905 - accuracy: 0.5414\n",
            "Epoch 25/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6910 - accuracy: 0.5112\n",
            "Epoch 26/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6898 - accuracy: 0.5551\n",
            "Epoch 27/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6895 - accuracy: 0.5473\n",
            "Epoch 28/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6894 - accuracy: 0.5243\n",
            "Epoch 29/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6903 - accuracy: 0.5420\n",
            "Epoch 30/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6865 - accuracy: 0.5410\n",
            "Epoch 31/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6863 - accuracy: 0.5452\n",
            "Epoch 32/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6895 - accuracy: 0.5431\n",
            "Epoch 33/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6908 - accuracy: 0.5311\n",
            "Epoch 34/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6899 - accuracy: 0.5348\n",
            "Epoch 35/90\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6887 - accuracy: 0.5368\n",
            "Epoch 36/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6897 - accuracy: 0.5379\n",
            "Epoch 37/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6910 - accuracy: 0.5139\n",
            "Epoch 38/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6869 - accuracy: 0.5311\n",
            "Epoch 39/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6904 - accuracy: 0.5243\n",
            "Epoch 40/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6881 - accuracy: 0.5457\n",
            "Epoch 41/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6860 - accuracy: 0.5447\n",
            "Epoch 42/90\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6881 - accuracy: 0.5608\n",
            "Epoch 43/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6860 - accuracy: 0.5441\n",
            "Epoch 44/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6884 - accuracy: 0.5545\n",
            "Epoch 45/90\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.6853 - accuracy: 0.5551\n",
            "Epoch 46/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6871 - accuracy: 0.5264\n",
            "Epoch 47/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6885 - accuracy: 0.5363\n",
            "Epoch 48/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5400\n",
            "Epoch 49/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6858 - accuracy: 0.5759\n",
            "Epoch 50/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6847 - accuracy: 0.5608\n",
            "Epoch 51/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6888 - accuracy: 0.5556\n",
            "Epoch 52/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6884 - accuracy: 0.5374\n",
            "Epoch 53/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6878 - accuracy: 0.5457\n",
            "Epoch 54/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6886 - accuracy: 0.5478\n",
            "Epoch 55/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6898 - accuracy: 0.5400\n",
            "Epoch 56/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6915 - accuracy: 0.5170\n",
            "Epoch 57/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6863 - accuracy: 0.5358\n",
            "Epoch 58/90\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.6889 - accuracy: 0.5514\n",
            "Epoch 59/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6868 - accuracy: 0.5572\n",
            "Epoch 60/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6895 - accuracy: 0.5431\n",
            "Epoch 61/90\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6894 - accuracy: 0.5447\n",
            "Epoch 62/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6890 - accuracy: 0.5405\n",
            "Epoch 63/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6861 - accuracy: 0.5863\n",
            "Epoch 64/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6855 - accuracy: 0.5712\n",
            "Epoch 65/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6855 - accuracy: 0.5457\n",
            "Epoch 66/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6843 - accuracy: 0.5603\n",
            "Epoch 67/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6857 - accuracy: 0.5420\n",
            "Epoch 68/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6856 - accuracy: 0.5436\n",
            "Epoch 69/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6868 - accuracy: 0.5290\n",
            "Epoch 70/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6890 - accuracy: 0.5394\n",
            "Epoch 71/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6895 - accuracy: 0.5327\n",
            "Epoch 72/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6892 - accuracy: 0.5363\n",
            "Epoch 73/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6883 - accuracy: 0.5113\n",
            "Epoch 74/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5118\n",
            "Epoch 75/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6833 - accuracy: 0.5686\n",
            "Epoch 76/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6898 - accuracy: 0.5176\n",
            "Epoch 77/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6872 - accuracy: 0.5243\n",
            "Epoch 78/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6855 - accuracy: 0.5431\n",
            "Epoch 79/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6876 - accuracy: 0.5426\n",
            "Epoch 80/90\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6897 - accuracy: 0.5301\n",
            "Epoch 81/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6852 - accuracy: 0.5467\n",
            "Epoch 82/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6880 - accuracy: 0.5488\n",
            "Epoch 83/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6896 - accuracy: 0.5306\n",
            "Epoch 84/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6876 - accuracy: 0.5420\n",
            "Epoch 85/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6910 - accuracy: 0.5160\n",
            "Epoch 86/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6875 - accuracy: 0.5420\n",
            "Epoch 87/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6852 - accuracy: 0.5452\n",
            "Epoch 88/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6873 - accuracy: 0.5551\n",
            "Epoch 89/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6853 - accuracy: 0.5441\n",
            "Epoch 90/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6892 - accuracy: 0.5243\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe7f647b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.6770 - accuracy: 0.5902\n",
            "Epoch 1/90\n",
            "4/4 [==============================] - 1s 4ms/step - loss: 44.9500 - accuracy: 0.4878\n",
            "Epoch 2/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6833 - accuracy: 0.5546\n",
            "Epoch 3/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6810 - accuracy: 0.5853\n",
            "Epoch 4/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6821 - accuracy: 0.5389\n",
            "Epoch 5/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6588 - accuracy: 0.5837\n",
            "Epoch 6/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6883 - accuracy: 0.5358\n",
            "Epoch 7/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6856 - accuracy: 0.5832\n",
            "Epoch 8/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6801 - accuracy: 0.5796\n",
            "Epoch 9/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6782 - accuracy: 0.5520\n",
            "Epoch 10/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6867 - accuracy: 0.5900\n",
            "Epoch 11/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6657 - accuracy: 0.5749\n",
            "Epoch 12/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6681 - accuracy: 0.5728\n",
            "Epoch 13/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6687 - accuracy: 0.5369\n",
            "Epoch 14/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6917 - accuracy: 0.5836\n",
            "Epoch 15/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6923 - accuracy: 0.5435\n",
            "Epoch 16/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.5560\n",
            "Epoch 17/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6917 - accuracy: 0.5603\n",
            "Epoch 18/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6918 - accuracy: 0.5530\n",
            "Epoch 19/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6913 - accuracy: 0.5655\n",
            "Epoch 20/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5416\n",
            "Epoch 21/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6914 - accuracy: 0.5541\n",
            "Epoch 22/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6905 - accuracy: 0.5759\n",
            "Epoch 23/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6908 - accuracy: 0.5650\n",
            "Epoch 24/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6904 - accuracy: 0.5718\n",
            "Epoch 25/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6904 - accuracy: 0.5702\n",
            "Epoch 26/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6899 - accuracy: 0.5764\n",
            "Epoch 27/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6898 - accuracy: 0.5759\n",
            "Epoch 28/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6900 - accuracy: 0.5702\n",
            "Epoch 29/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6906 - accuracy: 0.5551\n",
            "Epoch 30/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6918 - accuracy: 0.5327\n",
            "Epoch 31/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6914 - accuracy: 0.5374\n",
            "Epoch 32/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6903 - accuracy: 0.5567\n",
            "Epoch 33/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6893 - accuracy: 0.5702\n",
            "Epoch 34/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6907 - accuracy: 0.5468\n",
            "Epoch 35/90\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6896 - accuracy: 0.5629\n",
            "Epoch 36/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6904 - accuracy: 0.5488\n",
            "Epoch 37/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6875 - accuracy: 0.5905\n",
            "Epoch 38/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6878 - accuracy: 0.5837\n",
            "Epoch 39/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6875 - accuracy: 0.5869\n",
            "Epoch 40/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6887 - accuracy: 0.5697\n",
            "Epoch 41/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6885 - accuracy: 0.5707\n",
            "Epoch 42/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6887 - accuracy: 0.5660\n",
            "Epoch 43/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6866 - accuracy: 0.5916\n",
            "Epoch 44/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6903 - accuracy: 0.5452\n",
            "Epoch 45/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6881 - accuracy: 0.5712\n",
            "Epoch 46/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6893 - accuracy: 0.5556\n",
            "Epoch 47/90\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6873 - accuracy: 0.5785\n",
            "Epoch 48/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6875 - accuracy: 0.5744\n",
            "Epoch 49/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6883 - accuracy: 0.5645\n",
            "Epoch 50/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6911 - accuracy: 0.5337\n",
            "Epoch 51/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6878 - accuracy: 0.5686\n",
            "Epoch 52/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6868 - accuracy: 0.5780\n",
            "Epoch 53/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6879 - accuracy: 0.5660\n",
            "Epoch 54/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6902 - accuracy: 0.5421\n",
            "Epoch 55/90\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6878 - accuracy: 0.5660\n",
            "Epoch 56/90\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6844 - accuracy: 0.5973\n",
            "Epoch 57/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6870 - accuracy: 0.5718\n",
            "Epoch 58/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6868 - accuracy: 0.5733\n",
            "Epoch 59/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6873 - accuracy: 0.5676\n",
            "Epoch 60/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6870 - accuracy: 0.5697\n",
            "Epoch 61/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6850 - accuracy: 0.5874\n",
            "Epoch 62/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6872 - accuracy: 0.5666\n",
            "Epoch 63/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6883 - accuracy: 0.5567\n",
            "Epoch 64/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6885 - accuracy: 0.5551\n",
            "Epoch 65/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6909 - accuracy: 0.5337\n",
            "Epoch 66/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6899 - accuracy: 0.5426\n",
            "Epoch 67/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6893 - accuracy: 0.5468\n",
            "Epoch 68/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6871 - accuracy: 0.5650\n",
            "Epoch 69/90\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6883 - accuracy: 0.5551\n",
            "Epoch 70/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6893 - accuracy: 0.5468\n",
            "Epoch 71/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6860 - accuracy: 0.5723\n",
            "Epoch 72/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6874 - accuracy: 0.5608\n",
            "Epoch 73/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6900 - accuracy: 0.5405\n",
            "Epoch 74/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6824 - accuracy: 0.5988\n",
            "Epoch 75/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6814 - accuracy: 0.6056\n",
            "Epoch 76/90\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6850 - accuracy: 0.5775\n",
            "Epoch 77/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6879 - accuracy: 0.5556\n",
            "Epoch 78/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6860 - accuracy: 0.5697\n",
            "Epoch 79/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6830 - accuracy: 0.5910\n",
            "Epoch 80/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6849 - accuracy: 0.5770\n",
            "Epoch 81/90\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6857 - accuracy: 0.5707\n",
            "Epoch 82/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6878 - accuracy: 0.5556\n",
            "Epoch 83/90\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6894 - accuracy: 0.5442\n",
            "Epoch 84/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6837 - accuracy: 0.5837\n",
            "Epoch 85/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6877 - accuracy: 0.5556\n",
            "Epoch 86/90\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6836 - accuracy: 0.5837\n",
            "Epoch 87/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6825 - accuracy: 0.5905\n",
            "Epoch 88/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6849 - accuracy: 0.5744\n",
            "Epoch 89/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6862 - accuracy: 0.5650\n",
            "Epoch 90/90\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6874 - accuracy: 0.5572\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0ff1058bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.7023 - accuracy: 0.4590\n",
            "Epoch 1/90\n",
            "4/4 [==============================] - 1s 5ms/step - loss: 55.5887 - accuracy: 0.5322\n",
            "Epoch 2/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.6245 - accuracy: 0.4230\n",
            "Epoch 3/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6953 - accuracy: 0.4937\n",
            "Epoch 4/90\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6932 - accuracy: 0.5490\n",
            "Epoch 5/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7000 - accuracy: 0.5027\n",
            "Epoch 6/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6719 - accuracy: 0.5605\n",
            "Epoch 7/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6581 - accuracy: 0.6050\n",
            "Epoch 8/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6425 - accuracy: 0.6374\n",
            "Epoch 9/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6500 - accuracy: 0.5991\n",
            "Epoch 10/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7096 - accuracy: 0.5426\n",
            "Epoch 11/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6450 - accuracy: 0.6271\n",
            "Epoch 12/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6318 - accuracy: 0.6467\n",
            "Epoch 13/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6869 - accuracy: 0.5950\n",
            "Epoch 14/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6459 - accuracy: 0.5882\n",
            "Epoch 15/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7419 - accuracy: 0.5752\n",
            "Epoch 16/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6302 - accuracy: 0.6821\n",
            "Epoch 17/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6372 - accuracy: 0.6778\n",
            "Epoch 18/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6551 - accuracy: 0.6256\n",
            "Epoch 19/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5967 - accuracy: 0.6811\n",
            "Epoch 20/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6019 - accuracy: 0.6761\n",
            "Epoch 21/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7675 - accuracy: 0.5359\n",
            "Epoch 22/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6362 - accuracy: 0.6292\n",
            "Epoch 23/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6014 - accuracy: 0.6844\n",
            "Epoch 24/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6539 - accuracy: 0.6071\n",
            "Epoch 25/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6250 - accuracy: 0.6402\n",
            "Epoch 26/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6190 - accuracy: 0.6607\n",
            "Epoch 27/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6332 - accuracy: 0.6349\n",
            "Epoch 28/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6863 - accuracy: 0.6342\n",
            "Epoch 29/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6109 - accuracy: 0.6798\n",
            "Epoch 30/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6741 - accuracy: 0.6024\n",
            "Epoch 31/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6047 - accuracy: 0.6977\n",
            "Epoch 32/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6711 - accuracy: 0.6235\n",
            "Epoch 33/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6197 - accuracy: 0.6324\n",
            "Epoch 34/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6386 - accuracy: 0.6340\n",
            "Epoch 35/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6783 - accuracy: 0.6009\n",
            "Epoch 36/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6148 - accuracy: 0.6478\n",
            "Epoch 37/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6007 - accuracy: 0.6657\n",
            "Epoch 38/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5693 - accuracy: 0.7266\n",
            "Epoch 39/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6221 - accuracy: 0.6751\n",
            "Epoch 40/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6147 - accuracy: 0.6679\n",
            "Epoch 41/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6293 - accuracy: 0.6534\n",
            "Epoch 42/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5966 - accuracy: 0.6694\n",
            "Epoch 43/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6028 - accuracy: 0.6711\n",
            "Epoch 44/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5854 - accuracy: 0.6843\n",
            "Epoch 45/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5765 - accuracy: 0.7388\n",
            "Epoch 46/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7083 - accuracy: 0.5810\n",
            "Epoch 47/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6120 - accuracy: 0.6438\n",
            "Epoch 48/90\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5742 - accuracy: 0.7107\n",
            "Epoch 49/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5881 - accuracy: 0.6866\n",
            "Epoch 50/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6748 - accuracy: 0.5838\n",
            "Epoch 51/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5974 - accuracy: 0.6797\n",
            "Epoch 52/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5821 - accuracy: 0.6968\n",
            "Epoch 53/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5995 - accuracy: 0.6568\n",
            "Epoch 54/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5898 - accuracy: 0.6709\n",
            "Epoch 55/90\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6483 - accuracy: 0.6122\n",
            "Epoch 56/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6032 - accuracy: 0.6566\n",
            "Epoch 57/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6201 - accuracy: 0.6291\n",
            "Epoch 58/90\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.7005 - accuracy: 0.5908\n",
            "Epoch 59/90\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.5881 - accuracy: 0.6963\n",
            "Epoch 60/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5792 - accuracy: 0.7113\n",
            "Epoch 61/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5863 - accuracy: 0.6880\n",
            "Epoch 62/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5993 - accuracy: 0.6903\n",
            "Epoch 63/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5616 - accuracy: 0.6996\n",
            "Epoch 64/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5579 - accuracy: 0.7276\n",
            "Epoch 65/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5915 - accuracy: 0.6914\n",
            "Epoch 66/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5674 - accuracy: 0.7197\n",
            "Epoch 67/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5731 - accuracy: 0.6750\n",
            "Epoch 68/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5915 - accuracy: 0.6736\n",
            "Epoch 69/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6419 - accuracy: 0.6461\n",
            "Epoch 70/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5917 - accuracy: 0.6783\n",
            "Epoch 71/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5975 - accuracy: 0.6716\n",
            "Epoch 72/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5811 - accuracy: 0.7056\n",
            "Epoch 73/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6423 - accuracy: 0.6036\n",
            "Epoch 74/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5807 - accuracy: 0.6758\n",
            "Epoch 75/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6058 - accuracy: 0.6676\n",
            "Epoch 76/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6233 - accuracy: 0.6831\n",
            "Epoch 77/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5703 - accuracy: 0.7144\n",
            "Epoch 78/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6349 - accuracy: 0.6488\n",
            "Epoch 79/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6199 - accuracy: 0.6400\n",
            "Epoch 80/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5746 - accuracy: 0.6993\n",
            "Epoch 81/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5964 - accuracy: 0.6920\n",
            "Epoch 82/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5617 - accuracy: 0.7095\n",
            "Epoch 83/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5504 - accuracy: 0.6951\n",
            "Epoch 84/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5905 - accuracy: 0.6297\n",
            "Epoch 85/90\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6185 - accuracy: 0.6317\n",
            "Epoch 86/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5711 - accuracy: 0.7063\n",
            "Epoch 87/90\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.5505 - accuracy: 0.7287\n",
            "Epoch 88/90\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.5850 - accuracy: 0.6767\n",
            "Epoch 89/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5734 - accuracy: 0.6808\n",
            "Epoch 90/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5664 - accuracy: 0.7086\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fee66f840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.5974 - accuracy: 0.6885\n",
            "Epoch 1/90\n",
            "4/4 [==============================] - 1s 4ms/step - loss: 39.0181 - accuracy: 0.4941\n",
            "Epoch 2/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7112 - accuracy: 0.5124\n",
            "Epoch 3/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6948 - accuracy: 0.5659\n",
            "Epoch 4/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5544\n",
            "Epoch 5/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5253\n",
            "Epoch 6/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6924 - accuracy: 0.5721\n",
            "Epoch 7/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6923 - accuracy: 0.5644\n",
            "Epoch 8/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5670\n",
            "Epoch 9/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6925 - accuracy: 0.5416\n",
            "Epoch 10/90\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.6915 - accuracy: 0.5806\n",
            "Epoch 11/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6919 - accuracy: 0.5567\n",
            "Epoch 12/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6912 - accuracy: 0.5765\n",
            "Epoch 13/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6914 - accuracy: 0.5614\n",
            "Epoch 14/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6908 - accuracy: 0.5864\n",
            "Epoch 15/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6910 - accuracy: 0.5749\n",
            "Epoch 16/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6911 - accuracy: 0.5525\n",
            "Epoch 17/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6913 - accuracy: 0.5405\n",
            "Epoch 18/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6909 - accuracy: 0.5582\n",
            "Epoch 19/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6906 - accuracy: 0.5681\n",
            "Epoch 20/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6907 - accuracy: 0.5504\n",
            "Epoch 21/90\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6919 - accuracy: 0.5228\n",
            "Epoch 22/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6912 - accuracy: 0.5411\n",
            "Epoch 23/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6888 - accuracy: 0.5734\n",
            "Epoch 24/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6884 - accuracy: 0.5771\n",
            "Epoch 25/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6897 - accuracy: 0.5516\n",
            "Epoch 26/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6881 - accuracy: 0.5953\n",
            "Epoch 27/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6887 - accuracy: 0.5766\n",
            "Epoch 28/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6891 - accuracy: 0.5687\n",
            "Epoch 29/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6887 - accuracy: 0.5561\n",
            "Epoch 30/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6887 - accuracy: 0.5503\n",
            "Epoch 31/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6871 - accuracy: 0.5951\n",
            "Epoch 32/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6890 - accuracy: 0.5524\n",
            "Epoch 33/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6883 - accuracy: 0.5582\n",
            "Epoch 34/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6881 - accuracy: 0.5582\n",
            "Epoch 35/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6886 - accuracy: 0.5535\n",
            "Epoch 36/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6889 - accuracy: 0.5373\n",
            "Epoch 37/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6874 - accuracy: 0.5530\n",
            "Epoch 38/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6889 - accuracy: 0.5363\n",
            "Epoch 39/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6870 - accuracy: 0.5681\n",
            "Epoch 40/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6844 - accuracy: 0.6041\n",
            "Epoch 41/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6878 - accuracy: 0.5794\n",
            "Epoch 42/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6860 - accuracy: 0.5845\n",
            "Epoch 43/90\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6844 - accuracy: 0.5790\n",
            "Epoch 44/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5484\n",
            "Epoch 45/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6842 - accuracy: 0.5673\n",
            "Epoch 46/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6826 - accuracy: 0.5825\n",
            "Epoch 47/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6798 - accuracy: 0.6095\n",
            "Epoch 48/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6878 - accuracy: 0.5669\n",
            "Epoch 49/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6813 - accuracy: 0.5816\n",
            "Epoch 50/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6836 - accuracy: 0.5648\n",
            "Epoch 51/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6804 - accuracy: 0.5892\n",
            "Epoch 52/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6805 - accuracy: 0.6025\n",
            "Epoch 53/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6838 - accuracy: 0.5890\n",
            "Epoch 54/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6857 - accuracy: 0.5710\n",
            "Epoch 55/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6803 - accuracy: 0.5883\n",
            "Epoch 56/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6844 - accuracy: 0.5921\n",
            "Epoch 57/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6823 - accuracy: 0.5778\n",
            "Epoch 58/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6823 - accuracy: 0.5739\n",
            "Epoch 59/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6855 - accuracy: 0.5500\n",
            "Epoch 60/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6810 - accuracy: 0.5846\n",
            "Epoch 61/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6775 - accuracy: 0.5819\n",
            "Epoch 62/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6744 - accuracy: 0.6180\n",
            "Epoch 63/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6819 - accuracy: 0.5817\n",
            "Epoch 64/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6825 - accuracy: 0.5561\n",
            "Epoch 65/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6796 - accuracy: 0.5848\n",
            "Epoch 66/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6824 - accuracy: 0.5606\n",
            "Epoch 67/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6845 - accuracy: 0.5730\n",
            "Epoch 68/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6779 - accuracy: 0.5894\n",
            "Epoch 69/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6793 - accuracy: 0.5841\n",
            "Epoch 70/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6805 - accuracy: 0.5806\n",
            "Epoch 71/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6803 - accuracy: 0.5845\n",
            "Epoch 72/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6843 - accuracy: 0.5744\n",
            "Epoch 73/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6815 - accuracy: 0.5742\n",
            "Epoch 74/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6822 - accuracy: 0.5823\n",
            "Epoch 75/90\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6830 - accuracy: 0.5972\n",
            "Epoch 76/90\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6779 - accuracy: 0.5771\n",
            "Epoch 77/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6842 - accuracy: 0.5689\n",
            "Epoch 78/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6762 - accuracy: 0.6156\n",
            "Epoch 79/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6893 - accuracy: 0.5443\n",
            "Epoch 80/90\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6794 - accuracy: 0.5825\n",
            "Epoch 81/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6770 - accuracy: 0.6041\n",
            "Epoch 82/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6837 - accuracy: 0.5748\n",
            "Epoch 83/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6797 - accuracy: 0.5968\n",
            "Epoch 84/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6817 - accuracy: 0.5535\n",
            "Epoch 85/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6864 - accuracy: 0.5462\n",
            "Epoch 86/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6742 - accuracy: 0.5903\n",
            "Epoch 87/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6822 - accuracy: 0.5763\n",
            "Epoch 88/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6784 - accuracy: 0.5791\n",
            "Epoch 89/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6857 - accuracy: 0.5462\n",
            "Epoch 90/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6829 - accuracy: 0.5842\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe6e30f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.7107 - accuracy: 0.5167\n",
            "Epoch 1/90\n",
            "4/4 [==============================] - 1s 4ms/step - loss: 88.7803 - accuracy: 0.4732\n",
            "Epoch 2/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.7007 - accuracy: 0.4756\n",
            "Epoch 3/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6958 - accuracy: 0.4599\n",
            "Epoch 4/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.4912\n",
            "Epoch 5/90\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.6943 - accuracy: 0.4743\n",
            "Epoch 6/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6902 - accuracy: 0.5306\n",
            "Epoch 7/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6889 - accuracy: 0.5505\n",
            "Epoch 8/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6851 - accuracy: 0.5344\n",
            "Epoch 9/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6832 - accuracy: 0.5651\n",
            "Epoch 10/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6806 - accuracy: 0.5339\n",
            "Epoch 11/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6797 - accuracy: 0.5057\n",
            "Epoch 12/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6801 - accuracy: 0.5219\n",
            "Epoch 13/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6796 - accuracy: 0.5458\n",
            "Epoch 14/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6703 - accuracy: 0.5349\n",
            "Epoch 15/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6758 - accuracy: 0.5708\n",
            "Epoch 16/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6751 - accuracy: 0.5469\n",
            "Epoch 17/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6797 - accuracy: 0.5125\n",
            "Epoch 18/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6638 - accuracy: 0.5208\n",
            "Epoch 19/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6719 - accuracy: 0.5349\n",
            "Epoch 20/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6681 - accuracy: 0.5281\n",
            "Epoch 21/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6721 - accuracy: 0.5375\n",
            "Epoch 22/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6724 - accuracy: 0.5250\n",
            "Epoch 23/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6741 - accuracy: 0.5406\n",
            "Epoch 24/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6792 - accuracy: 0.5365\n",
            "Epoch 25/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6809 - accuracy: 0.5141\n",
            "Epoch 26/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6776 - accuracy: 0.5344\n",
            "Epoch 27/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6692 - accuracy: 0.5698\n",
            "Epoch 28/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6709 - accuracy: 0.5511\n",
            "Epoch 29/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6897 - accuracy: 0.5162\n",
            "Epoch 30/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6764 - accuracy: 0.5250\n",
            "Epoch 31/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6666 - accuracy: 0.5516\n",
            "Epoch 32/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6652 - accuracy: 0.5656\n",
            "Epoch 33/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6696 - accuracy: 0.5531\n",
            "Epoch 34/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6744 - accuracy: 0.5281\n",
            "Epoch 35/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6767 - accuracy: 0.5396\n",
            "Epoch 36/90\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6686 - accuracy: 0.5594\n",
            "Epoch 37/90\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6634 - accuracy: 0.5469\n",
            "Epoch 38/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6742 - accuracy: 0.5083\n",
            "Epoch 39/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6791 - accuracy: 0.5110\n",
            "Epoch 40/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6589 - accuracy: 0.5662\n",
            "Epoch 41/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6780 - accuracy: 0.5125\n",
            "Epoch 42/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6650 - accuracy: 0.5458\n",
            "Epoch 43/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6689 - accuracy: 0.5443\n",
            "Epoch 44/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6680 - accuracy: 0.5208\n",
            "Epoch 45/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6585 - accuracy: 0.5417\n",
            "Epoch 46/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6714 - accuracy: 0.5288\n",
            "Epoch 47/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6742 - accuracy: 0.4701\n",
            "Epoch 48/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6702 - accuracy: 0.5333\n",
            "Epoch 49/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6619 - accuracy: 0.5570\n",
            "Epoch 50/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6779 - accuracy: 0.5536\n",
            "Epoch 51/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6791 - accuracy: 0.5252\n",
            "Epoch 52/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6539 - accuracy: 0.5677\n",
            "Epoch 53/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6673 - accuracy: 0.5490\n",
            "Epoch 54/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6662 - accuracy: 0.5676\n",
            "Epoch 55/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6565 - accuracy: 0.5670\n",
            "Epoch 56/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6569 - accuracy: 0.5724\n",
            "Epoch 57/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6737 - accuracy: 0.5164\n",
            "Epoch 58/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6675 - accuracy: 0.5442\n",
            "Epoch 59/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6577 - accuracy: 0.5990\n",
            "Epoch 60/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6657 - accuracy: 0.5812\n",
            "Epoch 61/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6567 - accuracy: 0.5677\n",
            "Epoch 62/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6717 - accuracy: 0.5259\n",
            "Epoch 63/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6552 - accuracy: 0.5983\n",
            "Epoch 64/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6594 - accuracy: 0.5854\n",
            "Epoch 65/90\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6708 - accuracy: 0.5478\n",
            "Epoch 66/90\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6697 - accuracy: 0.5496\n",
            "Epoch 67/90\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6646 - accuracy: 0.5680\n",
            "Epoch 68/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6588 - accuracy: 0.5896\n",
            "Epoch 69/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6587 - accuracy: 0.6110\n",
            "Epoch 70/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6558 - accuracy: 0.5806\n",
            "Epoch 71/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6570 - accuracy: 0.5895\n",
            "Epoch 72/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6512 - accuracy: 0.5963\n",
            "Epoch 73/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6582 - accuracy: 0.5696\n",
            "Epoch 74/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6612 - accuracy: 0.5840\n",
            "Epoch 75/90\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6521 - accuracy: 0.5929\n",
            "Epoch 76/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6701 - accuracy: 0.5784\n",
            "Epoch 77/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6467 - accuracy: 0.6176\n",
            "Epoch 78/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6608 - accuracy: 0.5852\n",
            "Epoch 79/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6710 - accuracy: 0.5898\n",
            "Epoch 80/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6579 - accuracy: 0.5979\n",
            "Epoch 81/90\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6476 - accuracy: 0.5997\n",
            "Epoch 82/90\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6626 - accuracy: 0.6109\n",
            "Epoch 83/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6434 - accuracy: 0.6119\n",
            "Epoch 84/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6464 - accuracy: 0.6040\n",
            "Epoch 85/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6472 - accuracy: 0.6098\n",
            "Epoch 86/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6447 - accuracy: 0.5951\n",
            "Epoch 87/90\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6508 - accuracy: 0.6116\n",
            "Epoch 88/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6585 - accuracy: 0.6347\n",
            "Epoch 89/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6437 - accuracy: 0.6022\n",
            "Epoch 90/90\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6428 - accuracy: 0.6089\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe5c76d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.6221 - accuracy: 0.7167\n",
            "Epoch 1/90\n",
            "10/10 [==============================] - 1s 2ms/step - loss: 65.0607 - accuracy: 0.5083\n",
            "Epoch 2/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6785 - accuracy: 0.5282\n",
            "Epoch 3/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7508 - accuracy: 0.5300\n",
            "Epoch 4/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6862 - accuracy: 0.5271\n",
            "Epoch 5/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6677 - accuracy: 0.5642\n",
            "Epoch 6/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6609 - accuracy: 0.5575\n",
            "Epoch 7/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6646 - accuracy: 0.4920\n",
            "Epoch 8/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6568 - accuracy: 0.5312\n",
            "Epoch 9/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6869 - accuracy: 0.5237\n",
            "Epoch 10/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6525 - accuracy: 0.6003\n",
            "Epoch 11/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6666 - accuracy: 0.5988\n",
            "Epoch 12/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6454 - accuracy: 0.5897\n",
            "Epoch 13/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6593 - accuracy: 0.6487\n",
            "Epoch 14/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6250 - accuracy: 0.6902\n",
            "Epoch 15/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6572 - accuracy: 0.6639\n",
            "Epoch 16/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6263 - accuracy: 0.6645\n",
            "Epoch 17/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6287 - accuracy: 0.6720\n",
            "Epoch 18/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6536 - accuracy: 0.5997\n",
            "Epoch 19/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6314 - accuracy: 0.6593\n",
            "Epoch 20/90\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.6520 - accuracy: 0.6526\n",
            "Epoch 21/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6363 - accuracy: 0.6537\n",
            "Epoch 22/90\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.6128 - accuracy: 0.6808\n",
            "Epoch 23/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6449 - accuracy: 0.6159\n",
            "Epoch 24/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6363 - accuracy: 0.6547\n",
            "Epoch 25/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6526 - accuracy: 0.6343\n",
            "Epoch 26/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6268 - accuracy: 0.7096\n",
            "Epoch 27/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6417 - accuracy: 0.6611\n",
            "Epoch 28/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5970 - accuracy: 0.6810\n",
            "Epoch 29/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6453 - accuracy: 0.6546\n",
            "Epoch 30/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6074 - accuracy: 0.7046\n",
            "Epoch 31/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6653 - accuracy: 0.6475\n",
            "Epoch 32/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5855 - accuracy: 0.7155\n",
            "Epoch 33/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6321 - accuracy: 0.6605\n",
            "Epoch 34/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6154 - accuracy: 0.6293\n",
            "Epoch 35/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6307 - accuracy: 0.6822\n",
            "Epoch 36/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6418 - accuracy: 0.6774\n",
            "Epoch 37/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6482 - accuracy: 0.5997\n",
            "Epoch 38/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6309 - accuracy: 0.6104\n",
            "Epoch 39/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6304 - accuracy: 0.6342\n",
            "Epoch 40/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6202 - accuracy: 0.6837\n",
            "Epoch 41/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6030 - accuracy: 0.7007\n",
            "Epoch 42/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6436 - accuracy: 0.6567\n",
            "Epoch 43/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6158 - accuracy: 0.6603\n",
            "Epoch 44/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6782 - accuracy: 0.6464\n",
            "Epoch 45/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6323 - accuracy: 0.6599\n",
            "Epoch 46/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6028 - accuracy: 0.7017\n",
            "Epoch 47/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6204 - accuracy: 0.6682\n",
            "Epoch 48/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6523 - accuracy: 0.6057\n",
            "Epoch 49/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6302 - accuracy: 0.6733\n",
            "Epoch 50/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6072 - accuracy: 0.6817\n",
            "Epoch 51/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6187 - accuracy: 0.6823\n",
            "Epoch 52/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6232 - accuracy: 0.6702\n",
            "Epoch 53/90\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6460 - accuracy: 0.6635\n",
            "Epoch 54/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6125 - accuracy: 0.6731\n",
            "Epoch 55/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6351 - accuracy: 0.6763\n",
            "Epoch 56/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6180 - accuracy: 0.6803\n",
            "Epoch 57/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6256 - accuracy: 0.6849\n",
            "Epoch 58/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5914 - accuracy: 0.6971\n",
            "Epoch 59/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6242 - accuracy: 0.6472\n",
            "Epoch 60/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6124 - accuracy: 0.6916\n",
            "Epoch 61/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6130 - accuracy: 0.7142\n",
            "Epoch 62/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6452 - accuracy: 0.6509\n",
            "Epoch 63/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6366 - accuracy: 0.6993\n",
            "Epoch 64/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5995 - accuracy: 0.7001\n",
            "Epoch 65/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6135 - accuracy: 0.6663\n",
            "Epoch 66/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5947 - accuracy: 0.7052\n",
            "Epoch 67/90\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6120 - accuracy: 0.6597\n",
            "Epoch 68/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6184 - accuracy: 0.6486\n",
            "Epoch 69/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6193 - accuracy: 0.6600\n",
            "Epoch 70/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5916 - accuracy: 0.6688\n",
            "Epoch 71/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6369 - accuracy: 0.6477\n",
            "Epoch 72/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6026 - accuracy: 0.6706\n",
            "Epoch 73/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6343 - accuracy: 0.6362\n",
            "Epoch 74/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6115 - accuracy: 0.6952\n",
            "Epoch 75/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6087 - accuracy: 0.6745\n",
            "Epoch 76/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6037 - accuracy: 0.7097\n",
            "Epoch 77/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6266 - accuracy: 0.6639\n",
            "Epoch 78/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6153 - accuracy: 0.6782\n",
            "Epoch 79/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6085 - accuracy: 0.6894\n",
            "Epoch 80/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6153 - accuracy: 0.6639\n",
            "Epoch 81/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6254 - accuracy: 0.7081\n",
            "Epoch 82/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6587 - accuracy: 0.7079\n",
            "Epoch 83/90\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.6102 - accuracy: 0.6884\n",
            "Epoch 84/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5928 - accuracy: 0.7246\n",
            "Epoch 85/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5815 - accuracy: 0.7224\n",
            "Epoch 86/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6105 - accuracy: 0.6833\n",
            "Epoch 87/90\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6354 - accuracy: 0.6314\n",
            "Epoch 88/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5820 - accuracy: 0.6963\n",
            "Epoch 89/90\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.5981 - accuracy: 0.7125\n",
            "Epoch 90/90\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6160 - accuracy: 0.6942\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzgTlN93d7mm",
        "outputId": "a1e7361a-96f7-4d6a-a370-9991c23871ce"
      },
      "source": [
        "# your grid_result object should be able to run in this code \n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.6368852376937866 using {'batch_size': 32, 'epochs': 90}\n",
            "Means: 0.5314207553863526, Stdev: 0.0451658512841708 with: {'batch_size': 32, 'epochs': 10}\n",
            "Means: 0.5942076563835144, Stdev: 0.0637146287396847 with: {'batch_size': 32, 'epochs': 50}\n",
            "Means: 0.6368852376937866, Stdev: 0.06034133102305014 with: {'batch_size': 32, 'epochs': 90}\n",
            "Means: 0.5874863386154174, Stdev: 0.017247697294622936 with: {'batch_size': 64, 'epochs': 10}\n",
            "Means: 0.6068852424621582, Stdev: 0.06343656869720858 with: {'batch_size': 64, 'epochs': 50}\n",
            "Means: 0.5942076444625854, Stdev: 0.09818098450373483 with: {'batch_size': 64, 'epochs': 90}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f522d3a9a4bb90f7231d1be98d067c62",
          "grade": false,
          "grade_id": "cell-985c0425f3b1304d",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYx_fO-UyY2O",
        "outputId": "c5786e43-e67c-4bf3-a239-f1407f52f8b3"
      },
      "source": [
        "# Define my new search parameters\n",
        "param_grid = {\n",
        "    'batch_size': [16, 64],\n",
        "    'epochs': [36, 63]\n",
        "}\n",
        "\n",
        "# Run Grid Search \n",
        "gs = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
        "grid_result = gs.fit(X, Y)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 49.4806 - accuracy: 0.4965\n",
            "Epoch 2/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6999 - accuracy: 0.4458\n",
            "Epoch 3/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6986 - accuracy: 0.4924\n",
            "Epoch 4/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6953 - accuracy: 0.4973\n",
            "Epoch 5/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7020 - accuracy: 0.4862\n",
            "Epoch 6/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6968 - accuracy: 0.5179\n",
            "Epoch 7/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6897 - accuracy: 0.5626\n",
            "Epoch 8/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6826 - accuracy: 0.5919\n",
            "Epoch 9/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.5943\n",
            "Epoch 10/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6812 - accuracy: 0.5988\n",
            "Epoch 11/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7008 - accuracy: 0.5999\n",
            "Epoch 12/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6824 - accuracy: 0.6088\n",
            "Epoch 13/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6901 - accuracy: 0.6194\n",
            "Epoch 14/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6828 - accuracy: 0.5679\n",
            "Epoch 15/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6856 - accuracy: 0.5967\n",
            "Epoch 16/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6813 - accuracy: 0.6117\n",
            "Epoch 17/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6832 - accuracy: 0.5742\n",
            "Epoch 18/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6811 - accuracy: 0.6100\n",
            "Epoch 19/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6714 - accuracy: 0.6183\n",
            "Epoch 20/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6741 - accuracy: 0.6307\n",
            "Epoch 21/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6762 - accuracy: 0.6313\n",
            "Epoch 22/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6920 - accuracy: 0.5396\n",
            "Epoch 23/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6810 - accuracy: 0.5990\n",
            "Epoch 24/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6744 - accuracy: 0.6209\n",
            "Epoch 25/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6834 - accuracy: 0.6049\n",
            "Epoch 26/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6829 - accuracy: 0.5922\n",
            "Epoch 27/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6947 - accuracy: 0.5865\n",
            "Epoch 28/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6792 - accuracy: 0.6012\n",
            "Epoch 29/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6672 - accuracy: 0.6502\n",
            "Epoch 30/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6782 - accuracy: 0.6216\n",
            "Epoch 31/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6743 - accuracy: 0.6420\n",
            "Epoch 32/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6835 - accuracy: 0.6329\n",
            "Epoch 33/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6757 - accuracy: 0.5865\n",
            "Epoch 34/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6769 - accuracy: 0.6040\n",
            "Epoch 35/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6743 - accuracy: 0.6102\n",
            "Epoch 36/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6725 - accuracy: 0.6186\n",
            "WARNING:tensorflow:7 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0febb8fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6636 - accuracy: 0.6066\n",
            "Epoch 1/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 11.6734 - accuracy: 0.4974\n",
            "Epoch 2/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6918 - accuracy: 0.5670\n",
            "Epoch 3/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6942 - accuracy: 0.5437\n",
            "Epoch 4/36\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6933 - accuracy: 0.5380\n",
            "Epoch 5/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6875 - accuracy: 0.6078\n",
            "Epoch 6/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6914 - accuracy: 0.5866\n",
            "Epoch 7/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6933 - accuracy: 0.5482\n",
            "Epoch 8/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6915 - accuracy: 0.5697\n",
            "Epoch 9/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6900 - accuracy: 0.5572\n",
            "Epoch 10/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6906 - accuracy: 0.5449\n",
            "Epoch 11/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6906 - accuracy: 0.5338\n",
            "Epoch 12/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6851 - accuracy: 0.5915\n",
            "Epoch 13/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6904 - accuracy: 0.5169\n",
            "Epoch 14/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6867 - accuracy: 0.5856\n",
            "Epoch 15/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7056 - accuracy: 0.5279\n",
            "Epoch 16/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7065 - accuracy: 0.5361\n",
            "Epoch 17/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6760 - accuracy: 0.6062\n",
            "Epoch 18/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6885 - accuracy: 0.5631\n",
            "Epoch 19/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6856 - accuracy: 0.5655\n",
            "Epoch 20/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6795 - accuracy: 0.5966\n",
            "Epoch 21/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6868 - accuracy: 0.5621\n",
            "Epoch 22/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6906 - accuracy: 0.5535\n",
            "Epoch 23/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6855 - accuracy: 0.5740\n",
            "Epoch 24/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6859 - accuracy: 0.5584\n",
            "Epoch 25/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6896 - accuracy: 0.5471\n",
            "Epoch 26/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6824 - accuracy: 0.5901\n",
            "Epoch 27/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6831 - accuracy: 0.5764\n",
            "Epoch 28/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6844 - accuracy: 0.5809\n",
            "Epoch 29/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6738 - accuracy: 0.6334\n",
            "Epoch 30/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6755 - accuracy: 0.6078\n",
            "Epoch 31/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6887 - accuracy: 0.5367\n",
            "Epoch 32/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6783 - accuracy: 0.6014\n",
            "Epoch 33/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6858 - accuracy: 0.5649\n",
            "Epoch 34/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6818 - accuracy: 0.5640\n",
            "Epoch 35/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6816 - accuracy: 0.5900\n",
            "Epoch 36/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6920 - accuracy: 0.5513\n",
            "WARNING:tensorflow:7 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fee60b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7230 - accuracy: 0.4754\n",
            "Epoch 1/36\n",
            "16/16 [==============================] - 1s 1ms/step - loss: 44.4790 - accuracy: 0.5610\n",
            "Epoch 2/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6844 - accuracy: 0.5401\n",
            "Epoch 3/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6923 - accuracy: 0.5001\n",
            "Epoch 4/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6877 - accuracy: 0.5391\n",
            "Epoch 5/36\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6873 - accuracy: 0.5668\n",
            "Epoch 6/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6884 - accuracy: 0.5434\n",
            "Epoch 7/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6896 - accuracy: 0.5150\n",
            "Epoch 8/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7006 - accuracy: 0.5263\n",
            "Epoch 9/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6944 - accuracy: 0.5272\n",
            "Epoch 10/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6754 - accuracy: 0.5621\n",
            "Epoch 11/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6915 - accuracy: 0.5245\n",
            "Epoch 12/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6805 - accuracy: 0.5464\n",
            "Epoch 13/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6884 - accuracy: 0.5278\n",
            "Epoch 14/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6794 - accuracy: 0.5596\n",
            "Epoch 15/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6894 - accuracy: 0.5575\n",
            "Epoch 16/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6851 - accuracy: 0.5763\n",
            "Epoch 17/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6850 - accuracy: 0.5620\n",
            "Epoch 18/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6829 - accuracy: 0.5669\n",
            "Epoch 19/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6861 - accuracy: 0.5700\n",
            "Epoch 20/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6766 - accuracy: 0.6383\n",
            "Epoch 21/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6739 - accuracy: 0.6169\n",
            "Epoch 22/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6939 - accuracy: 0.5649\n",
            "Epoch 23/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6776 - accuracy: 0.5550\n",
            "Epoch 24/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6639 - accuracy: 0.6539\n",
            "Epoch 25/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6704 - accuracy: 0.5931\n",
            "Epoch 26/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6624 - accuracy: 0.6351\n",
            "Epoch 27/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6736 - accuracy: 0.5992\n",
            "Epoch 28/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6915 - accuracy: 0.5028\n",
            "Epoch 29/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6739 - accuracy: 0.6153\n",
            "Epoch 30/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6620 - accuracy: 0.6069\n",
            "Epoch 31/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6802 - accuracy: 0.6026\n",
            "Epoch 32/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6435 - accuracy: 0.6410\n",
            "Epoch 33/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6583 - accuracy: 0.6370\n",
            "Epoch 34/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6402 - accuracy: 0.6430\n",
            "Epoch 35/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6289 - accuracy: 0.6718\n",
            "Epoch 36/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6534 - accuracy: 0.5882\n",
            "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0ff1050e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6453 - accuracy: 0.6721\n",
            "Epoch 1/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 13.0313 - accuracy: 0.4890\n",
            "Epoch 2/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7077 - accuracy: 0.6169\n",
            "Epoch 3/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6987 - accuracy: 0.6056\n",
            "Epoch 4/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6631 - accuracy: 0.6198\n",
            "Epoch 5/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6364 - accuracy: 0.6281\n",
            "Epoch 6/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5154\n",
            "Epoch 7/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7001 - accuracy: 0.5325\n",
            "Epoch 8/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6772 - accuracy: 0.5866\n",
            "Epoch 9/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7121 - accuracy: 0.5589\n",
            "Epoch 10/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6662 - accuracy: 0.6031\n",
            "Epoch 11/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6557 - accuracy: 0.6260\n",
            "Epoch 12/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6280 - accuracy: 0.6167\n",
            "Epoch 13/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6311 - accuracy: 0.6301\n",
            "Epoch 14/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6377 - accuracy: 0.6432\n",
            "Epoch 15/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6098 - accuracy: 0.6877\n",
            "Epoch 16/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6002 - accuracy: 0.6738\n",
            "Epoch 17/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6189 - accuracy: 0.6471\n",
            "Epoch 18/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6136 - accuracy: 0.6470\n",
            "Epoch 19/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6376 - accuracy: 0.6349\n",
            "Epoch 20/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6134 - accuracy: 0.6522\n",
            "Epoch 21/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6190 - accuracy: 0.6260\n",
            "Epoch 22/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6154 - accuracy: 0.6372\n",
            "Epoch 23/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6189 - accuracy: 0.6329\n",
            "Epoch 24/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6389 - accuracy: 0.6853\n",
            "Epoch 25/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6093 - accuracy: 0.6051\n",
            "Epoch 26/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6313 - accuracy: 0.5926\n",
            "Epoch 27/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6039 - accuracy: 0.6163\n",
            "Epoch 28/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6153 - accuracy: 0.6142\n",
            "Epoch 29/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6651 - accuracy: 0.5842\n",
            "Epoch 30/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6350 - accuracy: 0.6232\n",
            "Epoch 31/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5848 - accuracy: 0.6476\n",
            "Epoch 32/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6025 - accuracy: 0.6703\n",
            "Epoch 33/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6258 - accuracy: 0.6284\n",
            "Epoch 34/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6060 - accuracy: 0.6064\n",
            "Epoch 35/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6512 - accuracy: 0.6169\n",
            "Epoch 36/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5471 - accuracy: 0.6705\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6591 - accuracy: 0.5667\n",
            "Epoch 1/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 14.2102 - accuracy: 0.5052\n",
            "Epoch 2/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.4994\n",
            "Epoch 3/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6928 - accuracy: 0.5500\n",
            "Epoch 4/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5208\n",
            "Epoch 5/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.5041\n",
            "Epoch 6/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6915 - accuracy: 0.5673\n",
            "Epoch 7/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6923 - accuracy: 0.5151\n",
            "Epoch 8/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6910 - accuracy: 0.5633\n",
            "Epoch 9/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6902 - accuracy: 0.5553\n",
            "Epoch 10/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6932 - accuracy: 0.5007\n",
            "Epoch 11/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6938 - accuracy: 0.4850\n",
            "Epoch 12/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6916 - accuracy: 0.5284\n",
            "Epoch 13/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6927 - accuracy: 0.5090\n",
            "Epoch 14/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6923 - accuracy: 0.5203\n",
            "Epoch 15/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6889 - accuracy: 0.5565\n",
            "Epoch 16/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6896 - accuracy: 0.5441\n",
            "Epoch 17/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6905 - accuracy: 0.5410\n",
            "Epoch 18/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6925 - accuracy: 0.5169\n",
            "Epoch 19/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6889 - accuracy: 0.5581\n",
            "Epoch 20/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6890 - accuracy: 0.5513\n",
            "Epoch 21/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6822 - accuracy: 0.5909\n",
            "Epoch 22/36\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6908 - accuracy: 0.5372\n",
            "Epoch 23/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6914 - accuracy: 0.5243\n",
            "Epoch 24/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6904 - accuracy: 0.5279\n",
            "Epoch 25/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6909 - accuracy: 0.5335\n",
            "Epoch 26/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6859 - accuracy: 0.5678\n",
            "Epoch 27/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6886 - accuracy: 0.5146\n",
            "Epoch 28/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6907 - accuracy: 0.5294\n",
            "Epoch 29/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6899 - accuracy: 0.5124\n",
            "Epoch 30/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6901 - accuracy: 0.5331\n",
            "Epoch 31/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6880 - accuracy: 0.5105\n",
            "Epoch 32/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6884 - accuracy: 0.5361\n",
            "Epoch 33/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6890 - accuracy: 0.5451\n",
            "Epoch 34/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6875 - accuracy: 0.5578\n",
            "Epoch 35/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6908 - accuracy: 0.5256\n",
            "Epoch 36/36\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6894 - accuracy: 0.5310\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6872 - accuracy: 0.5667\n",
            "Epoch 1/63\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 26.4017 - accuracy: 0.5230\n",
            "Epoch 2/63\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6748 - accuracy: 0.5238\n",
            "Epoch 3/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6805 - accuracy: 0.5562\n",
            "Epoch 4/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6864 - accuracy: 0.5503\n",
            "Epoch 5/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6921 - accuracy: 0.4645\n",
            "Epoch 6/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6807 - accuracy: 0.5749\n",
            "Epoch 7/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.4844\n",
            "Epoch 8/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.8390 - accuracy: 0.4959\n",
            "Epoch 9/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7426 - accuracy: 0.4952\n",
            "Epoch 10/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6719 - accuracy: 0.5886\n",
            "Epoch 11/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6804 - accuracy: 0.5160\n",
            "Epoch 12/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6814 - accuracy: 0.5437\n",
            "Epoch 13/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6613 - accuracy: 0.6006\n",
            "Epoch 14/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6903 - accuracy: 0.4601\n",
            "Epoch 15/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6892 - accuracy: 0.4856\n",
            "Epoch 16/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6744 - accuracy: 0.4932\n",
            "Epoch 17/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6868 - accuracy: 0.5007\n",
            "Epoch 18/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6938 - accuracy: 0.4476\n",
            "Epoch 19/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.0320 - accuracy: 0.5043\n",
            "Epoch 20/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6834 - accuracy: 0.5344\n",
            "Epoch 21/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6903 - accuracy: 0.4589\n",
            "Epoch 22/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6920 - accuracy: 0.4760\n",
            "Epoch 23/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6895 - accuracy: 0.5343\n",
            "Epoch 24/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6743 - accuracy: 0.5030\n",
            "Epoch 25/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6695 - accuracy: 0.5143\n",
            "Epoch 26/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6379 - accuracy: 0.6308\n",
            "Epoch 27/63\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6880 - accuracy: 0.5034\n",
            "Epoch 28/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6375 - accuracy: 0.6257\n",
            "Epoch 29/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6775 - accuracy: 0.6127\n",
            "Epoch 30/63\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6324 - accuracy: 0.6357\n",
            "Epoch 31/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6302 - accuracy: 0.5838\n",
            "Epoch 32/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6645 - accuracy: 0.6098\n",
            "Epoch 33/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6536 - accuracy: 0.6481\n",
            "Epoch 34/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6799 - accuracy: 0.5325\n",
            "Epoch 35/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6797 - accuracy: 0.5458\n",
            "Epoch 36/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6637 - accuracy: 0.6495\n",
            "Epoch 37/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6793 - accuracy: 0.5172\n",
            "Epoch 38/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7239 - accuracy: 0.5943\n",
            "Epoch 39/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6404 - accuracy: 0.6970\n",
            "Epoch 40/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7002 - accuracy: 0.5004\n",
            "Epoch 41/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7042 - accuracy: 0.4079\n",
            "Epoch 42/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6958 - accuracy: 0.4281\n",
            "Epoch 43/63\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6905 - accuracy: 0.5064\n",
            "Epoch 44/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6916 - accuracy: 0.5136\n",
            "Epoch 45/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6862 - accuracy: 0.5059\n",
            "Epoch 46/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6842 - accuracy: 0.5551\n",
            "Epoch 47/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6119 - accuracy: 0.5779\n",
            "Epoch 48/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.8403 - accuracy: 0.5044\n",
            "Epoch 49/63\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6565 - accuracy: 0.6751\n",
            "Epoch 50/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6197 - accuracy: 0.7244\n",
            "Epoch 51/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6186 - accuracy: 0.6340\n",
            "Epoch 52/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6428 - accuracy: 0.6622\n",
            "Epoch 53/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6690 - accuracy: 0.5740\n",
            "Epoch 54/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7232 - accuracy: 0.4891\n",
            "Epoch 55/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.5007\n",
            "Epoch 56/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6950 - accuracy: 0.4518\n",
            "Epoch 57/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6940 - accuracy: 0.4739\n",
            "Epoch 58/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6941 - accuracy: 0.4516\n",
            "Epoch 59/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6899 - accuracy: 0.5032\n",
            "Epoch 60/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6791 - accuracy: 0.5317\n",
            "Epoch 61/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6705 - accuracy: 0.5836\n",
            "Epoch 62/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6941 - accuracy: 0.4540\n",
            "Epoch 63/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6938 - accuracy: 0.4432\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6937 - accuracy: 0.4262\n",
            "Epoch 1/63\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 45.3485 - accuracy: 0.5108\n",
            "Epoch 2/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6778 - accuracy: 0.5324\n",
            "Epoch 3/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.8646 - accuracy: 0.5044\n",
            "Epoch 4/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6711 - accuracy: 0.5609\n",
            "Epoch 5/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7544 - accuracy: 0.5371\n",
            "Epoch 6/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6598 - accuracy: 0.5726\n",
            "Epoch 7/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6644 - accuracy: 0.5174\n",
            "Epoch 8/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6878 - accuracy: 0.5892\n",
            "Epoch 9/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6880 - accuracy: 0.6066\n",
            "Epoch 10/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6915 - accuracy: 0.5323\n",
            "Epoch 11/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6860 - accuracy: 0.5936\n",
            "Epoch 12/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6872 - accuracy: 0.5700\n",
            "Epoch 13/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6836 - accuracy: 0.6072\n",
            "Epoch 14/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6847 - accuracy: 0.5818\n",
            "Epoch 15/63\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6849 - accuracy: 0.5789\n",
            "Epoch 16/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6852 - accuracy: 0.5653\n",
            "Epoch 17/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6866 - accuracy: 0.5586\n",
            "Epoch 18/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6774 - accuracy: 0.6140\n",
            "Epoch 19/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6866 - accuracy: 0.5584\n",
            "Epoch 20/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6881 - accuracy: 0.5509\n",
            "Epoch 21/63\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6889 - accuracy: 0.5463\n",
            "Epoch 22/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6882 - accuracy: 0.5510\n",
            "Epoch 23/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6888 - accuracy: 0.5473\n",
            "Epoch 24/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6811 - accuracy: 0.5869\n",
            "Epoch 25/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6893 - accuracy: 0.5445\n",
            "Epoch 26/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6862 - accuracy: 0.5592\n",
            "Epoch 27/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6811 - accuracy: 0.5818\n",
            "Epoch 28/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6865 - accuracy: 0.5582\n",
            "Epoch 29/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6910 - accuracy: 0.5376\n",
            "Epoch 30/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6867 - accuracy: 0.5567\n",
            "Epoch 31/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6876 - accuracy: 0.5527\n",
            "Epoch 32/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6832 - accuracy: 0.5720\n",
            "Epoch 33/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6805 - accuracy: 0.5824\n",
            "Epoch 34/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6821 - accuracy: 0.5760\n",
            "Epoch 35/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6825 - accuracy: 0.5739\n",
            "Epoch 36/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6933 - accuracy: 0.5312\n",
            "Epoch 37/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6777 - accuracy: 0.5927\n",
            "Epoch 38/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6988 - accuracy: 0.5092\n",
            "Epoch 39/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6865 - accuracy: 0.5585\n",
            "Epoch 40/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6910 - accuracy: 0.5417\n",
            "Epoch 41/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6767 - accuracy: 0.5951\n",
            "Epoch 42/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6900 - accuracy: 0.5453\n",
            "Epoch 43/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6793 - accuracy: 0.5852\n",
            "Epoch 44/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6838 - accuracy: 0.5685\n",
            "Epoch 45/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6752 - accuracy: 0.6010\n",
            "Epoch 46/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6959 - accuracy: 0.5235\n",
            "Epoch 47/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6871 - accuracy: 0.5563\n",
            "Epoch 48/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6795 - accuracy: 0.5839\n",
            "Epoch 49/63\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6838 - accuracy: 0.5690\n",
            "Epoch 50/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6767 - accuracy: 0.5944\n",
            "Epoch 51/63\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6808 - accuracy: 0.5798\n",
            "Epoch 52/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6884 - accuracy: 0.5515\n",
            "Epoch 53/63\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6756 - accuracy: 0.5979\n",
            "Epoch 54/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6920 - accuracy: 0.5375\n",
            "Epoch 55/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6855 - accuracy: 0.5624\n",
            "Epoch 56/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6750 - accuracy: 0.6010\n",
            "Epoch 57/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6975 - accuracy: 0.5161\n",
            "Epoch 58/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6824 - accuracy: 0.5738\n",
            "Epoch 59/63\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6851 - accuracy: 0.5637\n",
            "Epoch 60/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6905 - accuracy: 0.5424\n",
            "Epoch 61/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6757 - accuracy: 0.5996\n",
            "Epoch 62/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6865 - accuracy: 0.5579\n",
            "Epoch 63/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6897 - accuracy: 0.5452\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.7113 - accuracy: 0.4590\n",
            "Epoch 1/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 14.8160 - accuracy: 0.5445\n",
            "Epoch 2/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6915 - accuracy: 0.4319\n",
            "Epoch 3/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6911 - accuracy: 0.5165\n",
            "Epoch 4/63\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6923 - accuracy: 0.5347\n",
            "Epoch 5/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6928 - accuracy: 0.5235\n",
            "Epoch 6/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6924 - accuracy: 0.5214\n",
            "Epoch 7/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6922 - accuracy: 0.5473\n",
            "Epoch 8/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6915 - accuracy: 0.5331\n",
            "Epoch 9/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6907 - accuracy: 0.5449\n",
            "Epoch 10/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6904 - accuracy: 0.5381\n",
            "Epoch 11/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6917 - accuracy: 0.5228\n",
            "Epoch 12/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6897 - accuracy: 0.5133\n",
            "Epoch 13/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6893 - accuracy: 0.5826\n",
            "Epoch 14/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6920 - accuracy: 0.5145\n",
            "Epoch 15/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6915 - accuracy: 0.5181\n",
            "Epoch 16/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6908 - accuracy: 0.5158\n",
            "Epoch 17/63\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6900 - accuracy: 0.5137\n",
            "Epoch 18/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6921 - accuracy: 0.5190\n",
            "Epoch 19/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6872 - accuracy: 0.5609\n",
            "Epoch 20/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6915 - accuracy: 0.5293\n",
            "Epoch 21/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6912 - accuracy: 0.5262\n",
            "Epoch 22/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6897 - accuracy: 0.5310\n",
            "Epoch 23/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6864 - accuracy: 0.5737\n",
            "Epoch 24/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6887 - accuracy: 0.5261\n",
            "Epoch 25/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4928\n",
            "Epoch 26/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6926 - accuracy: 0.5064\n",
            "Epoch 27/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.5000\n",
            "Epoch 28/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6898 - accuracy: 0.5482\n",
            "Epoch 29/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6881 - accuracy: 0.5462\n",
            "Epoch 30/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6861 - accuracy: 0.5712\n",
            "Epoch 31/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6894 - accuracy: 0.5297\n",
            "Epoch 32/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6907 - accuracy: 0.5315\n",
            "Epoch 33/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.5025\n",
            "Epoch 34/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6823 - accuracy: 0.5584\n",
            "Epoch 35/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6915 - accuracy: 0.5196\n",
            "Epoch 36/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6957 - accuracy: 0.4779\n",
            "Epoch 37/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6889 - accuracy: 0.5359\n",
            "Epoch 38/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6921 - accuracy: 0.5148\n",
            "Epoch 39/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6921 - accuracy: 0.5097\n",
            "Epoch 40/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6863 - accuracy: 0.5342\n",
            "Epoch 41/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6916 - accuracy: 0.5203\n",
            "Epoch 42/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6904 - accuracy: 0.5262\n",
            "Epoch 43/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6862 - accuracy: 0.5450\n",
            "Epoch 44/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6883 - accuracy: 0.5508\n",
            "Epoch 45/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6845 - accuracy: 0.5660\n",
            "Epoch 46/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6809 - accuracy: 0.5658\n",
            "Epoch 47/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6899 - accuracy: 0.5185\n",
            "Epoch 48/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6850 - accuracy: 0.5652\n",
            "Epoch 49/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6899 - accuracy: 0.5232\n",
            "Epoch 50/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6960 - accuracy: 0.4821\n",
            "Epoch 51/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6924 - accuracy: 0.5114\n",
            "Epoch 52/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6813 - accuracy: 0.5986\n",
            "Epoch 53/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6937 - accuracy: 0.5051\n",
            "Epoch 54/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6906 - accuracy: 0.5312\n",
            "Epoch 55/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6880 - accuracy: 0.5355\n",
            "Epoch 56/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.4857\n",
            "Epoch 57/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.5133\n",
            "Epoch 58/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6892 - accuracy: 0.5046\n",
            "Epoch 59/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6849 - accuracy: 0.5641\n",
            "Epoch 60/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6811 - accuracy: 0.5740\n",
            "Epoch 61/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6899 - accuracy: 0.5141\n",
            "Epoch 62/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6913 - accuracy: 0.5234\n",
            "Epoch 63/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6896 - accuracy: 0.5214\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6840 - accuracy: 0.5902\n",
            "Epoch 1/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 10.3635 - accuracy: 0.5603\n",
            "Epoch 2/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6862 - accuracy: 0.6337\n",
            "Epoch 3/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6813 - accuracy: 0.6081\n",
            "Epoch 4/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6690 - accuracy: 0.6767\n",
            "Epoch 5/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6768 - accuracy: 0.6727\n",
            "Epoch 6/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6596 - accuracy: 0.6745\n",
            "Epoch 7/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6636 - accuracy: 0.6577\n",
            "Epoch 8/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6546 - accuracy: 0.6351\n",
            "Epoch 9/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6607 - accuracy: 0.6484\n",
            "Epoch 10/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6669 - accuracy: 0.6629\n",
            "Epoch 11/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.8413 - accuracy: 0.5806\n",
            "Epoch 12/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6573 - accuracy: 0.6709\n",
            "Epoch 13/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6675 - accuracy: 0.6483\n",
            "Epoch 14/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6345 - accuracy: 0.6711\n",
            "Epoch 15/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6550 - accuracy: 0.6265\n",
            "Epoch 16/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6691 - accuracy: 0.6004\n",
            "Epoch 17/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6552 - accuracy: 0.6859\n",
            "Epoch 18/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6626 - accuracy: 0.6132\n",
            "Epoch 19/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6435 - accuracy: 0.6657\n",
            "Epoch 20/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6481 - accuracy: 0.6404\n",
            "Epoch 21/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6392 - accuracy: 0.6748\n",
            "Epoch 22/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6346 - accuracy: 0.6898\n",
            "Epoch 23/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6288 - accuracy: 0.6850\n",
            "Epoch 24/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6646 - accuracy: 0.6598\n",
            "Epoch 25/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6413 - accuracy: 0.6939\n",
            "Epoch 26/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6354 - accuracy: 0.6535\n",
            "Epoch 27/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6617 - accuracy: 0.6207\n",
            "Epoch 28/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7400 - accuracy: 0.6930\n",
            "Epoch 29/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6595 - accuracy: 0.6481\n",
            "Epoch 30/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6479 - accuracy: 0.6456\n",
            "Epoch 31/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6302 - accuracy: 0.6618\n",
            "Epoch 32/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6535 - accuracy: 0.6265\n",
            "Epoch 33/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6244 - accuracy: 0.7106\n",
            "Epoch 34/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6157 - accuracy: 0.7260\n",
            "Epoch 35/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6129 - accuracy: 0.6936\n",
            "Epoch 36/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6655 - accuracy: 0.6431\n",
            "Epoch 37/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6283 - accuracy: 0.6690\n",
            "Epoch 38/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6037 - accuracy: 0.7418\n",
            "Epoch 39/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6172 - accuracy: 0.7064\n",
            "Epoch 40/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6212 - accuracy: 0.6780\n",
            "Epoch 41/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6310 - accuracy: 0.6489\n",
            "Epoch 42/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6301 - accuracy: 0.6500\n",
            "Epoch 43/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6300 - accuracy: 0.6578\n",
            "Epoch 44/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6266 - accuracy: 0.6897\n",
            "Epoch 45/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5890 - accuracy: 0.6957\n",
            "Epoch 46/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6284 - accuracy: 0.6497\n",
            "Epoch 47/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6203 - accuracy: 0.6633\n",
            "Epoch 48/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6529 - accuracy: 0.6202\n",
            "Epoch 49/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6061 - accuracy: 0.7184\n",
            "Epoch 50/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6257 - accuracy: 0.7035\n",
            "Epoch 51/63\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6564 - accuracy: 0.6587\n",
            "Epoch 52/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6401 - accuracy: 0.6717\n",
            "Epoch 53/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6055 - accuracy: 0.6710\n",
            "Epoch 54/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6529 - accuracy: 0.5899\n",
            "Epoch 55/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6408 - accuracy: 0.6206\n",
            "Epoch 56/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6314 - accuracy: 0.6876\n",
            "Epoch 57/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5826 - accuracy: 0.6777\n",
            "Epoch 58/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6340 - accuracy: 0.6491\n",
            "Epoch 59/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6217 - accuracy: 0.6590\n",
            "Epoch 60/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6126 - accuracy: 0.6708\n",
            "Epoch 61/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6695 - accuracy: 0.5987\n",
            "Epoch 62/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6077 - accuracy: 0.6933\n",
            "Epoch 63/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5673 - accuracy: 0.7641\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6888 - accuracy: 0.5667\n",
            "Epoch 1/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 25.1100 - accuracy: 0.4709\n",
            "Epoch 2/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6977 - accuracy: 0.5523\n",
            "Epoch 3/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6825 - accuracy: 0.5805\n",
            "Epoch 4/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5328\n",
            "Epoch 5/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6941 - accuracy: 0.6062\n",
            "Epoch 6/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6876 - accuracy: 0.5715\n",
            "Epoch 7/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6921 - accuracy: 0.5325\n",
            "Epoch 8/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6903 - accuracy: 0.5548\n",
            "Epoch 9/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6886 - accuracy: 0.5656\n",
            "Epoch 10/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6924 - accuracy: 0.5842\n",
            "Epoch 11/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5269\n",
            "Epoch 12/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6862 - accuracy: 0.5890\n",
            "Epoch 13/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6869 - accuracy: 0.5490\n",
            "Epoch 14/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7060 - accuracy: 0.5666\n",
            "Epoch 15/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6879 - accuracy: 0.5888\n",
            "Epoch 16/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6895 - accuracy: 0.5370\n",
            "Epoch 17/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6886 - accuracy: 0.5389\n",
            "Epoch 18/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6881 - accuracy: 0.5326\n",
            "Epoch 19/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6891 - accuracy: 0.5367\n",
            "Epoch 20/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6887 - accuracy: 0.5420\n",
            "Epoch 21/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6825 - accuracy: 0.5652\n",
            "Epoch 22/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6858 - accuracy: 0.5485\n",
            "Epoch 23/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6808 - accuracy: 0.5781\n",
            "Epoch 24/63\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6834 - accuracy: 0.5282\n",
            "Epoch 25/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6809 - accuracy: 0.5680\n",
            "Epoch 26/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6892 - accuracy: 0.5353\n",
            "Epoch 27/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6757 - accuracy: 0.5615\n",
            "Epoch 28/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6858 - accuracy: 0.5577\n",
            "Epoch 29/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6872 - accuracy: 0.5365\n",
            "Epoch 30/63\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6865 - accuracy: 0.5536\n",
            "Epoch 31/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6857 - accuracy: 0.5148\n",
            "Epoch 32/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6861 - accuracy: 0.5256\n",
            "Epoch 33/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6836 - accuracy: 0.5699\n",
            "Epoch 34/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6788 - accuracy: 0.5630\n",
            "Epoch 35/63\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6868 - accuracy: 0.5418\n",
            "Epoch 36/63\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6892 - accuracy: 0.5425\n",
            "Epoch 37/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6912 - accuracy: 0.5111\n",
            "Epoch 38/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6902 - accuracy: 0.5191\n",
            "Epoch 39/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6854 - accuracy: 0.5659\n",
            "Epoch 40/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6849 - accuracy: 0.5310\n",
            "Epoch 41/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6902 - accuracy: 0.5290\n",
            "Epoch 42/63\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6928 - accuracy: 0.5057\n",
            "Epoch 43/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6917 - accuracy: 0.5111\n",
            "Epoch 44/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6848 - accuracy: 0.5527\n",
            "Epoch 45/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6890 - accuracy: 0.5369\n",
            "Epoch 46/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6889 - accuracy: 0.5203\n",
            "Epoch 47/63\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6815 - accuracy: 0.5613\n",
            "Epoch 48/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6819 - accuracy: 0.5523\n",
            "Epoch 49/63\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6766 - accuracy: 0.6147\n",
            "Epoch 50/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6902 - accuracy: 0.5096\n",
            "Epoch 51/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6891 - accuracy: 0.5304\n",
            "Epoch 52/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6857 - accuracy: 0.5432\n",
            "Epoch 53/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6874 - accuracy: 0.5351\n",
            "Epoch 54/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6822 - accuracy: 0.5623\n",
            "Epoch 55/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6891 - accuracy: 0.5326\n",
            "Epoch 56/63\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6902 - accuracy: 0.5133\n",
            "Epoch 57/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6788 - accuracy: 0.5711\n",
            "Epoch 58/63\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6852 - accuracy: 0.5524\n",
            "Epoch 59/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6805 - accuracy: 0.5772\n",
            "Epoch 60/63\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6811 - accuracy: 0.5605\n",
            "Epoch 61/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6829 - accuracy: 0.5745\n",
            "Epoch 62/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6780 - accuracy: 0.5931\n",
            "Epoch 63/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6912 - accuracy: 0.4931\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6865 - accuracy: 0.5667\n",
            "Epoch 1/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 77.5671 - accuracy: 0.4754\n",
            "Epoch 2/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 4.3868 - accuracy: 0.5523\n",
            "Epoch 3/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6917 - accuracy: 0.5639\n",
            "Epoch 4/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6963 - accuracy: 0.5282\n",
            "Epoch 5/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6991 - accuracy: 0.5619\n",
            "Epoch 6/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6941 - accuracy: 0.5398\n",
            "Epoch 7/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7342 - accuracy: 0.4694\n",
            "Epoch 8/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6822 - accuracy: 0.5224\n",
            "Epoch 9/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.7058 - accuracy: 0.4896\n",
            "Epoch 10/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6749 - accuracy: 0.5646\n",
            "Epoch 11/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6530 - accuracy: 0.5797\n",
            "Epoch 12/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6514 - accuracy: 0.6148\n",
            "Epoch 13/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6588 - accuracy: 0.5805\n",
            "Epoch 14/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6806 - accuracy: 0.5483\n",
            "Epoch 15/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6808 - accuracy: 0.5833\n",
            "Epoch 16/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6415 - accuracy: 0.6336\n",
            "Epoch 17/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6284 - accuracy: 0.6575\n",
            "Epoch 18/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6318 - accuracy: 0.6480\n",
            "Epoch 19/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.7007 - accuracy: 0.5864\n",
            "Epoch 20/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6379 - accuracy: 0.6563\n",
            "Epoch 21/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6375 - accuracy: 0.6597\n",
            "Epoch 22/36\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.7087 - accuracy: 0.5708\n",
            "Epoch 23/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6536 - accuracy: 0.6248\n",
            "Epoch 24/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6326 - accuracy: 0.6747\n",
            "Epoch 25/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6769 - accuracy: 0.5897\n",
            "Epoch 26/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6381 - accuracy: 0.6280\n",
            "Epoch 27/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6560 - accuracy: 0.6167\n",
            "Epoch 28/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6708 - accuracy: 0.5824\n",
            "Epoch 29/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6395 - accuracy: 0.6333\n",
            "Epoch 30/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6921 - accuracy: 0.5623\n",
            "Epoch 31/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6260 - accuracy: 0.6342\n",
            "Epoch 32/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6169 - accuracy: 0.6550\n",
            "Epoch 33/36\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6303 - accuracy: 0.6536\n",
            "Epoch 34/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6139 - accuracy: 0.6687\n",
            "Epoch 35/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6329 - accuracy: 0.6171\n",
            "Epoch 36/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5985 - accuracy: 0.6515\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6380 - accuracy: 0.5738\n",
            "Epoch 1/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 38.2109 - accuracy: 0.4967\n",
            "Epoch 2/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6999 - accuracy: 0.5209\n",
            "Epoch 3/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6953 - accuracy: 0.5457\n",
            "Epoch 4/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6962 - accuracy: 0.5366\n",
            "Epoch 5/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6903 - accuracy: 0.5514\n",
            "Epoch 6/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6593 - accuracy: 0.5800\n",
            "Epoch 7/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6614 - accuracy: 0.5600\n",
            "Epoch 8/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6787 - accuracy: 0.5288\n",
            "Epoch 9/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6626 - accuracy: 0.5613\n",
            "Epoch 10/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6629 - accuracy: 0.5400\n",
            "Epoch 11/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6518 - accuracy: 0.5555\n",
            "Epoch 12/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6516 - accuracy: 0.5712\n",
            "Epoch 13/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6501 - accuracy: 0.5692\n",
            "Epoch 14/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6616 - accuracy: 0.5927\n",
            "Epoch 15/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6808 - accuracy: 0.5310\n",
            "Epoch 16/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6431 - accuracy: 0.5697\n",
            "Epoch 17/36\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6744 - accuracy: 0.5533\n",
            "Epoch 18/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6568 - accuracy: 0.5755\n",
            "Epoch 19/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6432 - accuracy: 0.5843\n",
            "Epoch 20/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6450 - accuracy: 0.5473\n",
            "Epoch 21/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6564 - accuracy: 0.5821\n",
            "Epoch 22/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6149 - accuracy: 0.6351\n",
            "Epoch 23/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6386 - accuracy: 0.6189\n",
            "Epoch 24/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6131 - accuracy: 0.5915\n",
            "Epoch 25/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6665 - accuracy: 0.6086\n",
            "Epoch 26/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6371 - accuracy: 0.6271\n",
            "Epoch 27/36\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6369 - accuracy: 0.5827\n",
            "Epoch 28/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7581 - accuracy: 0.4613\n",
            "Epoch 29/36\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6824 - accuracy: 0.5583\n",
            "Epoch 30/36\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6668 - accuracy: 0.5651\n",
            "Epoch 31/36\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6470 - accuracy: 0.5864\n",
            "Epoch 32/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6377 - accuracy: 0.5739\n",
            "Epoch 33/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6379 - accuracy: 0.5622\n",
            "Epoch 34/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6231 - accuracy: 0.5680\n",
            "Epoch 35/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6442 - accuracy: 0.6367\n",
            "Epoch 36/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6095 - accuracy: 0.6427\n",
            "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe4b067b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.6917 - accuracy: 0.6066\n",
            "Epoch 1/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 97.7162 - accuracy: 0.4762\n",
            "Epoch 2/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6840 - accuracy: 0.5920\n",
            "Epoch 3/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6894 - accuracy: 0.6196\n",
            "Epoch 4/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6958 - accuracy: 0.5433\n",
            "Epoch 5/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6951 - accuracy: 0.5298\n",
            "Epoch 6/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6945 - accuracy: 0.5100\n",
            "Epoch 7/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6895 - accuracy: 0.5190\n",
            "Epoch 8/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6769 - accuracy: 0.5504\n",
            "Epoch 9/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6643 - accuracy: 0.5647\n",
            "Epoch 10/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6409 - accuracy: 0.6909\n",
            "Epoch 11/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6677 - accuracy: 0.6103\n",
            "Epoch 12/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6708 - accuracy: 0.6504\n",
            "Epoch 13/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6749 - accuracy: 0.5657\n",
            "Epoch 14/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6643 - accuracy: 0.6466\n",
            "Epoch 15/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6736 - accuracy: 0.5867\n",
            "Epoch 16/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6662 - accuracy: 0.6605\n",
            "Epoch 17/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6929 - accuracy: 0.5106\n",
            "Epoch 18/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6921 - accuracy: 0.5262\n",
            "Epoch 19/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6917 - accuracy: 0.5330\n",
            "Epoch 20/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6927 - accuracy: 0.5148\n",
            "Epoch 21/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6896 - accuracy: 0.5710\n",
            "Epoch 22/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6900 - accuracy: 0.5637\n",
            "Epoch 23/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6903 - accuracy: 0.5575\n",
            "Epoch 24/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6900 - accuracy: 0.5611\n",
            "Epoch 25/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6906 - accuracy: 0.5507\n",
            "Epoch 26/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.5038\n",
            "Epoch 27/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6918 - accuracy: 0.5304\n",
            "Epoch 28/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6909 - accuracy: 0.5440\n",
            "Epoch 29/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6916 - accuracy: 0.5325\n",
            "Epoch 30/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6901 - accuracy: 0.5565\n",
            "Epoch 31/36\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6913 - accuracy: 0.5377\n",
            "Epoch 32/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6905 - accuracy: 0.5502\n",
            "Epoch 33/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6913 - accuracy: 0.5372\n",
            "Epoch 34/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6897 - accuracy: 0.5611\n",
            "Epoch 35/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6921 - accuracy: 0.5247\n",
            "Epoch 36/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6919 - accuracy: 0.5273\n",
            "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe3a50400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.6875 - accuracy: 0.5902\n",
            "Epoch 1/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 66.0712 - accuracy: 0.4982\n",
            "Epoch 2/36\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.8161 - accuracy: 0.5136\n",
            "Epoch 3/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6787 - accuracy: 0.5762\n",
            "Epoch 4/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6721 - accuracy: 0.6205\n",
            "Epoch 5/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6658 - accuracy: 0.6329\n",
            "Epoch 6/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6506 - accuracy: 0.5981\n",
            "Epoch 7/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6654 - accuracy: 0.5930\n",
            "Epoch 8/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6604 - accuracy: 0.6166\n",
            "Epoch 9/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6531 - accuracy: 0.6414\n",
            "Epoch 10/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6552 - accuracy: 0.6043\n",
            "Epoch 11/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6442 - accuracy: 0.6072\n",
            "Epoch 12/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6580 - accuracy: 0.6025\n",
            "Epoch 13/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6633 - accuracy: 0.6302\n",
            "Epoch 14/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6705 - accuracy: 0.6698\n",
            "Epoch 15/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6585 - accuracy: 0.6086\n",
            "Epoch 16/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6539 - accuracy: 0.6068\n",
            "Epoch 17/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6566 - accuracy: 0.6151\n",
            "Epoch 18/36\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6568 - accuracy: 0.6066\n",
            "Epoch 19/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6694 - accuracy: 0.5988\n",
            "Epoch 20/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6558 - accuracy: 0.6119\n",
            "Epoch 21/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6527 - accuracy: 0.6502\n",
            "Epoch 22/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6326 - accuracy: 0.6237\n",
            "Epoch 23/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6534 - accuracy: 0.6317\n",
            "Epoch 24/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6621 - accuracy: 0.6157\n",
            "Epoch 25/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6601 - accuracy: 0.6214\n",
            "Epoch 26/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6586 - accuracy: 0.6268\n",
            "Epoch 27/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6483 - accuracy: 0.6165\n",
            "Epoch 28/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6507 - accuracy: 0.6283\n",
            "Epoch 29/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6473 - accuracy: 0.6647\n",
            "Epoch 30/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6458 - accuracy: 0.6260\n",
            "Epoch 31/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6583 - accuracy: 0.6058\n",
            "Epoch 32/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6474 - accuracy: 0.6478\n",
            "Epoch 33/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6483 - accuracy: 0.6401\n",
            "Epoch 34/36\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6486 - accuracy: 0.6462\n",
            "Epoch 35/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6439 - accuracy: 0.6431\n",
            "Epoch 36/36\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6490 - accuracy: 0.6330\n",
            "WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe0eff620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.7259 - accuracy: 0.6500\n",
            "Epoch 1/36\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 29.1390 - accuracy: 0.5713\n",
            "Epoch 2/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 6.2974 - accuracy: 0.4567\n",
            "Epoch 3/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6779 - accuracy: 0.5544\n",
            "Epoch 4/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6672 - accuracy: 0.6054\n",
            "Epoch 5/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6832 - accuracy: 0.5532\n",
            "Epoch 6/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6806 - accuracy: 0.5532\n",
            "Epoch 7/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6805 - accuracy: 0.5689\n",
            "Epoch 8/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6780 - accuracy: 0.5856\n",
            "Epoch 9/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6574 - accuracy: 0.6187\n",
            "Epoch 10/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6297 - accuracy: 0.6467\n",
            "Epoch 11/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6247 - accuracy: 0.6432\n",
            "Epoch 12/36\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6308 - accuracy: 0.6305\n",
            "Epoch 13/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6438 - accuracy: 0.6124\n",
            "Epoch 14/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6542 - accuracy: 0.6132\n",
            "Epoch 15/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6571 - accuracy: 0.6044\n",
            "Epoch 16/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6494 - accuracy: 0.6481\n",
            "Epoch 17/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6437 - accuracy: 0.6534\n",
            "Epoch 18/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6687 - accuracy: 0.6113\n",
            "Epoch 19/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6488 - accuracy: 0.6650\n",
            "Epoch 20/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6269 - accuracy: 0.6470\n",
            "Epoch 21/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5791 - accuracy: 0.6903\n",
            "Epoch 22/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6129 - accuracy: 0.6871\n",
            "Epoch 23/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6185 - accuracy: 0.6564\n",
            "Epoch 24/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6214 - accuracy: 0.6595\n",
            "Epoch 25/36\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6537 - accuracy: 0.6274\n",
            "Epoch 26/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6828 - accuracy: 0.6134\n",
            "Epoch 27/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6027 - accuracy: 0.6526\n",
            "Epoch 28/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6139 - accuracy: 0.6242\n",
            "Epoch 29/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5858 - accuracy: 0.6918\n",
            "Epoch 30/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5776 - accuracy: 0.7098\n",
            "Epoch 31/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6170 - accuracy: 0.6520\n",
            "Epoch 32/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5914 - accuracy: 0.6813\n",
            "Epoch 33/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5894 - accuracy: 0.6793\n",
            "Epoch 34/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6152 - accuracy: 0.6818\n",
            "Epoch 35/36\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5853 - accuracy: 0.7085\n",
            "Epoch 36/36\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6118 - accuracy: 0.6364\n",
            "WARNING:tensorflow:7 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0ff1058048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.6395 - accuracy: 0.6333\n",
            "Epoch 1/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 54.1558 - accuracy: 0.4803\n",
            "Epoch 2/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.7751 - accuracy: 0.4643\n",
            "Epoch 3/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6927 - accuracy: 0.5357\n",
            "Epoch 4/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6895 - accuracy: 0.5445\n",
            "Epoch 5/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6939 - accuracy: 0.5117\n",
            "Epoch 6/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6923 - accuracy: 0.5231\n",
            "Epoch 7/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6894 - accuracy: 0.5466\n",
            "Epoch 8/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6907 - accuracy: 0.5387\n",
            "Epoch 9/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5413\n",
            "Epoch 10/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6967 - accuracy: 0.5007\n",
            "Epoch 11/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6899 - accuracy: 0.5372\n",
            "Epoch 12/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6904 - accuracy: 0.5466\n",
            "Epoch 13/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6967 - accuracy: 0.4986\n",
            "Epoch 14/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6877 - accuracy: 0.5571\n",
            "Epoch 15/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6922 - accuracy: 0.5231\n",
            "Epoch 16/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6869 - accuracy: 0.5795\n",
            "Epoch 17/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6921 - accuracy: 0.5148\n",
            "Epoch 18/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5169\n",
            "Epoch 19/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6933 - accuracy: 0.5211\n",
            "Epoch 20/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6901 - accuracy: 0.5435\n",
            "Epoch 21/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6911 - accuracy: 0.5331\n",
            "Epoch 22/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6924 - accuracy: 0.5269\n",
            "Epoch 23/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6909 - accuracy: 0.5279\n",
            "Epoch 24/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6894 - accuracy: 0.5466\n",
            "Epoch 25/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6925 - accuracy: 0.5274\n",
            "Epoch 26/63\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6911 - accuracy: 0.5362\n",
            "Epoch 27/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6870 - accuracy: 0.5685\n",
            "Epoch 28/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6946 - accuracy: 0.5144\n",
            "Epoch 29/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6919 - accuracy: 0.5326\n",
            "Epoch 30/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5201\n",
            "Epoch 31/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6904 - accuracy: 0.5321\n",
            "Epoch 32/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6866 - accuracy: 0.5581\n",
            "Epoch 33/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6935 - accuracy: 0.5211\n",
            "Epoch 34/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6902 - accuracy: 0.5466\n",
            "Epoch 35/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6905 - accuracy: 0.5357\n",
            "Epoch 36/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6906 - accuracy: 0.5331\n",
            "Epoch 37/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6886 - accuracy: 0.5519\n",
            "Epoch 38/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6917 - accuracy: 0.5227\n",
            "Epoch 39/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6894 - accuracy: 0.5420\n",
            "Epoch 40/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6923 - accuracy: 0.5237\n",
            "Epoch 41/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6876 - accuracy: 0.5451\n",
            "Epoch 42/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5159\n",
            "Epoch 43/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6874 - accuracy: 0.5487\n",
            "Epoch 44/63\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6902 - accuracy: 0.5216\n",
            "Epoch 45/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6878 - accuracy: 0.5409\n",
            "Epoch 46/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6927 - accuracy: 0.5019\n",
            "Epoch 47/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6915 - accuracy: 0.5102\n",
            "Epoch 48/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6836 - accuracy: 0.5466\n",
            "Epoch 49/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6815 - accuracy: 0.5628\n",
            "Epoch 50/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6822 - accuracy: 0.5493\n",
            "Epoch 51/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6853 - accuracy: 0.5258\n",
            "Epoch 52/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6764 - accuracy: 0.5644\n",
            "Epoch 53/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6840 - accuracy: 0.5138\n",
            "Epoch 54/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6776 - accuracy: 0.5310\n",
            "Epoch 55/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6722 - accuracy: 0.5420\n",
            "Epoch 56/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6741 - accuracy: 0.5253\n",
            "Epoch 57/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6679 - accuracy: 0.5513\n",
            "Epoch 58/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6706 - accuracy: 0.5295\n",
            "Epoch 59/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6686 - accuracy: 0.5722\n",
            "Epoch 60/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6699 - accuracy: 0.5611\n",
            "Epoch 61/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6616 - accuracy: 0.5910\n",
            "Epoch 62/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6685 - accuracy: 0.5839\n",
            "Epoch 63/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6631 - accuracy: 0.5699\n",
            "WARNING:tensorflow:8 out of the last 14 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe90509d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.6442 - accuracy: 0.6393\n",
            "Epoch 1/63\n",
            "4/4 [==============================] - 1s 4ms/step - loss: 85.0565 - accuracy: 0.4885\n",
            "Epoch 2/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7316 - accuracy: 0.5738\n",
            "Epoch 3/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6886 - accuracy: 0.6284\n",
            "Epoch 4/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6770 - accuracy: 0.5977\n",
            "Epoch 5/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6887 - accuracy: 0.5529\n",
            "Epoch 6/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7007 - accuracy: 0.6084\n",
            "Epoch 7/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6649 - accuracy: 0.6338\n",
            "Epoch 8/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6890 - accuracy: 0.5866\n",
            "Epoch 9/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6772 - accuracy: 0.6261\n",
            "Epoch 10/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6730 - accuracy: 0.6227\n",
            "Epoch 11/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6752 - accuracy: 0.5925\n",
            "Epoch 12/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6829 - accuracy: 0.6132\n",
            "Epoch 13/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6911 - accuracy: 0.5592\n",
            "Epoch 14/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6894 - accuracy: 0.5915\n",
            "Epoch 15/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6901 - accuracy: 0.5718\n",
            "Epoch 16/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6905 - accuracy: 0.5598\n",
            "Epoch 17/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6897 - accuracy: 0.5738\n",
            "Epoch 18/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6885 - accuracy: 0.5952\n",
            "Epoch 19/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6896 - accuracy: 0.5712\n",
            "Epoch 20/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6908 - accuracy: 0.5478\n",
            "Epoch 21/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6911 - accuracy: 0.5421\n",
            "Epoch 22/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6882 - accuracy: 0.5879\n",
            "Epoch 23/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6894 - accuracy: 0.5666\n",
            "Epoch 24/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6907 - accuracy: 0.5457\n",
            "Epoch 25/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6894 - accuracy: 0.5639\n",
            "Epoch 26/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6885 - accuracy: 0.5744\n",
            "Epoch 27/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6915 - accuracy: 0.5317\n",
            "Epoch 28/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6884 - accuracy: 0.5728\n",
            "Epoch 29/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6883 - accuracy: 0.5728\n",
            "Epoch 30/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6889 - accuracy: 0.5645\n",
            "Epoch 31/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6880 - accuracy: 0.5744\n",
            "Epoch 32/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6900 - accuracy: 0.5483\n",
            "Epoch 33/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6888 - accuracy: 0.5624\n",
            "Epoch 34/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6889 - accuracy: 0.5603\n",
            "Epoch 35/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6888 - accuracy: 0.5608\n",
            "Epoch 36/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6901 - accuracy: 0.5452\n",
            "Epoch 37/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6878 - accuracy: 0.5707\n",
            "Epoch 38/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6865 - accuracy: 0.5837\n",
            "Epoch 39/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5858\n",
            "Epoch 40/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6875 - accuracy: 0.5707\n",
            "Epoch 41/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6893 - accuracy: 0.5520\n",
            "Epoch 42/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6893 - accuracy: 0.5514\n",
            "Epoch 43/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6866 - accuracy: 0.5775\n",
            "Epoch 44/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6908 - accuracy: 0.5353\n",
            "Epoch 45/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6868 - accuracy: 0.5733\n",
            "Epoch 46/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6875 - accuracy: 0.5660\n",
            "Epoch 47/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6875 - accuracy: 0.5655\n",
            "Epoch 48/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6867 - accuracy: 0.5723\n",
            "Epoch 49/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6864 - accuracy: 0.5749\n",
            "Epoch 50/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6840 - accuracy: 0.5952\n",
            "Epoch 51/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6836 - accuracy: 0.5978\n",
            "Epoch 52/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6893 - accuracy: 0.5478\n",
            "Epoch 53/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6882 - accuracy: 0.5572\n",
            "Epoch 54/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6843 - accuracy: 0.5895\n",
            "Epoch 55/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6881 - accuracy: 0.5572\n",
            "Epoch 56/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6910 - accuracy: 0.5327\n",
            "Epoch 57/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6864 - accuracy: 0.5702\n",
            "Epoch 58/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6880 - accuracy: 0.5572\n",
            "Epoch 59/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6877 - accuracy: 0.5593\n",
            "Epoch 60/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6862 - accuracy: 0.5702\n",
            "Epoch 61/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6837 - accuracy: 0.5895\n",
            "Epoch 62/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6875 - accuracy: 0.5598\n",
            "Epoch 63/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6858 - accuracy: 0.5723\n",
            "WARNING:tensorflow:8 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe90500d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.7008 - accuracy: 0.4590\n",
            "Epoch 1/63\n",
            "4/4 [==============================] - 1s 3ms/step - loss: 50.9243 - accuracy: 0.4885\n",
            "Epoch 2/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5559\n",
            "Epoch 3/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6930 - accuracy: 0.5492\n",
            "Epoch 4/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033\n",
            "Epoch 5/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5143\n",
            "Epoch 6/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5262\n",
            "Epoch 7/63\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6927 - accuracy: 0.5596\n",
            "Epoch 8/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6929 - accuracy: 0.5262\n",
            "Epoch 9/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6925 - accuracy: 0.5606\n",
            "Epoch 10/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6928 - accuracy: 0.5341\n",
            "Epoch 11/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6927 - accuracy: 0.5356\n",
            "Epoch 12/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6927 - accuracy: 0.5346\n",
            "Epoch 13/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6928 - accuracy: 0.5278\n",
            "Epoch 14/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5762\n",
            "Epoch 15/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6925 - accuracy: 0.5382\n",
            "Epoch 16/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6924 - accuracy: 0.5440\n",
            "Epoch 17/63\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6930 - accuracy: 0.5117\n",
            "Epoch 18/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6927 - accuracy: 0.5262\n",
            "Epoch 19/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6925 - accuracy: 0.5346\n",
            "Epoch 20/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6923 - accuracy: 0.5393\n",
            "Epoch 21/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.5533\n",
            "Epoch 22/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6927 - accuracy: 0.5200\n",
            "Epoch 23/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5023\n",
            "Epoch 24/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6927 - accuracy: 0.5190\n",
            "Epoch 25/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6920 - accuracy: 0.5429\n",
            "Epoch 26/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6923 - accuracy: 0.5309\n",
            "Epoch 27/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5091\n",
            "Epoch 28/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6924 - accuracy: 0.5283\n",
            "Epoch 29/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6921 - accuracy: 0.5367\n",
            "Epoch 30/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6923 - accuracy: 0.5304\n",
            "Epoch 31/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6923 - accuracy: 0.5294\n",
            "Epoch 32/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.5408\n",
            "Epoch 33/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6926 - accuracy: 0.5200\n",
            "Epoch 34/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6923 - accuracy: 0.5283\n",
            "Epoch 35/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6910 - accuracy: 0.5596\n",
            "Epoch 36/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.5372\n",
            "Epoch 37/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6918 - accuracy: 0.5393\n",
            "Epoch 38/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6918 - accuracy: 0.5367\n",
            "Epoch 39/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6916 - accuracy: 0.5413\n",
            "Epoch 40/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.5351\n",
            "Epoch 41/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6915 - accuracy: 0.5429\n",
            "Epoch 42/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6914 - accuracy: 0.5445\n",
            "Epoch 43/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6927 - accuracy: 0.5148\n",
            "Epoch 44/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6911 - accuracy: 0.5507\n",
            "Epoch 45/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6925 - accuracy: 0.5205\n",
            "Epoch 46/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6929 - accuracy: 0.5106\n",
            "Epoch 47/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6941 - accuracy: 0.4887\n",
            "Epoch 48/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6920 - accuracy: 0.5294\n",
            "Epoch 49/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6910 - accuracy: 0.5481\n",
            "Epoch 50/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5278\n",
            "Epoch 51/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6914 - accuracy: 0.5403\n",
            "Epoch 52/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6945 - accuracy: 0.4815\n",
            "Epoch 53/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6921 - accuracy: 0.5262\n",
            "Epoch 54/63\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6921 - accuracy: 0.5262\n",
            "Epoch 55/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6897 - accuracy: 0.5690\n",
            "Epoch 56/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6908 - accuracy: 0.5476\n",
            "Epoch 57/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6926 - accuracy: 0.5163\n",
            "Epoch 58/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6918 - accuracy: 0.5304\n",
            "Epoch 59/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6914 - accuracy: 0.5367\n",
            "Epoch 60/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6912 - accuracy: 0.5398\n",
            "Epoch 61/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6919 - accuracy: 0.5283\n",
            "Epoch 62/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6911 - accuracy: 0.5413\n",
            "Epoch 63/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6921 - accuracy: 0.5247\n",
            "WARNING:tensorflow:9 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fecc69f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.6880 - accuracy: 0.5902\n",
            "Epoch 1/63\n",
            "4/4 [==============================] - 1s 4ms/step - loss: 66.6200 - accuracy: 0.4170\n",
            "Epoch 2/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.8997 - accuracy: 0.5278\n",
            "Epoch 3/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7031 - accuracy: 0.5184\n",
            "Epoch 4/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6882 - accuracy: 0.5586\n",
            "Epoch 5/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6831 - accuracy: 0.5762\n",
            "Epoch 6/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6854 - accuracy: 0.5767\n",
            "Epoch 7/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6928 - accuracy: 0.5320\n",
            "Epoch 8/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6911 - accuracy: 0.5346\n",
            "Epoch 9/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6818 - accuracy: 0.5820\n",
            "Epoch 10/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.5226\n",
            "Epoch 11/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6883 - accuracy: 0.5455\n",
            "Epoch 12/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6865 - accuracy: 0.5523\n",
            "Epoch 13/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6917 - accuracy: 0.5304\n",
            "Epoch 14/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6895 - accuracy: 0.5372\n",
            "Epoch 15/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6947 - accuracy: 0.5200\n",
            "Epoch 16/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6944 - accuracy: 0.5262\n",
            "Epoch 17/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6906 - accuracy: 0.5392\n",
            "Epoch 18/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6900 - accuracy: 0.5330\n",
            "Epoch 19/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6963 - accuracy: 0.5226\n",
            "Epoch 20/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6894 - accuracy: 0.5392\n",
            "Epoch 21/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6850 - accuracy: 0.5590\n",
            "Epoch 22/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6870 - accuracy: 0.5476\n",
            "Epoch 23/63\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.6873 - accuracy: 0.5533\n",
            "Epoch 24/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6844 - accuracy: 0.5590\n",
            "Epoch 25/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6820 - accuracy: 0.5788\n",
            "Epoch 26/63\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6861 - accuracy: 0.5486\n",
            "Epoch 27/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6871 - accuracy: 0.5387\n",
            "Epoch 28/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6890 - accuracy: 0.5241\n",
            "Epoch 29/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6856 - accuracy: 0.5366\n",
            "Epoch 30/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6788 - accuracy: 0.5460\n",
            "Epoch 31/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6747 - accuracy: 0.5698\n",
            "Epoch 32/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6506 - accuracy: 0.5930\n",
            "Epoch 33/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6832 - accuracy: 0.5252\n",
            "Epoch 34/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6631 - accuracy: 0.5744\n",
            "Epoch 35/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6618 - accuracy: 0.6344\n",
            "Epoch 36/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6558 - accuracy: 0.6104\n",
            "Epoch 37/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6576 - accuracy: 0.6208\n",
            "Epoch 38/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6527 - accuracy: 0.6733\n",
            "Epoch 39/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6610 - accuracy: 0.5857\n",
            "Epoch 40/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6855 - accuracy: 0.5212\n",
            "Epoch 41/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6831 - accuracy: 0.5641\n",
            "Epoch 42/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6379 - accuracy: 0.6771\n",
            "Epoch 43/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7069 - accuracy: 0.6636\n",
            "Epoch 44/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7016 - accuracy: 0.5382\n",
            "Epoch 45/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6789 - accuracy: 0.5445\n",
            "Epoch 46/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6688 - accuracy: 0.5985\n",
            "Epoch 47/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6728 - accuracy: 0.5860\n",
            "Epoch 48/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6531 - accuracy: 0.6064\n",
            "Epoch 49/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6614 - accuracy: 0.6367\n",
            "Epoch 50/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6527 - accuracy: 0.6353\n",
            "Epoch 51/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6456 - accuracy: 0.6297\n",
            "Epoch 52/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6473 - accuracy: 0.6232\n",
            "Epoch 53/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6526 - accuracy: 0.6167\n",
            "Epoch 54/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6371 - accuracy: 0.6441\n",
            "Epoch 55/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6258 - accuracy: 0.6716\n",
            "Epoch 56/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6429 - accuracy: 0.6913\n",
            "Epoch 57/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6427 - accuracy: 0.6379\n",
            "Epoch 58/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6325 - accuracy: 0.6590\n",
            "Epoch 59/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6155 - accuracy: 0.6547\n",
            "Epoch 60/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6417 - accuracy: 0.6531\n",
            "Epoch 61/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6466 - accuracy: 0.6556\n",
            "Epoch 62/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6389 - accuracy: 0.6477\n",
            "Epoch 63/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6350 - accuracy: 0.6556\n",
            "WARNING:tensorflow:10 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe9050c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.6630 - accuracy: 0.5667\n",
            "Epoch 1/63\n",
            "4/4 [==============================] - 1s 4ms/step - loss: 42.5460 - accuracy: 0.5083\n",
            "Epoch 2/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.8349 - accuracy: 0.5429\n",
            "Epoch 3/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6928 - accuracy: 0.5458\n",
            "Epoch 4/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6930 - accuracy: 0.5214\n",
            "Epoch 5/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5287\n",
            "Epoch 6/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5224\n",
            "Epoch 7/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6925 - accuracy: 0.5573\n",
            "Epoch 8/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6927 - accuracy: 0.5370\n",
            "Epoch 9/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6923 - accuracy: 0.5552\n",
            "Epoch 10/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6926 - accuracy: 0.5370\n",
            "Epoch 11/63\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6928 - accuracy: 0.5214\n",
            "Epoch 12/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6926 - accuracy: 0.5328\n",
            "Epoch 13/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5042\n",
            "Epoch 14/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6925 - accuracy: 0.5323\n",
            "Epoch 15/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6929 - accuracy: 0.5162\n",
            "Epoch 16/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6922 - accuracy: 0.5422\n",
            "Epoch 17/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6914 - accuracy: 0.5719\n",
            "Epoch 18/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6925 - accuracy: 0.5287\n",
            "Epoch 19/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5422\n",
            "Epoch 20/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6920 - accuracy: 0.5438\n",
            "Epoch 21/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6929 - accuracy: 0.5136\n",
            "Epoch 22/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6924 - accuracy: 0.5276\n",
            "Epoch 23/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6910 - accuracy: 0.5667\n",
            "Epoch 24/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6910 - accuracy: 0.5651\n",
            "Epoch 25/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6910 - accuracy: 0.5646\n",
            "Epoch 26/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6917 - accuracy: 0.5443\n",
            "Epoch 27/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6917 - accuracy: 0.5422\n",
            "Epoch 28/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6924 - accuracy: 0.5229\n",
            "Epoch 29/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6904 - accuracy: 0.5735\n",
            "Epoch 30/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6914 - accuracy: 0.5474\n",
            "Epoch 31/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6924 - accuracy: 0.5229\n",
            "Epoch 32/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6905 - accuracy: 0.5667\n",
            "Epoch 33/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6916 - accuracy: 0.5412\n",
            "Epoch 34/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6924 - accuracy: 0.5219\n",
            "Epoch 35/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6914 - accuracy: 0.5438\n",
            "Epoch 36/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6911 - accuracy: 0.5490\n",
            "Epoch 37/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6908 - accuracy: 0.5547\n",
            "Epoch 38/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6916 - accuracy: 0.5380\n",
            "Epoch 39/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6912 - accuracy: 0.5438\n",
            "Epoch 40/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6925 - accuracy: 0.5198\n",
            "Epoch 41/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6903 - accuracy: 0.5604\n",
            "Epoch 42/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6924 - accuracy: 0.5214\n",
            "Epoch 43/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6910 - accuracy: 0.5453\n",
            "Epoch 44/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6911 - accuracy: 0.5438\n",
            "Epoch 45/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6898 - accuracy: 0.5656\n",
            "Epoch 46/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5266\n",
            "Epoch 47/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6917 - accuracy: 0.5328\n",
            "Epoch 48/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6905 - accuracy: 0.5521\n",
            "Epoch 49/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6907 - accuracy: 0.5485\n",
            "Epoch 50/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6925 - accuracy: 0.5182\n",
            "Epoch 51/63\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6913 - accuracy: 0.5370\n",
            "Epoch 52/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6896 - accuracy: 0.5636\n",
            "Epoch 53/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6922 - accuracy: 0.5235\n",
            "Epoch 54/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6918 - accuracy: 0.5297\n",
            "Epoch 55/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6924 - accuracy: 0.5198\n",
            "Epoch 56/63\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6908 - accuracy: 0.5427\n",
            "Epoch 57/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6929 - accuracy: 0.5115\n",
            "Epoch 58/63\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6926 - accuracy: 0.5172\n",
            "Epoch 59/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6908 - accuracy: 0.5417\n",
            "Epoch 60/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6909 - accuracy: 0.5406\n",
            "Epoch 61/63\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6903 - accuracy: 0.5485\n",
            "Epoch 62/63\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6916 - accuracy: 0.5313\n",
            "Epoch 63/63\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6908 - accuracy: 0.5417\n",
            "WARNING:tensorflow:11 out of the last 14 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0feb2aba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.6889 - accuracy: 0.5667\n",
            "Epoch 1/36\n",
            "5/5 [==============================] - 1s 4ms/step - loss: 80.5361 - accuracy: 0.5163\n",
            "Epoch 2/36\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.7040 - accuracy: 0.5257\n",
            "Epoch 3/36\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.6851 - accuracy: 0.5729\n",
            "Epoch 4/36\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.7173 - accuracy: 0.5833\n",
            "Epoch 5/36\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6892 - accuracy: 0.5494\n",
            "Epoch 6/36\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6906 - accuracy: 0.5379\n",
            "Epoch 7/36\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6914 - accuracy: 0.5583\n",
            "Epoch 8/36\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6914 - accuracy: 0.5513\n",
            "Epoch 9/36\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6902 - accuracy: 0.5478\n",
            "Epoch 10/36\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6911 - accuracy: 0.5322\n",
            "Epoch 11/36\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6884 - accuracy: 0.5541\n",
            "Epoch 12/36\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6915 - accuracy: 0.5287\n",
            "Epoch 13/36\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6884 - accuracy: 0.5426\n",
            "Epoch 14/36\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6901 - accuracy: 0.5583\n",
            "Epoch 15/36\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.6884 - accuracy: 0.5411\n",
            "Epoch 16/36\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6909 - accuracy: 0.5287\n",
            "Epoch 17/36\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6902 - accuracy: 0.5476\n",
            "Epoch 18/36\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.6887 - accuracy: 0.5669\n",
            "Epoch 19/36\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6911 - accuracy: 0.5326\n",
            "Epoch 20/36\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6912 - accuracy: 0.5376\n",
            "Epoch 21/36\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.6910 - accuracy: 0.5415\n",
            "Epoch 22/36\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.6873 - accuracy: 0.5537\n",
            "Epoch 23/36\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.5118\n",
            "Epoch 24/36\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6909 - accuracy: 0.5422\n",
            "Epoch 25/36\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.6897 - accuracy: 0.5472\n",
            "Epoch 26/36\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6876 - accuracy: 0.5424\n",
            "Epoch 27/36\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6900 - accuracy: 0.5487\n",
            "Epoch 28/36\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.6879 - accuracy: 0.5339\n",
            "Epoch 29/36\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.6885 - accuracy: 0.5220\n",
            "Epoch 30/36\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6878 - accuracy: 0.5632\n",
            "Epoch 31/36\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6901 - accuracy: 0.5437\n",
            "Epoch 32/36\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6901 - accuracy: 0.5415\n",
            "Epoch 33/36\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6900 - accuracy: 0.5357\n",
            "Epoch 34/36\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6892 - accuracy: 0.5478\n",
            "Epoch 35/36\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6882 - accuracy: 0.5509\n",
            "Epoch 36/36\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.6885 - accuracy: 0.5211\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi1taEWxxP67",
        "outputId": "3db8ae97-aa19-40ec-f29f-ccf4177c6202"
      },
      "source": [
        "# your grid_result object should be able to run in this code \r\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\r\n",
        "means = grid_result.cv_results_['mean_test_score']\r\n",
        "stds = grid_result.cv_results_['std_test_score']\r\n",
        "params = grid_result.cv_results_['params']\r\n",
        "for mean, stdev, param in zip(means, stds, params):\r\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.610765016078949 using {'batch_size': 64, 'epochs': 36}\n",
            "Means: 0.5774863302707672, Stdev: 0.063963275292073 with: {'batch_size': 16, 'epochs': 36}\n",
            "Means: 0.5217486321926117, Stdev: 0.06599265499776052 with: {'batch_size': 16, 'epochs': 63}\n",
            "Means: 0.610765016078949, Stdev: 0.027782785945669634 with: {'batch_size': 64, 'epochs': 36}\n",
            "Means: 0.5643715858459473, Stdev: 0.05898542795537737 with: {'batch_size': 64, 'epochs': 63}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mSEDvwa1n2T",
        "outputId": "256bd6be-b793-4a87-d4a3-6ff539fe758e"
      },
      "source": [
        "# Define my new search parameters\r\n",
        "param_grid = {\r\n",
        "    'batch_size': [16, 64],\r\n",
        "    'epochs': [33]\r\n",
        "}\r\n",
        "\r\n",
        "# Run Grid Search \r\n",
        "gs = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\r\n",
        "grid_result = gs.fit(X, Y)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 80.0527 - accuracy: 0.4818\n",
            "Epoch 2/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6915 - accuracy: 0.6016\n",
            "Epoch 3/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6926 - accuracy: 0.5454\n",
            "Epoch 4/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6920 - accuracy: 0.5703\n",
            "Epoch 5/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6936 - accuracy: 0.4896\n",
            "Epoch 6/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6932 - accuracy: 0.5003\n",
            "Epoch 7/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6903 - accuracy: 0.5887\n",
            "Epoch 8/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5096\n",
            "Epoch 9/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6904 - accuracy: 0.5693\n",
            "Epoch 10/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.5122\n",
            "Epoch 11/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6899 - accuracy: 0.5687\n",
            "Epoch 12/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6898 - accuracy: 0.5643\n",
            "Epoch 13/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6912 - accuracy: 0.5382\n",
            "Epoch 14/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6904 - accuracy: 0.5458\n",
            "Epoch 15/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6905 - accuracy: 0.5442\n",
            "Epoch 16/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6934 - accuracy: 0.5082\n",
            "Epoch 17/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6912 - accuracy: 0.5355\n",
            "Epoch 18/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6933 - accuracy: 0.5086\n",
            "Epoch 19/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6936 - accuracy: 0.5059\n",
            "Epoch 20/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6941 - accuracy: 0.5011\n",
            "Epoch 21/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6936 - accuracy: 0.5074\n",
            "Epoch 22/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6914 - accuracy: 0.5303\n",
            "Epoch 23/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6889 - accuracy: 0.5556\n",
            "Epoch 24/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6906 - accuracy: 0.5379\n",
            "Epoch 25/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6873 - accuracy: 0.5706\n",
            "Epoch 26/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6915 - accuracy: 0.5290\n",
            "Epoch 27/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6886 - accuracy: 0.5564\n",
            "Epoch 28/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6850 - accuracy: 0.5864\n",
            "Epoch 29/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6893 - accuracy: 0.5497\n",
            "Epoch 30/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6933 - accuracy: 0.5137\n",
            "Epoch 31/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6933 - accuracy: 0.5138\n",
            "Epoch 32/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6859 - accuracy: 0.5784\n",
            "Epoch 33/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6878 - accuracy: 0.5616\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe4bb9378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6864 - accuracy: 0.5738\n",
            "Epoch 1/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 21.3868 - accuracy: 0.5788\n",
            "Epoch 2/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6696 - accuracy: 0.5679\n",
            "Epoch 3/33\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6593 - accuracy: 0.6296\n",
            "Epoch 4/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6714 - accuracy: 0.5866\n",
            "Epoch 5/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6361 - accuracy: 0.6261\n",
            "Epoch 6/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6836 - accuracy: 0.5742\n",
            "Epoch 7/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6429 - accuracy: 0.6363\n",
            "Epoch 8/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6650 - accuracy: 0.6229\n",
            "Epoch 9/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6536 - accuracy: 0.6068\n",
            "Epoch 10/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6421 - accuracy: 0.6089\n",
            "Epoch 11/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6137 - accuracy: 0.6656\n",
            "Epoch 12/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6462 - accuracy: 0.6525\n",
            "Epoch 13/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6396 - accuracy: 0.6841\n",
            "Epoch 14/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6501 - accuracy: 0.6050\n",
            "Epoch 15/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6267 - accuracy: 0.5969\n",
            "Epoch 16/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6554 - accuracy: 0.6068\n",
            "Epoch 17/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6288 - accuracy: 0.6518\n",
            "Epoch 18/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6611 - accuracy: 0.5902\n",
            "Epoch 19/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7616 - accuracy: 0.5342\n",
            "Epoch 20/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6593 - accuracy: 0.6137\n",
            "Epoch 21/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6713 - accuracy: 0.5650\n",
            "Epoch 22/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6677 - accuracy: 0.5854\n",
            "Epoch 23/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5949 - accuracy: 0.7368\n",
            "Epoch 24/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6392 - accuracy: 0.6337\n",
            "Epoch 25/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7141 - accuracy: 0.5885\n",
            "Epoch 26/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6385 - accuracy: 0.6650\n",
            "Epoch 27/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6359 - accuracy: 0.6387\n",
            "Epoch 28/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6362 - accuracy: 0.6270\n",
            "Epoch 29/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5930 - accuracy: 0.7059\n",
            "Epoch 30/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6542 - accuracy: 0.6139\n",
            "Epoch 31/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7498 - accuracy: 0.5477\n",
            "Epoch 32/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5955 - accuracy: 0.6851\n",
            "Epoch 33/33\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6307 - accuracy: 0.6792\n",
            "WARNING:tensorflow:8 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe3a546a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.9274 - accuracy: 0.4590\n",
            "Epoch 1/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 28.1397 - accuracy: 0.5613\n",
            "Epoch 2/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6739 - accuracy: 0.6096\n",
            "Epoch 3/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6744 - accuracy: 0.6012\n",
            "Epoch 4/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6373 - accuracy: 0.6514\n",
            "Epoch 5/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6817 - accuracy: 0.6188\n",
            "Epoch 6/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6697 - accuracy: 0.6451\n",
            "Epoch 7/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6679 - accuracy: 0.5633\n",
            "Epoch 8/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6890 - accuracy: 0.5850\n",
            "Epoch 9/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6721 - accuracy: 0.6148\n",
            "Epoch 10/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6549 - accuracy: 0.6092\n",
            "Epoch 11/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6786 - accuracy: 0.6036\n",
            "Epoch 12/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6612 - accuracy: 0.6540\n",
            "Epoch 13/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6408 - accuracy: 0.6424\n",
            "Epoch 14/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6556 - accuracy: 0.6154\n",
            "Epoch 15/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6864 - accuracy: 0.5328\n",
            "Epoch 16/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6668 - accuracy: 0.6298\n",
            "Epoch 17/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6521 - accuracy: 0.6419\n",
            "Epoch 18/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6153 - accuracy: 0.6977\n",
            "Epoch 19/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6361 - accuracy: 0.6717\n",
            "Epoch 20/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6155 - accuracy: 0.6535\n",
            "Epoch 21/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6954 - accuracy: 0.6481\n",
            "Epoch 22/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6509 - accuracy: 0.5904\n",
            "Epoch 23/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6830 - accuracy: 0.5250\n",
            "Epoch 24/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6269 - accuracy: 0.6383\n",
            "Epoch 25/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6330 - accuracy: 0.6100\n",
            "Epoch 26/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6790 - accuracy: 0.5916\n",
            "Epoch 27/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7020 - accuracy: 0.6440\n",
            "Epoch 28/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6268 - accuracy: 0.6981\n",
            "Epoch 29/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6609 - accuracy: 0.5848\n",
            "Epoch 30/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6456 - accuracy: 0.5939\n",
            "Epoch 31/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6395 - accuracy: 0.6318\n",
            "Epoch 32/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6163 - accuracy: 0.7212\n",
            "Epoch 33/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6069 - accuracy: 0.6935\n",
            "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe5cd1ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7369 - accuracy: 0.6066\n",
            "Epoch 1/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 11.4289 - accuracy: 0.5169\n",
            "Epoch 2/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6512 - accuracy: 0.5719\n",
            "Epoch 3/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6683 - accuracy: 0.6215\n",
            "Epoch 4/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6258 - accuracy: 0.6198\n",
            "Epoch 5/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6554 - accuracy: 0.6407\n",
            "Epoch 6/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7384 - accuracy: 0.5972\n",
            "Epoch 7/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6241 - accuracy: 0.6354\n",
            "Epoch 8/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6864 - accuracy: 0.6115\n",
            "Epoch 9/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6666 - accuracy: 0.6224\n",
            "Epoch 10/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6192 - accuracy: 0.6085\n",
            "Epoch 11/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6657 - accuracy: 0.6128\n",
            "Epoch 12/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.6499\n",
            "Epoch 13/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6428 - accuracy: 0.6271\n",
            "Epoch 14/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6036 - accuracy: 0.6932\n",
            "Epoch 15/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6193 - accuracy: 0.6065\n",
            "Epoch 16/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6542 - accuracy: 0.6470\n",
            "Epoch 17/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6097 - accuracy: 0.6768\n",
            "Epoch 18/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6123 - accuracy: 0.6279\n",
            "Epoch 19/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7065 - accuracy: 0.5912\n",
            "Epoch 20/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6530 - accuracy: 0.6510\n",
            "Epoch 21/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5921 - accuracy: 0.6592\n",
            "Epoch 22/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5911 - accuracy: 0.6854\n",
            "Epoch 23/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6048 - accuracy: 0.6341\n",
            "Epoch 24/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6140 - accuracy: 0.6596\n",
            "Epoch 25/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4978 - accuracy: 0.7620\n",
            "Epoch 26/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6194 - accuracy: 0.6537\n",
            "Epoch 27/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6237 - accuracy: 0.6273\n",
            "Epoch 28/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5862 - accuracy: 0.6730\n",
            "Epoch 29/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6323 - accuracy: 0.6452\n",
            "Epoch 30/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5979 - accuracy: 0.6824\n",
            "Epoch 31/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6868 - accuracy: 0.6346\n",
            "Epoch 32/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5819 - accuracy: 0.6635\n",
            "Epoch 33/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5891 - accuracy: 0.6414\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.7377 - accuracy: 0.5667\n",
            "Epoch 1/33\n",
            "16/16 [==============================] - 1s 2ms/step - loss: 74.2072 - accuracy: 0.5265\n",
            "Epoch 2/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7151 - accuracy: 0.4928\n",
            "Epoch 3/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7031 - accuracy: 0.5486\n",
            "Epoch 4/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6924 - accuracy: 0.5631\n",
            "Epoch 5/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6872 - accuracy: 0.6607\n",
            "Epoch 6/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6828 - accuracy: 0.5924\n",
            "Epoch 7/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6874 - accuracy: 0.5810\n",
            "Epoch 8/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6742 - accuracy: 0.5929\n",
            "Epoch 9/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6712 - accuracy: 0.5969\n",
            "Epoch 10/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6810 - accuracy: 0.5854\n",
            "Epoch 11/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6835 - accuracy: 0.5795\n",
            "Epoch 12/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7379 - accuracy: 0.5524\n",
            "Epoch 13/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6773 - accuracy: 0.5936\n",
            "Epoch 14/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6738 - accuracy: 0.6116\n",
            "Epoch 15/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7115 - accuracy: 0.5650\n",
            "Epoch 16/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6811 - accuracy: 0.6381\n",
            "Epoch 17/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6721 - accuracy: 0.6093\n",
            "Epoch 18/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6775 - accuracy: 0.6234\n",
            "Epoch 19/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6726 - accuracy: 0.5764\n",
            "Epoch 20/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6652 - accuracy: 0.6195\n",
            "Epoch 21/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6583 - accuracy: 0.6581\n",
            "Epoch 22/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6596 - accuracy: 0.5849\n",
            "Epoch 23/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6662 - accuracy: 0.6334\n",
            "Epoch 24/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6816 - accuracy: 0.5408\n",
            "Epoch 25/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6755 - accuracy: 0.5736\n",
            "Epoch 26/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7085 - accuracy: 0.5853\n",
            "Epoch 27/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6869 - accuracy: 0.5814\n",
            "Epoch 28/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6724 - accuracy: 0.5881\n",
            "Epoch 29/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6580 - accuracy: 0.6423\n",
            "Epoch 30/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6586 - accuracy: 0.6086\n",
            "Epoch 31/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6750 - accuracy: 0.5783\n",
            "Epoch 32/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6855 - accuracy: 0.5604\n",
            "Epoch 33/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6565 - accuracy: 0.5952\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6621 - accuracy: 0.5833\n",
            "Epoch 1/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 19.4003 - accuracy: 0.5194\n",
            "Epoch 2/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6920 - accuracy: 0.5531\n",
            "Epoch 3/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6869 - accuracy: 0.5748\n",
            "Epoch 4/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6807 - accuracy: 0.5491\n",
            "Epoch 5/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6520 - accuracy: 0.5348\n",
            "Epoch 6/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6633 - accuracy: 0.5396\n",
            "Epoch 7/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6361 - accuracy: 0.5703\n",
            "Epoch 8/33\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6449 - accuracy: 0.6196\n",
            "Epoch 9/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6418 - accuracy: 0.6165\n",
            "Epoch 10/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6304 - accuracy: 0.5964\n",
            "Epoch 11/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6384 - accuracy: 0.6421\n",
            "Epoch 12/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6232 - accuracy: 0.6653\n",
            "Epoch 13/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6262 - accuracy: 0.6536\n",
            "Epoch 14/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6262 - accuracy: 0.6336\n",
            "Epoch 15/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6184 - accuracy: 0.6559\n",
            "Epoch 16/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6252 - accuracy: 0.6842\n",
            "Epoch 17/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6417 - accuracy: 0.6205\n",
            "Epoch 18/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6070 - accuracy: 0.6587\n",
            "Epoch 19/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6197 - accuracy: 0.6115\n",
            "Epoch 20/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6333 - accuracy: 0.6474\n",
            "Epoch 21/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6337 - accuracy: 0.6296\n",
            "Epoch 22/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6894 - accuracy: 0.6283\n",
            "Epoch 23/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6161 - accuracy: 0.6705\n",
            "Epoch 24/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6141 - accuracy: 0.6419\n",
            "Epoch 25/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5841 - accuracy: 0.7355\n",
            "Epoch 26/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6027 - accuracy: 0.6640\n",
            "Epoch 27/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6144 - accuracy: 0.6708\n",
            "Epoch 28/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6041 - accuracy: 0.6680\n",
            "Epoch 29/33\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6395 - accuracy: 0.6424\n",
            "Epoch 30/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6132 - accuracy: 0.6694\n",
            "Epoch 31/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6275 - accuracy: 0.6470\n",
            "Epoch 32/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6288 - accuracy: 0.6410\n",
            "Epoch 33/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6206 - accuracy: 0.6466\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.6388 - accuracy: 0.6393\n",
            "Epoch 1/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 91.4001 - accuracy: 0.5285\n",
            "Epoch 2/33\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6632 - accuracy: 0.5864\n",
            "Epoch 3/33\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6645 - accuracy: 0.5649\n",
            "Epoch 4/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.9065 - accuracy: 0.5818\n",
            "Epoch 5/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7001 - accuracy: 0.5427\n",
            "Epoch 6/33\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6925 - accuracy: 0.5128\n",
            "Epoch 7/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6922 - accuracy: 0.5326\n",
            "Epoch 8/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6910 - accuracy: 0.5447\n",
            "Epoch 9/33\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6924 - accuracy: 0.5774\n",
            "Epoch 10/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5560\n",
            "Epoch 11/33\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6901 - accuracy: 0.5701\n",
            "Epoch 12/33\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6909 - accuracy: 0.5389\n",
            "Epoch 13/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6910 - accuracy: 0.5723\n",
            "Epoch 14/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6914 - accuracy: 0.5723\n",
            "Epoch 15/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6905 - accuracy: 0.5384\n",
            "Epoch 16/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6891 - accuracy: 0.5936\n",
            "Epoch 17/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6913 - accuracy: 0.5553\n",
            "Epoch 18/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6915 - accuracy: 0.5634\n",
            "Epoch 19/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6910 - accuracy: 0.5707\n",
            "Epoch 20/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6909 - accuracy: 0.5759\n",
            "Epoch 21/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6895 - accuracy: 0.5791\n",
            "Epoch 22/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6909 - accuracy: 0.5317\n",
            "Epoch 23/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6918 - accuracy: 0.5214\n",
            "Epoch 24/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6936 - accuracy: 0.5297\n",
            "Epoch 25/33\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6908 - accuracy: 0.5666\n",
            "Epoch 26/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6865 - accuracy: 0.5983\n",
            "Epoch 27/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6887 - accuracy: 0.5497\n",
            "Epoch 28/33\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6900 - accuracy: 0.5387\n",
            "Epoch 29/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6890 - accuracy: 0.5759\n",
            "Epoch 30/33\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6897 - accuracy: 0.5863\n",
            "Epoch 31/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6910 - accuracy: 0.5276\n",
            "Epoch 32/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6901 - accuracy: 0.5424\n",
            "Epoch 33/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6881 - accuracy: 0.5446\n",
            "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe5c502f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.6954 - accuracy: 0.4590\n",
            "Epoch 1/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 112.3386 - accuracy: 0.4745\n",
            "Epoch 2/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.7161 - accuracy: 0.3979\n",
            "Epoch 3/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6964 - accuracy: 0.3799\n",
            "Epoch 4/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6889 - accuracy: 0.6541\n",
            "Epoch 5/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6812 - accuracy: 0.6295\n",
            "Epoch 6/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6771 - accuracy: 0.6414\n",
            "Epoch 7/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6736 - accuracy: 0.6429\n",
            "Epoch 8/33\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6752 - accuracy: 0.5896\n",
            "Epoch 9/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6703 - accuracy: 0.6209\n",
            "Epoch 10/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6652 - accuracy: 0.6764\n",
            "Epoch 11/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6590 - accuracy: 0.6536\n",
            "Epoch 12/33\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6585 - accuracy: 0.6747\n",
            "Epoch 13/33\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6608 - accuracy: 0.7135\n",
            "Epoch 14/33\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6667 - accuracy: 0.6342\n",
            "Epoch 15/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6574 - accuracy: 0.6390\n",
            "Epoch 16/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6567 - accuracy: 0.6818\n",
            "Epoch 17/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6714 - accuracy: 0.6223\n",
            "Epoch 18/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6641 - accuracy: 0.6309\n",
            "Epoch 19/33\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6503 - accuracy: 0.6586\n",
            "Epoch 20/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6538 - accuracy: 0.6598\n",
            "Epoch 21/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6513 - accuracy: 0.6741\n",
            "Epoch 22/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6586 - accuracy: 0.6369\n",
            "Epoch 23/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6553 - accuracy: 0.6540\n",
            "Epoch 24/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6582 - accuracy: 0.6890\n",
            "Epoch 25/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6431 - accuracy: 0.6579\n",
            "Epoch 26/33\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6629 - accuracy: 0.6238\n",
            "Epoch 27/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6509 - accuracy: 0.6618\n",
            "Epoch 28/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6507 - accuracy: 0.6463\n",
            "Epoch 29/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6528 - accuracy: 0.6593\n",
            "Epoch 30/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6471 - accuracy: 0.6626\n",
            "Epoch 31/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6569 - accuracy: 0.6412\n",
            "Epoch 32/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6352 - accuracy: 0.7235\n",
            "Epoch 33/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6576 - accuracy: 0.6422\n",
            "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0ff105f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.6704 - accuracy: 0.6885\n",
            "Epoch 1/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 165.6977 - accuracy: 0.4610\n",
            "Epoch 2/33\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.9990 - accuracy: 0.5580\n",
            "Epoch 3/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.5909 - accuracy: 0.6925\n",
            "Epoch 4/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6243 - accuracy: 0.6370\n",
            "Epoch 5/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.5941 - accuracy: 0.6871\n",
            "Epoch 6/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6488 - accuracy: 0.6472\n",
            "Epoch 7/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6349 - accuracy: 0.6348\n",
            "Epoch 8/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.5982 - accuracy: 0.6883\n",
            "Epoch 9/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.5835 - accuracy: 0.6924\n",
            "Epoch 10/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.5715 - accuracy: 0.6952\n",
            "Epoch 11/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6421 - accuracy: 0.6372\n",
            "Epoch 12/33\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6489 - accuracy: 0.6375\n",
            "Epoch 13/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5971 - accuracy: 0.6831\n",
            "Epoch 14/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6064 - accuracy: 0.6988\n",
            "Epoch 15/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5863 - accuracy: 0.6980\n",
            "Epoch 16/33\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6448 - accuracy: 0.6604\n",
            "Epoch 17/33\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5875 - accuracy: 0.6976\n",
            "Epoch 18/33\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6074 - accuracy: 0.6635\n",
            "Epoch 19/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6541 - accuracy: 0.5941\n",
            "Epoch 20/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5918 - accuracy: 0.6768\n",
            "Epoch 21/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6042 - accuracy: 0.6732\n",
            "Epoch 22/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5762 - accuracy: 0.6993\n",
            "Epoch 23/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5693 - accuracy: 0.6901\n",
            "Epoch 24/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6178 - accuracy: 0.6392\n",
            "Epoch 25/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6149 - accuracy: 0.6443\n",
            "Epoch 26/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5895 - accuracy: 0.6903\n",
            "Epoch 27/33\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6086 - accuracy: 0.6831\n",
            "Epoch 28/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6068 - accuracy: 0.6616\n",
            "Epoch 29/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6144 - accuracy: 0.6379\n",
            "Epoch 30/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6056 - accuracy: 0.6749\n",
            "Epoch 31/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5789 - accuracy: 0.6881\n",
            "Epoch 32/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5980 - accuracy: 0.6809\n",
            "Epoch 33/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5891 - accuracy: 0.6469\n",
            "WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0ff1058a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.6319 - accuracy: 0.6833\n",
            "Epoch 1/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 64.2406 - accuracy: 0.5065\n",
            "Epoch 2/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.2604 - accuracy: 0.4786\n",
            "Epoch 3/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6990 - accuracy: 0.4856\n",
            "Epoch 4/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6945 - accuracy: 0.4622\n",
            "Epoch 5/33\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6950 - accuracy: 0.5094\n",
            "Epoch 6/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6957 - accuracy: 0.4780\n",
            "Epoch 7/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6936 - accuracy: 0.5362\n",
            "Epoch 8/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6945 - accuracy: 0.4865\n",
            "Epoch 9/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.4966\n",
            "Epoch 10/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6924 - accuracy: 0.5419\n",
            "Epoch 11/33\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6914 - accuracy: 0.5327\n",
            "Epoch 12/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6928 - accuracy: 0.5411\n",
            "Epoch 13/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6935 - accuracy: 0.5012\n",
            "Epoch 14/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6921 - accuracy: 0.5148\n",
            "Epoch 15/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.5321\n",
            "Epoch 16/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6908 - accuracy: 0.5229\n",
            "Epoch 17/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6904 - accuracy: 0.5465\n",
            "Epoch 18/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6917 - accuracy: 0.5281\n",
            "Epoch 19/33\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6900 - accuracy: 0.5557\n",
            "Epoch 20/33\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6894 - accuracy: 0.5583\n",
            "Epoch 21/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6922 - accuracy: 0.5214\n",
            "Epoch 22/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6869 - accuracy: 0.5662\n",
            "Epoch 23/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6899 - accuracy: 0.5302\n",
            "Epoch 24/33\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6945 - accuracy: 0.5037\n",
            "Epoch 25/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5479\n",
            "Epoch 26/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6897 - accuracy: 0.5276\n",
            "Epoch 27/33\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6867 - accuracy: 0.5469\n",
            "Epoch 28/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6852 - accuracy: 0.5365\n",
            "Epoch 29/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6838 - accuracy: 0.5542\n",
            "Epoch 30/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6843 - accuracy: 0.5469\n",
            "Epoch 31/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6878 - accuracy: 0.5136\n",
            "Epoch 32/33\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6903 - accuracy: 0.5037\n",
            "Epoch 33/33\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6774 - accuracy: 0.5313\n",
            "WARNING:tensorflow:7 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe7f73ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 0.6672 - accuracy: 0.6667\n",
            "Epoch 1/33\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 34.8360 - accuracy: 0.5307\n",
            "Epoch 2/33\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6979 - accuracy: 0.5510\n",
            "Epoch 3/33\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6901 - accuracy: 0.6400\n",
            "Epoch 4/33\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6849 - accuracy: 0.6689\n",
            "Epoch 5/33\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.6787 - accuracy: 0.6409\n",
            "Epoch 6/33\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.6637 - accuracy: 0.6264\n",
            "Epoch 7/33\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.6806 - accuracy: 0.6347\n",
            "Epoch 8/33\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6791 - accuracy: 0.5952\n",
            "Epoch 9/33\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6632 - accuracy: 0.6757\n",
            "Epoch 10/33\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6702 - accuracy: 0.6563\n",
            "Epoch 11/33\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6735 - accuracy: 0.6301\n",
            "Epoch 12/33\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6721 - accuracy: 0.6987\n",
            "Epoch 13/33\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6934 - accuracy: 0.5816\n",
            "Epoch 14/33\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6592 - accuracy: 0.6658\n",
            "Epoch 15/33\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.6580 - accuracy: 0.6515\n",
            "Epoch 16/33\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6489 - accuracy: 0.6739\n",
            "Epoch 17/33\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6692 - accuracy: 0.6531\n",
            "Epoch 18/33\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6665 - accuracy: 0.6506\n",
            "Epoch 19/33\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.6503 - accuracy: 0.6736\n",
            "Epoch 20/33\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6881 - accuracy: 0.5266\n",
            "Epoch 21/33\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.6886 - accuracy: 0.5179\n",
            "Epoch 22/33\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6872 - accuracy: 0.5623\n",
            "Epoch 23/33\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6516 - accuracy: 0.6684\n",
            "Epoch 24/33\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6572 - accuracy: 0.6479\n",
            "Epoch 25/33\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.6609 - accuracy: 0.6562\n",
            "Epoch 26/33\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6516 - accuracy: 0.6814\n",
            "Epoch 27/33\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6635 - accuracy: 0.5977\n",
            "Epoch 28/33\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6491 - accuracy: 0.6765\n",
            "Epoch 29/33\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6709 - accuracy: 0.6233\n",
            "Epoch 30/33\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6512 - accuracy: 0.6745\n",
            "Epoch 31/33\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6742 - accuracy: 0.6378\n",
            "Epoch 32/33\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6881 - accuracy: 0.5261\n",
            "Epoch 33/33\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6888 - accuracy: 0.5389\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBF6hLpu1yjj",
        "outputId": "0c83ef42-3c87-42ac-9abc-d81a7725abfd"
      },
      "source": [
        "# your grid_result object should be able to run in this code \r\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\r\n",
        "means = grid_result.cv_results_['mean_test_score']\r\n",
        "stds = grid_result.cv_results_['std_test_score']\r\n",
        "params = grid_result.cv_results_['params']\r\n",
        "for mean, stdev, param in zip(means, stds, params):\r\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.6273770570755005 using {'batch_size': 64, 'epochs': 33}\n",
            "Means: 0.5578688383102417, Stdev: 0.0512271098463448 with: {'batch_size': 16, 'epochs': 33}\n",
            "Means: 0.6273770570755005, Stdev: 0.08590964292682834 with: {'batch_size': 64, 'epochs': 33}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juG9V9SY19Dt",
        "outputId": "ed90b62b-4a85-4e37-e88b-129790947872"
      },
      "source": [
        "# Define my new search parameters\r\n",
        "param_grid = {\r\n",
        "    'batch_size': [16, 32],\r\n",
        "    'epochs': [33, 63]\r\n",
        "}\r\n",
        "\r\n",
        "# Run Grid Search \r\n",
        "gs = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\r\n",
        "grid_result = gs.fit(X, Y)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 31.3385 - accuracy: 0.5098\n",
            "Epoch 2/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6993 - accuracy: 0.4724\n",
            "Epoch 3/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6945 - accuracy: 0.4799\n",
            "Epoch 4/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6934 - accuracy: 0.4940\n",
            "Epoch 5/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6926 - accuracy: 0.5420\n",
            "Epoch 6/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6919 - accuracy: 0.5202\n",
            "Epoch 7/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6960 - accuracy: 0.4710\n",
            "Epoch 8/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6914 - accuracy: 0.5105\n",
            "Epoch 9/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6883 - accuracy: 0.5777\n",
            "Epoch 10/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6921 - accuracy: 0.5071\n",
            "Epoch 11/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6921 - accuracy: 0.4912\n",
            "Epoch 12/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6920 - accuracy: 0.5443\n",
            "Epoch 13/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6906 - accuracy: 0.5837\n",
            "Epoch 14/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6900 - accuracy: 0.5643\n",
            "Epoch 15/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5158\n",
            "Epoch 16/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6898 - accuracy: 0.5376\n",
            "Epoch 17/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6879 - accuracy: 0.5283\n",
            "Epoch 18/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6908 - accuracy: 0.5183\n",
            "Epoch 19/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6904 - accuracy: 0.5640\n",
            "Epoch 20/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6865 - accuracy: 0.5609\n",
            "Epoch 21/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6904 - accuracy: 0.5619\n",
            "Epoch 22/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6892 - accuracy: 0.5753\n",
            "Epoch 23/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6883 - accuracy: 0.5880\n",
            "Epoch 24/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6820 - accuracy: 0.5564\n",
            "Epoch 25/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6872 - accuracy: 0.5716\n",
            "Epoch 26/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6906 - accuracy: 0.5400\n",
            "Epoch 27/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6886 - accuracy: 0.5807\n",
            "Epoch 28/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6893 - accuracy: 0.4962\n",
            "Epoch 29/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6874 - accuracy: 0.6262\n",
            "Epoch 30/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6810 - accuracy: 0.5899\n",
            "Epoch 31/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6824 - accuracy: 0.5759\n",
            "Epoch 32/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6804 - accuracy: 0.5981\n",
            "Epoch 33/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6811 - accuracy: 0.5926\n",
            "WARNING:tensorflow:8 out of the last 14 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe7f73bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6796 - accuracy: 0.7049\n",
            "Epoch 1/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 68.8008 - accuracy: 0.4896\n",
            "Epoch 2/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6916 - accuracy: 0.5765\n",
            "Epoch 3/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6897 - accuracy: 0.5886\n",
            "Epoch 4/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6874 - accuracy: 0.5671\n",
            "Epoch 5/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6866 - accuracy: 0.5812\n",
            "Epoch 6/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6896 - accuracy: 0.5509\n",
            "Epoch 7/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6915 - accuracy: 0.5118\n",
            "Epoch 8/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6953 - accuracy: 0.5625\n",
            "Epoch 9/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6856 - accuracy: 0.5803\n",
            "Epoch 10/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6794 - accuracy: 0.5728\n",
            "Epoch 11/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6915 - accuracy: 0.5210\n",
            "Epoch 12/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6916 - accuracy: 0.5157\n",
            "Epoch 13/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6777 - accuracy: 0.6163\n",
            "Epoch 14/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6835 - accuracy: 0.5883\n",
            "Epoch 15/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6870 - accuracy: 0.5627\n",
            "Epoch 16/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6924 - accuracy: 0.5140\n",
            "Epoch 17/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6828 - accuracy: 0.5743\n",
            "Epoch 18/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6857 - accuracy: 0.5646\n",
            "Epoch 19/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6862 - accuracy: 0.5603\n",
            "Epoch 20/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6866 - accuracy: 0.5588\n",
            "Epoch 21/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6747 - accuracy: 0.5990\n",
            "Epoch 22/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6954 - accuracy: 0.4952\n",
            "Epoch 23/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6824 - accuracy: 0.5717\n",
            "Epoch 24/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6804 - accuracy: 0.6003\n",
            "Epoch 25/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6836 - accuracy: 0.5709\n",
            "Epoch 26/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6853 - accuracy: 0.5623\n",
            "Epoch 27/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6850 - accuracy: 0.5617\n",
            "Epoch 28/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6810 - accuracy: 0.5858\n",
            "Epoch 29/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6832 - accuracy: 0.5699\n",
            "Epoch 30/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6919 - accuracy: 0.5245\n",
            "Epoch 31/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6838 - accuracy: 0.5592\n",
            "Epoch 32/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6806 - accuracy: 0.5772\n",
            "Epoch 33/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6772 - accuracy: 0.5988\n",
            "WARNING:tensorflow:8 out of the last 14 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe9063c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7188 - accuracy: 0.4590\n",
            "Epoch 1/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 32.9740 - accuracy: 0.4787\n",
            "Epoch 2/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7174 - accuracy: 0.5407\n",
            "Epoch 3/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6948 - accuracy: 0.5342\n",
            "Epoch 4/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6921 - accuracy: 0.5495\n",
            "Epoch 5/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6834 - accuracy: 0.6185\n",
            "Epoch 6/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7550 - accuracy: 0.5527\n",
            "Epoch 7/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6708 - accuracy: 0.6396\n",
            "Epoch 8/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6754 - accuracy: 0.6113\n",
            "Epoch 9/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6910 - accuracy: 0.5423\n",
            "Epoch 10/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6938 - accuracy: 0.4889\n",
            "Epoch 11/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6603 - accuracy: 0.7221\n",
            "Epoch 12/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7788 - accuracy: 0.5902\n",
            "Epoch 13/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6684 - accuracy: 0.6381\n",
            "Epoch 14/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6779 - accuracy: 0.6102\n",
            "Epoch 15/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6675 - accuracy: 0.6085\n",
            "Epoch 16/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6627 - accuracy: 0.6106\n",
            "Epoch 17/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6659 - accuracy: 0.5886\n",
            "Epoch 18/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6556 - accuracy: 0.6400\n",
            "Epoch 19/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6874 - accuracy: 0.5494\n",
            "Epoch 20/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7771 - accuracy: 0.5748\n",
            "Epoch 21/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6626 - accuracy: 0.5912\n",
            "Epoch 22/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6608 - accuracy: 0.6693\n",
            "Epoch 23/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7303 - accuracy: 0.5707\n",
            "Epoch 24/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6531 - accuracy: 0.6334\n",
            "Epoch 25/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6597 - accuracy: 0.6191\n",
            "Epoch 26/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6681 - accuracy: 0.5726\n",
            "Epoch 27/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6512 - accuracy: 0.6368\n",
            "Epoch 28/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6395 - accuracy: 0.6754\n",
            "Epoch 29/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6496 - accuracy: 0.6327\n",
            "Epoch 30/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6546 - accuracy: 0.6576\n",
            "Epoch 31/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6782 - accuracy: 0.5725\n",
            "Epoch 32/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6287 - accuracy: 0.6418\n",
            "Epoch 33/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6518 - accuracy: 0.6048\n",
            "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe90630d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.1621 - accuracy: 0.4262\n",
            "Epoch 1/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 33.5019 - accuracy: 0.4978\n",
            "Epoch 2/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7017 - accuracy: 0.5327\n",
            "Epoch 3/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6929 - accuracy: 0.5405\n",
            "Epoch 4/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6903 - accuracy: 0.5360\n",
            "Epoch 5/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6755 - accuracy: 0.5896\n",
            "Epoch 6/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6890 - accuracy: 0.5350\n",
            "Epoch 7/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6674 - accuracy: 0.5773\n",
            "Epoch 8/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6851 - accuracy: 0.5572\n",
            "Epoch 9/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6751 - accuracy: 0.5112\n",
            "Epoch 10/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6913 - accuracy: 0.5448\n",
            "Epoch 11/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6782 - accuracy: 0.5359\n",
            "Epoch 12/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7440 - accuracy: 0.3992\n",
            "Epoch 13/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7034 - accuracy: 0.4641\n",
            "Epoch 14/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6945 - accuracy: 0.5373\n",
            "Epoch 15/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6857 - accuracy: 0.5679\n",
            "Epoch 16/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6861 - accuracy: 0.6070\n",
            "Epoch 17/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6879 - accuracy: 0.5371\n",
            "Epoch 18/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6841 - accuracy: 0.6134\n",
            "Epoch 19/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6877 - accuracy: 0.5551\n",
            "Epoch 20/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6817 - accuracy: 0.5556\n",
            "Epoch 21/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6883 - accuracy: 0.5256\n",
            "Epoch 22/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6877 - accuracy: 0.5478\n",
            "Epoch 23/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6833 - accuracy: 0.5810\n",
            "Epoch 24/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6866 - accuracy: 0.5727\n",
            "Epoch 25/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6829 - accuracy: 0.5419\n",
            "Epoch 26/33\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6816 - accuracy: 0.5512\n",
            "Epoch 27/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6915 - accuracy: 0.5456\n",
            "Epoch 28/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6781 - accuracy: 0.5972\n",
            "Epoch 29/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6798 - accuracy: 0.5995\n",
            "Epoch 30/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6820 - accuracy: 0.5602\n",
            "Epoch 31/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6880 - accuracy: 0.5425\n",
            "Epoch 32/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6746 - accuracy: 0.5999\n",
            "Epoch 33/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6829 - accuracy: 0.5579\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.7801 - accuracy: 0.4833\n",
            "Epoch 1/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 38.0827 - accuracy: 0.4766\n",
            "Epoch 2/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6862 - accuracy: 0.5750\n",
            "Epoch 3/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6900 - accuracy: 0.5837\n",
            "Epoch 4/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6913 - accuracy: 0.5080\n",
            "Epoch 5/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6916 - accuracy: 0.5338\n",
            "Epoch 6/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6871 - accuracy: 0.5843\n",
            "Epoch 7/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6906 - accuracy: 0.5284\n",
            "Epoch 8/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6904 - accuracy: 0.5449\n",
            "Epoch 9/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6828 - accuracy: 0.5451\n",
            "Epoch 10/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6901 - accuracy: 0.5404\n",
            "Epoch 11/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6899 - accuracy: 0.5509\n",
            "Epoch 12/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6907 - accuracy: 0.5395\n",
            "Epoch 13/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6867 - accuracy: 0.5780\n",
            "Epoch 14/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6902 - accuracy: 0.5053\n",
            "Epoch 15/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6879 - accuracy: 0.5491\n",
            "Epoch 16/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6876 - accuracy: 0.5527\n",
            "Epoch 17/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6853 - accuracy: 0.5603\n",
            "Epoch 18/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6908 - accuracy: 0.5285\n",
            "Epoch 19/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6879 - accuracy: 0.5531\n",
            "Epoch 20/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6883 - accuracy: 0.5433\n",
            "Epoch 21/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6871 - accuracy: 0.5570\n",
            "Epoch 22/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6922 - accuracy: 0.5166\n",
            "Epoch 23/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6793 - accuracy: 0.5792\n",
            "Epoch 24/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6823 - accuracy: 0.5895\n",
            "Epoch 25/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6799 - accuracy: 0.5850\n",
            "Epoch 26/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6788 - accuracy: 0.5886\n",
            "Epoch 27/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6841 - accuracy: 0.5847\n",
            "Epoch 28/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6770 - accuracy: 0.6065\n",
            "Epoch 29/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6887 - accuracy: 0.5549\n",
            "Epoch 30/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6803 - accuracy: 0.5745\n",
            "Epoch 31/33\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6808 - accuracy: 0.5526\n",
            "Epoch 32/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6783 - accuracy: 0.5956\n",
            "Epoch 33/33\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6914 - accuracy: 0.4891\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.7016 - accuracy: 0.4833\n",
            "Epoch 1/63\n",
            "16/16 [==============================] - 1s 2ms/step - loss: 8.4860 - accuracy: 0.5841\n",
            "Epoch 2/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6863 - accuracy: 0.6578\n",
            "Epoch 3/63\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6925 - accuracy: 0.5371\n",
            "Epoch 4/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.5594\n",
            "Epoch 5/63\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6917 - accuracy: 0.5584\n",
            "Epoch 6/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6909 - accuracy: 0.5776\n",
            "Epoch 7/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6921 - accuracy: 0.5327\n",
            "Epoch 8/63\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6921 - accuracy: 0.5314\n",
            "Epoch 9/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6921 - accuracy: 0.5283\n",
            "Epoch 10/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6910 - accuracy: 0.5488\n",
            "Epoch 11/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6908 - accuracy: 0.5488\n",
            "Epoch 12/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6902 - accuracy: 0.5551\n",
            "Epoch 13/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6911 - accuracy: 0.5387\n",
            "Epoch 14/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6896 - accuracy: 0.5622\n",
            "Epoch 15/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6900 - accuracy: 0.5555\n",
            "Epoch 16/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6898 - accuracy: 0.5594\n",
            "Epoch 17/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6926 - accuracy: 0.5172\n",
            "Epoch 18/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6894 - accuracy: 0.5643\n",
            "Epoch 19/63\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6926 - accuracy: 0.5164\n",
            "Epoch 20/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6916 - accuracy: 0.5306\n",
            "Epoch 21/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6907 - accuracy: 0.5415\n",
            "Epoch 22/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6901 - accuracy: 0.5470\n",
            "Epoch 23/63\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6925 - accuracy: 0.5187\n",
            "Epoch 24/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6912 - accuracy: 0.5349\n",
            "Epoch 25/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6905 - accuracy: 0.5419\n",
            "Epoch 26/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6921 - accuracy: 0.5229\n",
            "Epoch 27/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6961 - accuracy: 0.4807\n",
            "Epoch 28/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6891 - accuracy: 0.5540\n",
            "Epoch 29/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.5274\n",
            "Epoch 30/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6882 - accuracy: 0.5619\n",
            "Epoch 31/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6914 - accuracy: 0.5307\n",
            "Epoch 32/63\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6903 - accuracy: 0.5408\n",
            "Epoch 33/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6877 - accuracy: 0.5648\n",
            "Epoch 34/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6961 - accuracy: 0.4858\n",
            "Epoch 35/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6925 - accuracy: 0.5206\n",
            "Epoch 36/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6892 - accuracy: 0.5493\n",
            "Epoch 37/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6887 - accuracy: 0.5529\n",
            "Epoch 38/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6887 - accuracy: 0.5539\n",
            "Epoch 39/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6957 - accuracy: 0.4916\n",
            "Epoch 40/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6896 - accuracy: 0.5447\n",
            "Epoch 41/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6898 - accuracy: 0.5434\n",
            "Epoch 42/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6901 - accuracy: 0.5406\n",
            "Epoch 43/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.5114\n",
            "Epoch 44/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6937 - accuracy: 0.5102\n",
            "Epoch 45/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6922 - accuracy: 0.5228\n",
            "Epoch 46/63\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6894 - accuracy: 0.5473\n",
            "Epoch 47/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6883 - accuracy: 0.5557\n",
            "Epoch 48/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6819 - accuracy: 0.6046\n",
            "Epoch 49/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.5261\n",
            "Epoch 50/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6873 - accuracy: 0.5632\n",
            "Epoch 51/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6955 - accuracy: 0.4974\n",
            "Epoch 52/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6839 - accuracy: 0.5879\n",
            "Epoch 53/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6901 - accuracy: 0.5397\n",
            "Epoch 54/63\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6939 - accuracy: 0.5100\n",
            "Epoch 55/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.5180\n",
            "Epoch 56/63\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6935 - accuracy: 0.5130\n",
            "Epoch 57/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6916 - accuracy: 0.5284\n",
            "Epoch 58/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6958 - accuracy: 0.4969\n",
            "Epoch 59/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6961 - accuracy: 0.4940\n",
            "Epoch 60/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6840 - accuracy: 0.5871\n",
            "Epoch 61/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6887 - accuracy: 0.5508\n",
            "Epoch 62/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6892 - accuracy: 0.5475\n",
            "Epoch 63/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6898 - accuracy: 0.5436\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6860 - accuracy: 0.5738\n",
            "Epoch 1/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 16.1776 - accuracy: 0.4955\n",
            "Epoch 2/63\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6881 - accuracy: 0.5447\n",
            "Epoch 3/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6867 - accuracy: 0.6570\n",
            "Epoch 4/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7625 - accuracy: 0.5352\n",
            "Epoch 5/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6830 - accuracy: 0.5762\n",
            "Epoch 6/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6740 - accuracy: 0.6504\n",
            "Epoch 7/63\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6874 - accuracy: 0.5655\n",
            "Epoch 8/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6795 - accuracy: 0.6558\n",
            "Epoch 9/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6818 - accuracy: 0.6098\n",
            "Epoch 10/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6780 - accuracy: 0.6021\n",
            "Epoch 11/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6875 - accuracy: 0.5654\n",
            "Epoch 12/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6893 - accuracy: 0.5403\n",
            "Epoch 13/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6769 - accuracy: 0.6755\n",
            "Epoch 14/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6688 - accuracy: 0.6364\n",
            "Epoch 15/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.8211 - accuracy: 0.5089\n",
            "Epoch 16/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6817 - accuracy: 0.5993\n",
            "Epoch 17/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6855 - accuracy: 0.5683\n",
            "Epoch 18/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6774 - accuracy: 0.6142\n",
            "Epoch 19/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6759 - accuracy: 0.5841\n",
            "Epoch 20/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6668 - accuracy: 0.6457\n",
            "Epoch 21/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6653 - accuracy: 0.6510\n",
            "Epoch 22/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6634 - accuracy: 0.6423\n",
            "Epoch 23/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6556 - accuracy: 0.6327\n",
            "Epoch 24/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6510 - accuracy: 0.6985\n",
            "Epoch 25/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6783 - accuracy: 0.5940\n",
            "Epoch 26/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6763 - accuracy: 0.6012\n",
            "Epoch 27/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6837 - accuracy: 0.5694\n",
            "Epoch 28/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6808 - accuracy: 0.5809\n",
            "Epoch 29/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6873 - accuracy: 0.5545\n",
            "Epoch 30/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6837 - accuracy: 0.5695\n",
            "Epoch 31/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6855 - accuracy: 0.5622\n",
            "Epoch 32/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6771 - accuracy: 0.5959\n",
            "Epoch 33/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6834 - accuracy: 0.5704\n",
            "Epoch 34/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6796 - accuracy: 0.5866\n",
            "Epoch 35/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6880 - accuracy: 0.5521\n",
            "Epoch 36/63\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6819 - accuracy: 0.5770\n",
            "Epoch 37/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6858 - accuracy: 0.5605\n",
            "Epoch 38/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6839 - accuracy: 0.5683\n",
            "Epoch 39/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7001 - accuracy: 0.5023\n",
            "Epoch 40/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6761 - accuracy: 0.5994\n",
            "Epoch 41/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6887 - accuracy: 0.5488\n",
            "Epoch 42/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6779 - accuracy: 0.5924\n",
            "Epoch 43/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6749 - accuracy: 0.6053\n",
            "Epoch 44/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6916 - accuracy: 0.5368\n",
            "Epoch 45/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6871 - accuracy: 0.5551\n",
            "Epoch 46/63\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6811 - accuracy: 0.5801\n",
            "Epoch 47/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6848 - accuracy: 0.5648\n",
            "Epoch 48/63\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6762 - accuracy: 0.6000\n",
            "Epoch 49/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6742 - accuracy: 0.6072\n",
            "Epoch 50/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6789 - accuracy: 0.5882\n",
            "Epoch 51/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6783 - accuracy: 0.5898\n",
            "Epoch 52/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6798 - accuracy: 0.5841\n",
            "Epoch 53/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6904 - accuracy: 0.5434\n",
            "Epoch 54/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6753 - accuracy: 0.6011\n",
            "Epoch 55/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7038 - accuracy: 0.4904\n",
            "Epoch 56/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6793 - accuracy: 0.5860\n",
            "Epoch 57/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6824 - accuracy: 0.5741\n",
            "Epoch 58/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6810 - accuracy: 0.5795\n",
            "Epoch 59/63\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6846 - accuracy: 0.5657\n",
            "Epoch 60/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6733 - accuracy: 0.6106\n",
            "Epoch 61/63\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6847 - accuracy: 0.5653\n",
            "Epoch 62/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6869 - accuracy: 0.5565\n",
            "Epoch 63/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6781 - accuracy: 0.5919\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.7108 - accuracy: 0.4590\n",
            "Epoch 1/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 44.7753 - accuracy: 0.5277\n",
            "Epoch 2/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7039 - accuracy: 0.5934\n",
            "Epoch 3/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6661 - accuracy: 0.5720\n",
            "Epoch 4/63\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6724 - accuracy: 0.5901\n",
            "Epoch 5/63\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.7131 - accuracy: 0.5783\n",
            "Epoch 6/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6730 - accuracy: 0.6025\n",
            "Epoch 7/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7365 - accuracy: 0.5489\n",
            "Epoch 8/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6461 - accuracy: 0.6222\n",
            "Epoch 9/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7249 - accuracy: 0.5786\n",
            "Epoch 10/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6450 - accuracy: 0.6765\n",
            "Epoch 11/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6468 - accuracy: 0.6627\n",
            "Epoch 12/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7253 - accuracy: 0.6341\n",
            "Epoch 13/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7789 - accuracy: 0.5869\n",
            "Epoch 14/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6222 - accuracy: 0.6599\n",
            "Epoch 15/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6506 - accuracy: 0.6435\n",
            "Epoch 16/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6487 - accuracy: 0.6337\n",
            "Epoch 17/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6174 - accuracy: 0.6769\n",
            "Epoch 18/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6971 - accuracy: 0.5021\n",
            "Epoch 19/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6925 - accuracy: 0.4765\n",
            "Epoch 20/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6900 - accuracy: 0.4480\n",
            "Epoch 21/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6912 - accuracy: 0.4799\n",
            "Epoch 22/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6944 - accuracy: 0.4420\n",
            "Epoch 23/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6913 - accuracy: 0.4565\n",
            "Epoch 24/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6912 - accuracy: 0.4778\n",
            "Epoch 25/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6837 - accuracy: 0.4851\n",
            "Epoch 26/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6913 - accuracy: 0.4640\n",
            "Epoch 27/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6926 - accuracy: 0.4664\n",
            "Epoch 28/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6907 - accuracy: 0.4769\n",
            "Epoch 29/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6832 - accuracy: 0.4888\n",
            "Epoch 30/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6820 - accuracy: 0.5075\n",
            "Epoch 31/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6833 - accuracy: 0.4827\n",
            "Epoch 32/63\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6830 - accuracy: 0.5069\n",
            "Epoch 33/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6802 - accuracy: 0.5070\n",
            "Epoch 34/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6833 - accuracy: 0.5215\n",
            "Epoch 35/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6915 - accuracy: 0.5274\n",
            "Epoch 36/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6882 - accuracy: 0.5050\n",
            "Epoch 37/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6801 - accuracy: 0.5547\n",
            "Epoch 38/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6811 - accuracy: 0.5143\n",
            "Epoch 39/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6740 - accuracy: 0.5353\n",
            "Epoch 40/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6813 - accuracy: 0.5595\n",
            "Epoch 41/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6743 - accuracy: 0.5537\n",
            "Epoch 42/63\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6792 - accuracy: 0.5531\n",
            "Epoch 43/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6820 - accuracy: 0.5208\n",
            "Epoch 44/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6789 - accuracy: 0.5463\n",
            "Epoch 45/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6886 - accuracy: 0.5204\n",
            "Epoch 46/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.9091 - accuracy: 0.5032\n",
            "Epoch 47/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6855 - accuracy: 0.5236\n",
            "Epoch 48/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6887 - accuracy: 0.5199\n",
            "Epoch 49/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6834 - accuracy: 0.5656\n",
            "Epoch 50/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6679 - accuracy: 0.5241\n",
            "Epoch 51/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6909 - accuracy: 0.5363\n",
            "Epoch 52/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6922 - accuracy: 0.5136\n",
            "Epoch 53/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6901 - accuracy: 0.5522\n",
            "Epoch 54/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6863 - accuracy: 0.5468\n",
            "Epoch 55/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6877 - accuracy: 0.5260\n",
            "Epoch 56/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6842 - accuracy: 0.5846\n",
            "Epoch 57/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6783 - accuracy: 0.5953\n",
            "Epoch 58/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6860 - accuracy: 0.5550\n",
            "Epoch 59/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6890 - accuracy: 0.5443\n",
            "Epoch 60/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6905 - accuracy: 0.5340\n",
            "Epoch 61/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6841 - accuracy: 0.5173\n",
            "Epoch 62/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6752 - accuracy: 0.5842\n",
            "Epoch 63/63\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6892 - accuracy: 0.5027\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6706 - accuracy: 0.5902\n",
            "Epoch 1/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 140.9163 - accuracy: 0.4875\n",
            "Epoch 2/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7053 - accuracy: 0.5541\n",
            "Epoch 3/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6922 - accuracy: 0.5597\n",
            "Epoch 4/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6922 - accuracy: 0.5399\n",
            "Epoch 5/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6904 - accuracy: 0.5968\n",
            "Epoch 6/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6922 - accuracy: 0.5324\n",
            "Epoch 7/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6909 - accuracy: 0.5643\n",
            "Epoch 8/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6913 - accuracy: 0.5467\n",
            "Epoch 9/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6927 - accuracy: 0.5174\n",
            "Epoch 10/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6904 - accuracy: 0.5564\n",
            "Epoch 11/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6897 - accuracy: 0.5645\n",
            "Epoch 12/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6888 - accuracy: 0.5732\n",
            "Epoch 13/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6915 - accuracy: 0.5317\n",
            "Epoch 14/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6889 - accuracy: 0.5661\n",
            "Epoch 15/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6924 - accuracy: 0.5194\n",
            "Epoch 16/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6947 - accuracy: 0.4910\n",
            "Epoch 17/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6893 - accuracy: 0.5561\n",
            "Epoch 18/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6920 - accuracy: 0.5244\n",
            "Epoch 19/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6872 - accuracy: 0.5765\n",
            "Epoch 20/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6864 - accuracy: 0.5831\n",
            "Epoch 21/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6860 - accuracy: 0.5822\n",
            "Epoch 22/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6865 - accuracy: 0.5788\n",
            "Epoch 23/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6892 - accuracy: 0.5512\n",
            "Epoch 24/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6845 - accuracy: 0.5916\n",
            "Epoch 25/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6898 - accuracy: 0.5443\n",
            "Epoch 26/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6851 - accuracy: 0.5844\n",
            "Epoch 27/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6900 - accuracy: 0.5421\n",
            "Epoch 28/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6913 - accuracy: 0.5313\n",
            "Epoch 29/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6889 - accuracy: 0.5493\n",
            "Epoch 30/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6905 - accuracy: 0.5368\n",
            "Epoch 31/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6866 - accuracy: 0.5660\n",
            "Epoch 32/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6875 - accuracy: 0.5586\n",
            "Epoch 33/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6857 - accuracy: 0.5718\n",
            "Epoch 34/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6882 - accuracy: 0.5531\n",
            "Epoch 35/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6849 - accuracy: 0.5755\n",
            "Epoch 36/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6955 - accuracy: 0.5021\n",
            "Epoch 37/63\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6863 - accuracy: 0.5655\n",
            "Epoch 38/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6941 - accuracy: 0.5125\n",
            "Epoch 39/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6851 - accuracy: 0.5726\n",
            "Epoch 40/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6916 - accuracy: 0.5295\n",
            "Epoch 41/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6934 - accuracy: 0.5179\n",
            "Epoch 42/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6891 - accuracy: 0.5455\n",
            "Epoch 43/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6943 - accuracy: 0.5125\n",
            "Epoch 44/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6847 - accuracy: 0.5730\n",
            "Epoch 45/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6837 - accuracy: 0.5784\n",
            "Epoch 46/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6882 - accuracy: 0.5507\n",
            "Epoch 47/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6946 - accuracy: 0.5115\n",
            "Epoch 48/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6938 - accuracy: 0.5173\n",
            "Epoch 49/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6882 - accuracy: 0.5511\n",
            "Epoch 50/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5220\n",
            "Epoch 51/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6886 - accuracy: 0.5482\n",
            "Epoch 52/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6947 - accuracy: 0.5117\n",
            "Epoch 53/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5209\n",
            "Epoch 54/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6910 - accuracy: 0.5340\n",
            "Epoch 55/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6899 - accuracy: 0.5406\n",
            "Epoch 56/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6850 - accuracy: 0.5685\n",
            "Epoch 57/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6854 - accuracy: 0.5663\n",
            "Epoch 58/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6922 - accuracy: 0.5278\n",
            "Epoch 59/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.5234\n",
            "Epoch 60/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6903 - accuracy: 0.5385\n",
            "Epoch 61/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6860 - accuracy: 0.5637\n",
            "Epoch 62/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6783 - accuracy: 0.6089\n",
            "Epoch 63/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6920 - accuracy: 0.5283\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6911 - accuracy: 0.5333\n",
            "Epoch 1/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 21.2318 - accuracy: 0.5106\n",
            "Epoch 2/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6806 - accuracy: 0.6412\n",
            "Epoch 3/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7029 - accuracy: 0.5442\n",
            "Epoch 4/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.5193\n",
            "Epoch 5/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5118\n",
            "Epoch 6/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6920 - accuracy: 0.5434\n",
            "Epoch 7/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6924 - accuracy: 0.5307\n",
            "Epoch 8/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6917 - accuracy: 0.5537\n",
            "Epoch 9/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6920 - accuracy: 0.5386\n",
            "Epoch 10/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.5399\n",
            "Epoch 11/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5054\n",
            "Epoch 12/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6906 - accuracy: 0.5547\n",
            "Epoch 13/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.5300\n",
            "Epoch 14/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6900 - accuracy: 0.5575\n",
            "Epoch 15/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6913 - accuracy: 0.5367\n",
            "Epoch 16/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5082\n",
            "Epoch 17/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.5284\n",
            "Epoch 18/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6944 - accuracy: 0.4916\n",
            "Epoch 19/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6906 - accuracy: 0.5420\n",
            "Epoch 20/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6901 - accuracy: 0.5479\n",
            "Epoch 21/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6941 - accuracy: 0.4988\n",
            "Epoch 22/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6921 - accuracy: 0.5237\n",
            "Epoch 23/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6933 - accuracy: 0.5098\n",
            "Epoch 24/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6903 - accuracy: 0.5427\n",
            "Epoch 25/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6891 - accuracy: 0.5565\n",
            "Epoch 26/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6917 - accuracy: 0.5270\n",
            "Epoch 27/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6911 - accuracy: 0.5337\n",
            "Epoch 28/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6898 - accuracy: 0.5467\n",
            "Epoch 29/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6888 - accuracy: 0.5558\n",
            "Epoch 30/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.5097\n",
            "Epoch 31/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6854 - accuracy: 0.5886\n",
            "Epoch 32/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6908 - accuracy: 0.5367\n",
            "Epoch 33/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6943 - accuracy: 0.5027\n",
            "Epoch 34/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6902 - accuracy: 0.5409\n",
            "Epoch 35/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6983 - accuracy: 0.4683\n",
            "Epoch 36/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6940 - accuracy: 0.5077\n",
            "Epoch 37/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6976 - accuracy: 0.4780\n",
            "Epoch 38/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6914 - accuracy: 0.5299\n",
            "Epoch 39/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6916 - accuracy: 0.5287\n",
            "Epoch 40/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6865 - accuracy: 0.5713\n",
            "Epoch 41/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6910 - accuracy: 0.5333\n",
            "Epoch 42/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6902 - accuracy: 0.5401\n",
            "Epoch 43/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6902 - accuracy: 0.5405\n",
            "Epoch 44/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6916 - accuracy: 0.5279\n",
            "Epoch 45/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6870 - accuracy: 0.5661\n",
            "Epoch 46/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6849 - accuracy: 0.5833\n",
            "Epoch 47/63\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6889 - accuracy: 0.5502\n",
            "Epoch 48/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6935 - accuracy: 0.5126\n",
            "Epoch 49/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6943 - accuracy: 0.5062\n",
            "Epoch 50/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6938 - accuracy: 0.5108\n",
            "Epoch 51/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6882 - accuracy: 0.5551\n",
            "Epoch 52/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6885 - accuracy: 0.5524\n",
            "Epoch 53/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6950 - accuracy: 0.5031\n",
            "Epoch 54/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6903 - accuracy: 0.5379\n",
            "Epoch 55/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6824 - accuracy: 0.5961\n",
            "Epoch 56/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6923 - accuracy: 0.5236\n",
            "Epoch 57/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6899 - accuracy: 0.5411\n",
            "Epoch 58/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6950 - accuracy: 0.5046\n",
            "Epoch 59/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6924 - accuracy: 0.5237\n",
            "Epoch 60/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6859 - accuracy: 0.5683\n",
            "Epoch 61/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6866 - accuracy: 0.5636\n",
            "Epoch 62/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6915 - accuracy: 0.5301\n",
            "Epoch 63/63\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6939 - accuracy: 0.5124\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6862 - accuracy: 0.5667\n",
            "Epoch 1/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 27.3573 - accuracy: 0.5464\n",
            "Epoch 2/33\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6662 - accuracy: 0.5707\n",
            "Epoch 3/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6663 - accuracy: 0.5605\n",
            "Epoch 4/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6722 - accuracy: 0.5721\n",
            "Epoch 5/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6888 - accuracy: 0.5839\n",
            "Epoch 6/33\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6628 - accuracy: 0.6176\n",
            "Epoch 7/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6762 - accuracy: 0.5578\n",
            "Epoch 8/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6698 - accuracy: 0.5804\n",
            "Epoch 9/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6789 - accuracy: 0.6644\n",
            "Epoch 10/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.7026 - accuracy: 0.5870\n",
            "Epoch 11/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6427 - accuracy: 0.6071\n",
            "Epoch 12/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6514 - accuracy: 0.5864\n",
            "Epoch 13/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6358 - accuracy: 0.6264\n",
            "Epoch 14/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6383 - accuracy: 0.6456\n",
            "Epoch 15/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6515 - accuracy: 0.6670\n",
            "Epoch 16/33\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6615 - accuracy: 0.6474\n",
            "Epoch 17/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6498 - accuracy: 0.6226\n",
            "Epoch 18/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6275 - accuracy: 0.6577\n",
            "Epoch 19/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6270 - accuracy: 0.6425\n",
            "Epoch 20/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6817 - accuracy: 0.5814\n",
            "Epoch 21/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6484 - accuracy: 0.5976\n",
            "Epoch 22/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6685 - accuracy: 0.5772\n",
            "Epoch 23/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6259 - accuracy: 0.6595\n",
            "Epoch 24/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6252 - accuracy: 0.6855\n",
            "Epoch 25/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.7762 - accuracy: 0.5173\n",
            "Epoch 26/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6207 - accuracy: 0.6614\n",
            "Epoch 27/33\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6160 - accuracy: 0.6700\n",
            "Epoch 28/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6428 - accuracy: 0.6190\n",
            "Epoch 29/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6343 - accuracy: 0.6205\n",
            "Epoch 30/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6417 - accuracy: 0.6261\n",
            "Epoch 31/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6274 - accuracy: 0.6286\n",
            "Epoch 32/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6164 - accuracy: 0.6534\n",
            "Epoch 33/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6137 - accuracy: 0.6784\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5813 - accuracy: 0.6885\n",
            "Epoch 1/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 117.8291 - accuracy: 0.5091\n",
            "Epoch 2/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 3.0802 - accuracy: 0.4703\n",
            "Epoch 3/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6879 - accuracy: 0.5380\n",
            "Epoch 4/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6905 - accuracy: 0.5679\n",
            "Epoch 5/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6831 - accuracy: 0.5778\n",
            "Epoch 6/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6874 - accuracy: 0.5670\n",
            "Epoch 7/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6855 - accuracy: 0.5569\n",
            "Epoch 8/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6901 - accuracy: 0.5307\n",
            "Epoch 9/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6943 - accuracy: 0.5346\n",
            "Epoch 10/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6952 - accuracy: 0.5466\n",
            "Epoch 11/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6996 - accuracy: 0.5031\n",
            "Epoch 12/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6923 - accuracy: 0.5347\n",
            "Epoch 13/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6795 - accuracy: 0.5629\n",
            "Epoch 14/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6713 - accuracy: 0.5318\n",
            "Epoch 15/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6672 - accuracy: 0.5756\n",
            "Epoch 16/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6581 - accuracy: 0.6007\n",
            "Epoch 17/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6484 - accuracy: 0.6521\n",
            "Epoch 18/33\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6388 - accuracy: 0.6536\n",
            "Epoch 19/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6717 - accuracy: 0.5858\n",
            "Epoch 20/33\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6743 - accuracy: 0.5599\n",
            "Epoch 21/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6446 - accuracy: 0.6109\n",
            "Epoch 22/33\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6514 - accuracy: 0.6149\n",
            "Epoch 23/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6190 - accuracy: 0.6313\n",
            "Epoch 24/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6729 - accuracy: 0.5962\n",
            "Epoch 25/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6517 - accuracy: 0.6091\n",
            "Epoch 26/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6396 - accuracy: 0.6109\n",
            "Epoch 27/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6421 - accuracy: 0.5992\n",
            "Epoch 28/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6440 - accuracy: 0.6139\n",
            "Epoch 29/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6486 - accuracy: 0.6010\n",
            "Epoch 30/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6491 - accuracy: 0.6189\n",
            "Epoch 31/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6135 - accuracy: 0.6385\n",
            "Epoch 32/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6276 - accuracy: 0.6277\n",
            "Epoch 33/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6516 - accuracy: 0.5994\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.7053 - accuracy: 0.5082\n",
            "Epoch 1/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 41.7042 - accuracy: 0.5080\n",
            "Epoch 2/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5179\n",
            "Epoch 3/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5711\n",
            "Epoch 4/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5071\n",
            "Epoch 5/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.5242\n",
            "Epoch 6/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6924 - accuracy: 0.5660\n",
            "Epoch 7/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5185\n",
            "Epoch 8/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6926 - accuracy: 0.5326\n",
            "Epoch 9/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6927 - accuracy: 0.5269\n",
            "Epoch 10/33\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6927 - accuracy: 0.5256\n",
            "Epoch 11/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6926 - accuracy: 0.5279\n",
            "Epoch 12/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5068\n",
            "Epoch 13/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5000\n",
            "Epoch 14/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6933 - accuracy: 0.5001\n",
            "Epoch 15/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6930 - accuracy: 0.5093\n",
            "Epoch 16/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6917 - accuracy: 0.5457\n",
            "Epoch 17/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6912 - accuracy: 0.5593\n",
            "Epoch 18/33\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5364\n",
            "Epoch 19/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6921 - accuracy: 0.5310\n",
            "Epoch 20/33\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6917 - accuracy: 0.5407\n",
            "Epoch 21/33\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6924 - accuracy: 0.5226\n",
            "Epoch 22/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6912 - accuracy: 0.5484\n",
            "Epoch 23/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6927 - accuracy: 0.5159\n",
            "Epoch 24/33\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.5310\n",
            "Epoch 25/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6915 - accuracy: 0.5379\n",
            "Epoch 26/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6886 - accuracy: 0.5924\n",
            "Epoch 27/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6914 - accuracy: 0.5390\n",
            "Epoch 28/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6899 - accuracy: 0.5659\n",
            "Epoch 29/33\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5287\n",
            "Epoch 30/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6934 - accuracy: 0.5029\n",
            "Epoch 31/33\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6915 - accuracy: 0.5348\n",
            "Epoch 32/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6935 - accuracy: 0.5015\n",
            "Epoch 33/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6919 - accuracy: 0.5288\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe7eb3bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6917 - accuracy: 0.5738\n",
            "Epoch 1/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 62.9847 - accuracy: 0.4783\n",
            "Epoch 2/33\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6885 - accuracy: 0.5705\n",
            "Epoch 3/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6941 - accuracy: 0.5649\n",
            "Epoch 4/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6892 - accuracy: 0.5658\n",
            "Epoch 5/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5548\n",
            "Epoch 6/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6868 - accuracy: 0.5689\n",
            "Epoch 7/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6873 - accuracy: 0.5833\n",
            "Epoch 8/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6808 - accuracy: 0.5412\n",
            "Epoch 9/33\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6809 - accuracy: 0.5827\n",
            "Epoch 10/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6859 - accuracy: 0.5627\n",
            "Epoch 11/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6859 - accuracy: 0.5516\n",
            "Epoch 12/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.7255 - accuracy: 0.6210\n",
            "Epoch 13/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6916 - accuracy: 0.5341\n",
            "Epoch 14/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6913 - accuracy: 0.5368\n",
            "Epoch 15/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6920 - accuracy: 0.5254\n",
            "Epoch 16/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6871 - accuracy: 0.5926\n",
            "Epoch 17/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6879 - accuracy: 0.5819\n",
            "Epoch 18/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6952 - accuracy: 0.4834\n",
            "Epoch 19/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6912 - accuracy: 0.5355\n",
            "Epoch 20/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6934 - accuracy: 0.5069\n",
            "Epoch 21/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6928 - accuracy: 0.5153\n",
            "Epoch 22/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6913 - accuracy: 0.5329\n",
            "Epoch 23/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6937 - accuracy: 0.5057\n",
            "Epoch 24/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6897 - accuracy: 0.5488\n",
            "Epoch 25/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6915 - accuracy: 0.5293\n",
            "Epoch 26/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6916 - accuracy: 0.5291\n",
            "Epoch 27/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6910 - accuracy: 0.5341\n",
            "Epoch 28/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6934 - accuracy: 0.5101\n",
            "Epoch 29/33\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6943 - accuracy: 0.5022\n",
            "Epoch 30/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6883 - accuracy: 0.5597\n",
            "Epoch 31/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6884 - accuracy: 0.5582\n",
            "Epoch 32/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6917 - accuracy: 0.5268\n",
            "Epoch 33/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6872 - accuracy: 0.5678\n",
            "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0ff0f91620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.6965 - accuracy: 0.5167\n",
            "Epoch 1/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 15.7543 - accuracy: 0.4553\n",
            "Epoch 2/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6777 - accuracy: 0.6266\n",
            "Epoch 3/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6780 - accuracy: 0.5951\n",
            "Epoch 4/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6895 - accuracy: 0.5953\n",
            "Epoch 5/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6774 - accuracy: 0.5968\n",
            "Epoch 6/33\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6551 - accuracy: 0.6417\n",
            "Epoch 7/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6681 - accuracy: 0.6098\n",
            "Epoch 8/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6831 - accuracy: 0.6153\n",
            "Epoch 9/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6758 - accuracy: 0.5992\n",
            "Epoch 10/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6581 - accuracy: 0.6043\n",
            "Epoch 11/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6684 - accuracy: 0.6213\n",
            "Epoch 12/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6689 - accuracy: 0.5946\n",
            "Epoch 13/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6814 - accuracy: 0.5872\n",
            "Epoch 14/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6611 - accuracy: 0.6222\n",
            "Epoch 15/33\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6732 - accuracy: 0.5629\n",
            "Epoch 16/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6651 - accuracy: 0.6029\n",
            "Epoch 17/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6349 - accuracy: 0.6357\n",
            "Epoch 18/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6637 - accuracy: 0.6000\n",
            "Epoch 19/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6645 - accuracy: 0.5563\n",
            "Epoch 20/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6529 - accuracy: 0.6200\n",
            "Epoch 21/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6548 - accuracy: 0.5703\n",
            "Epoch 22/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6490 - accuracy: 0.6471\n",
            "Epoch 23/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6905 - accuracy: 0.6123\n",
            "Epoch 24/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6451 - accuracy: 0.6070\n",
            "Epoch 25/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6470 - accuracy: 0.5941\n",
            "Epoch 26/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6393 - accuracy: 0.6404\n",
            "Epoch 27/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6494 - accuracy: 0.6012\n",
            "Epoch 28/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6509 - accuracy: 0.6372\n",
            "Epoch 29/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6297 - accuracy: 0.6141\n",
            "Epoch 30/33\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6579 - accuracy: 0.6225\n",
            "Epoch 31/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6511 - accuracy: 0.6604\n",
            "Epoch 32/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6499 - accuracy: 0.6464\n",
            "Epoch 33/33\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6517 - accuracy: 0.6243\n",
            "WARNING:tensorflow:6 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe769cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 3ms/step - loss: 0.6314 - accuracy: 0.6333\n",
            "Epoch 1/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 21.2789 - accuracy: 0.4433\n",
            "Epoch 2/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6960 - accuracy: 0.4202\n",
            "Epoch 3/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6885 - accuracy: 0.5183\n",
            "Epoch 4/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6905 - accuracy: 0.5553\n",
            "Epoch 5/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6900 - accuracy: 0.5671\n",
            "Epoch 6/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6909 - accuracy: 0.5364\n",
            "Epoch 7/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6892 - accuracy: 0.5199\n",
            "Epoch 8/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6892 - accuracy: 0.5722\n",
            "Epoch 9/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6903 - accuracy: 0.5530\n",
            "Epoch 10/63\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6917 - accuracy: 0.5315\n",
            "Epoch 11/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6925 - accuracy: 0.4780\n",
            "Epoch 12/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6891 - accuracy: 0.5481\n",
            "Epoch 13/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6904 - accuracy: 0.5592\n",
            "Epoch 14/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6896 - accuracy: 0.5270\n",
            "Epoch 15/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6904 - accuracy: 0.5332\n",
            "Epoch 16/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6865 - accuracy: 0.5150\n",
            "Epoch 17/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6913 - accuracy: 0.5308\n",
            "Epoch 18/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.5137\n",
            "Epoch 19/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6843 - accuracy: 0.5679\n",
            "Epoch 20/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6894 - accuracy: 0.5206\n",
            "Epoch 21/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6916 - accuracy: 0.4896\n",
            "Epoch 22/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6909 - accuracy: 0.5354\n",
            "Epoch 23/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6874 - accuracy: 0.5384\n",
            "Epoch 24/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6923 - accuracy: 0.4964\n",
            "Epoch 25/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6784 - accuracy: 0.5980\n",
            "Epoch 26/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6891 - accuracy: 0.5037\n",
            "Epoch 27/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6947 - accuracy: 0.4910\n",
            "Epoch 28/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6755 - accuracy: 0.5533\n",
            "Epoch 29/63\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6763 - accuracy: 0.5506\n",
            "Epoch 30/63\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6767 - accuracy: 0.5488\n",
            "Epoch 31/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6710 - accuracy: 0.5474\n",
            "Epoch 32/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6724 - accuracy: 0.5179\n",
            "Epoch 33/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6752 - accuracy: 0.5266\n",
            "Epoch 34/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6808 - accuracy: 0.5074\n",
            "Epoch 35/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6582 - accuracy: 0.5684\n",
            "Epoch 36/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6620 - accuracy: 0.5323\n",
            "Epoch 37/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6683 - accuracy: 0.5402\n",
            "Epoch 38/63\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6566 - accuracy: 0.5433\n",
            "Epoch 39/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6656 - accuracy: 0.5031\n",
            "Epoch 40/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7037 - accuracy: 0.4846\n",
            "Epoch 41/63\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6478 - accuracy: 0.5794\n",
            "Epoch 42/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6593 - accuracy: 0.5191\n",
            "Epoch 43/63\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6550 - accuracy: 0.5345\n",
            "Epoch 44/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6583 - accuracy: 0.6514\n",
            "Epoch 45/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6612 - accuracy: 0.6383\n",
            "Epoch 46/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6368 - accuracy: 0.6859\n",
            "Epoch 47/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6665 - accuracy: 0.6287\n",
            "Epoch 48/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6832 - accuracy: 0.5512\n",
            "Epoch 49/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6416 - accuracy: 0.6527\n",
            "Epoch 50/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6342 - accuracy: 0.6621\n",
            "Epoch 51/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6397 - accuracy: 0.6717\n",
            "Epoch 52/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6235 - accuracy: 0.6901\n",
            "Epoch 53/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6299 - accuracy: 0.6698\n",
            "Epoch 54/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6535 - accuracy: 0.6236\n",
            "Epoch 55/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6520 - accuracy: 0.6415\n",
            "Epoch 56/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6364 - accuracy: 0.6947\n",
            "Epoch 57/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6330 - accuracy: 0.6336\n",
            "Epoch 58/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6492 - accuracy: 0.6310\n",
            "Epoch 59/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7342 - accuracy: 0.4938\n",
            "Epoch 60/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6698 - accuracy: 0.6116\n",
            "Epoch 61/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6406 - accuracy: 0.6661\n",
            "Epoch 62/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6170 - accuracy: 0.6920\n",
            "Epoch 63/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6464 - accuracy: 0.6866\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fed53e378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6088 - accuracy: 0.6885\n",
            "Epoch 1/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 50.7109 - accuracy: 0.5009\n",
            "Epoch 2/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7082 - accuracy: 0.5241\n",
            "Epoch 3/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.7012 - accuracy: 0.5235\n",
            "Epoch 4/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6819 - accuracy: 0.5745\n",
            "Epoch 5/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6890 - accuracy: 0.5705\n",
            "Epoch 6/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6919 - accuracy: 0.5330\n",
            "Epoch 7/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6815 - accuracy: 0.5646\n",
            "Epoch 8/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6900 - accuracy: 0.5736\n",
            "Epoch 9/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6854 - accuracy: 0.5764\n",
            "Epoch 10/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6970 - accuracy: 0.4879\n",
            "Epoch 11/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6808 - accuracy: 0.5775\n",
            "Epoch 12/63\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6900 - accuracy: 0.5824\n",
            "Epoch 13/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6844 - accuracy: 0.5816\n",
            "Epoch 14/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6828 - accuracy: 0.6067\n",
            "Epoch 15/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6951 - accuracy: 0.5425\n",
            "Epoch 16/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5599\n",
            "Epoch 17/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6897 - accuracy: 0.5835\n",
            "Epoch 18/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6859 - accuracy: 0.5695\n",
            "Epoch 19/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6884 - accuracy: 0.5573\n",
            "Epoch 20/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6853 - accuracy: 0.5780\n",
            "Epoch 21/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6866 - accuracy: 0.5600\n",
            "Epoch 22/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6961 - accuracy: 0.5294\n",
            "Epoch 23/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6897 - accuracy: 0.5559\n",
            "Epoch 24/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6907 - accuracy: 0.5605\n",
            "Epoch 25/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6870 - accuracy: 0.5743\n",
            "Epoch 26/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6869 - accuracy: 0.5566\n",
            "Epoch 27/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6809 - accuracy: 0.6113\n",
            "Epoch 28/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6900 - accuracy: 0.5581\n",
            "Epoch 29/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6850 - accuracy: 0.5706\n",
            "Epoch 30/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6808 - accuracy: 0.5865\n",
            "Epoch 31/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6841 - accuracy: 0.5601\n",
            "Epoch 32/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6892 - accuracy: 0.5476\n",
            "Epoch 33/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6909 - accuracy: 0.5432\n",
            "Epoch 34/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6826 - accuracy: 0.5744\n",
            "Epoch 35/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6853 - accuracy: 0.5737\n",
            "Epoch 36/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6897 - accuracy: 0.5579\n",
            "Epoch 37/63\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6833 - accuracy: 0.5708\n",
            "Epoch 38/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6761 - accuracy: 0.5766\n",
            "Epoch 39/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6833 - accuracy: 0.5438\n",
            "Epoch 40/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6906 - accuracy: 0.5338\n",
            "Epoch 41/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6812 - accuracy: 0.5613\n",
            "Epoch 42/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6882 - accuracy: 0.5308\n",
            "Epoch 43/63\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6766 - accuracy: 0.5593\n",
            "Epoch 44/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6859 - accuracy: 0.5594\n",
            "Epoch 45/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6791 - accuracy: 0.5257\n",
            "Epoch 46/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6754 - accuracy: 0.5691\n",
            "Epoch 47/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6643 - accuracy: 0.6014\n",
            "Epoch 48/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6749 - accuracy: 0.5996\n",
            "Epoch 49/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6823 - accuracy: 0.5882\n",
            "Epoch 50/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6730 - accuracy: 0.5406\n",
            "Epoch 51/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6823 - accuracy: 0.5603\n",
            "Epoch 52/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6642 - accuracy: 0.5636\n",
            "Epoch 53/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6749 - accuracy: 0.5671\n",
            "Epoch 54/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6561 - accuracy: 0.5791\n",
            "Epoch 55/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6429 - accuracy: 0.5866\n",
            "Epoch 56/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6473 - accuracy: 0.5846\n",
            "Epoch 57/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6362 - accuracy: 0.5824\n",
            "Epoch 58/63\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6857 - accuracy: 0.5472\n",
            "Epoch 59/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6448 - accuracy: 0.5694\n",
            "Epoch 60/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6464 - accuracy: 0.5717\n",
            "Epoch 61/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6258 - accuracy: 0.5638\n",
            "Epoch 62/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6604 - accuracy: 0.5401\n",
            "Epoch 63/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6771 - accuracy: 0.5620\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0ff1050ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.7078 - accuracy: 0.4590\n",
            "Epoch 1/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 47.5229 - accuracy: 0.5647\n",
            "Epoch 2/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5476\n",
            "Epoch 3/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6923 - accuracy: 0.5319\n",
            "Epoch 4/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6921 - accuracy: 0.5154\n",
            "Epoch 5/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6922 - accuracy: 0.5415\n",
            "Epoch 6/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6917 - accuracy: 0.5312\n",
            "Epoch 7/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6911 - accuracy: 0.4871\n",
            "Epoch 8/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6889 - accuracy: 0.5039\n",
            "Epoch 9/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6927 - accuracy: 0.4974\n",
            "Epoch 10/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6857 - accuracy: 0.5491\n",
            "Epoch 11/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6913 - accuracy: 0.5057\n",
            "Epoch 12/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6895 - accuracy: 0.5294\n",
            "Epoch 13/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6910 - accuracy: 0.5342\n",
            "Epoch 14/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6919 - accuracy: 0.5032\n",
            "Epoch 15/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6901 - accuracy: 0.5643\n",
            "Epoch 16/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6861 - accuracy: 0.5222\n",
            "Epoch 17/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6917 - accuracy: 0.5088\n",
            "Epoch 18/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6852 - accuracy: 0.5433\n",
            "Epoch 19/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6897 - accuracy: 0.5710\n",
            "Epoch 20/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6904 - accuracy: 0.5297\n",
            "Epoch 21/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6885 - accuracy: 0.5741\n",
            "Epoch 22/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6861 - accuracy: 0.5168\n",
            "Epoch 23/63\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6912 - accuracy: 0.5367\n",
            "Epoch 24/63\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6904 - accuracy: 0.4986\n",
            "Epoch 25/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6890 - accuracy: 0.5272\n",
            "Epoch 26/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6883 - accuracy: 0.5506\n",
            "Epoch 27/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6886 - accuracy: 0.5381\n",
            "Epoch 28/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6911 - accuracy: 0.5292\n",
            "Epoch 29/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6878 - accuracy: 0.5494\n",
            "Epoch 30/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6915 - accuracy: 0.5142\n",
            "Epoch 31/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6859 - accuracy: 0.6037\n",
            "Epoch 32/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6846 - accuracy: 0.5435\n",
            "Epoch 33/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6896 - accuracy: 0.5576\n",
            "Epoch 34/63\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6921 - accuracy: 0.5152\n",
            "Epoch 35/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6879 - accuracy: 0.5426\n",
            "Epoch 36/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6905 - accuracy: 0.5355\n",
            "Epoch 37/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6909 - accuracy: 0.5225\n",
            "Epoch 38/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6904 - accuracy: 0.5304\n",
            "Epoch 39/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6921 - accuracy: 0.5042\n",
            "Epoch 40/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6881 - accuracy: 0.5379\n",
            "Epoch 41/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6833 - accuracy: 0.5591\n",
            "Epoch 42/63\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6871 - accuracy: 0.5641\n",
            "Epoch 43/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6871 - accuracy: 0.5780\n",
            "Epoch 44/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6871 - accuracy: 0.5467\n",
            "Epoch 45/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6918 - accuracy: 0.5108\n",
            "Epoch 46/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6836 - accuracy: 0.5558\n",
            "Epoch 47/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6876 - accuracy: 0.5552\n",
            "Epoch 48/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4971\n",
            "Epoch 49/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6859 - accuracy: 0.5655\n",
            "Epoch 50/63\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6874 - accuracy: 0.5679\n",
            "Epoch 51/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6901 - accuracy: 0.5107\n",
            "Epoch 52/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6858 - accuracy: 0.5229\n",
            "Epoch 53/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6905 - accuracy: 0.5147\n",
            "Epoch 54/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6905 - accuracy: 0.5313\n",
            "Epoch 55/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6863 - accuracy: 0.5577\n",
            "Epoch 56/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6862 - accuracy: 0.5666\n",
            "Epoch 57/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6918 - accuracy: 0.5112\n",
            "Epoch 58/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6890 - accuracy: 0.5399\n",
            "Epoch 59/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6900 - accuracy: 0.5273\n",
            "Epoch 60/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6872 - accuracy: 0.5324\n",
            "Epoch 61/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6928 - accuracy: 0.5037\n",
            "Epoch 62/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6886 - accuracy: 0.5289\n",
            "Epoch 63/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6845 - accuracy: 0.5363\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0feb2abb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6861 - accuracy: 0.5902\n",
            "Epoch 1/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 46.9230 - accuracy: 0.5245\n",
            "Epoch 2/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6914 - accuracy: 0.5579\n",
            "Epoch 3/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6945 - accuracy: 0.5569\n",
            "Epoch 4/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6921 - accuracy: 0.4967\n",
            "Epoch 5/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6921 - accuracy: 0.5610\n",
            "Epoch 6/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6940 - accuracy: 0.5572\n",
            "Epoch 7/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6927 - accuracy: 0.5440\n",
            "Epoch 8/63\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6896 - accuracy: 0.5325\n",
            "Epoch 9/63\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6911 - accuracy: 0.5392\n",
            "Epoch 10/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6910 - accuracy: 0.5385\n",
            "Epoch 11/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6913 - accuracy: 0.5052\n",
            "Epoch 12/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6876 - accuracy: 0.5762\n",
            "Epoch 13/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6875 - accuracy: 0.5556\n",
            "Epoch 14/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6801 - accuracy: 0.6139\n",
            "Epoch 15/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6900 - accuracy: 0.5265\n",
            "Epoch 16/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6886 - accuracy: 0.5601\n",
            "Epoch 17/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6875 - accuracy: 0.5656\n",
            "Epoch 18/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6878 - accuracy: 0.5477\n",
            "Epoch 19/63\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6863 - accuracy: 0.5586\n",
            "Epoch 20/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6854 - accuracy: 0.6023\n",
            "Epoch 21/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6616 - accuracy: 0.6479\n",
            "Epoch 22/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6702 - accuracy: 0.6557\n",
            "Epoch 23/63\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6688 - accuracy: 0.6776\n",
            "Epoch 24/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6638 - accuracy: 0.6272\n",
            "Epoch 25/63\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6604 - accuracy: 0.6601\n",
            "Epoch 26/63\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6608 - accuracy: 0.6827\n",
            "Epoch 27/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6624 - accuracy: 0.6425\n",
            "Epoch 28/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6635 - accuracy: 0.5823\n",
            "Epoch 29/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6666 - accuracy: 0.6533\n",
            "Epoch 30/63\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6705 - accuracy: 0.6757\n",
            "Epoch 31/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6549 - accuracy: 0.6864\n",
            "Epoch 32/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6755 - accuracy: 0.5897\n",
            "Epoch 33/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6241 - accuracy: 0.7128\n",
            "Epoch 34/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6656 - accuracy: 0.6772\n",
            "Epoch 35/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6349 - accuracy: 0.6818\n",
            "Epoch 36/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6618 - accuracy: 0.7043\n",
            "Epoch 37/63\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6535 - accuracy: 0.7002\n",
            "Epoch 38/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6581 - accuracy: 0.6507\n",
            "Epoch 39/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6400 - accuracy: 0.7155\n",
            "Epoch 40/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6444 - accuracy: 0.6970\n",
            "Epoch 41/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6425 - accuracy: 0.6833\n",
            "Epoch 42/63\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6512 - accuracy: 0.6827\n",
            "Epoch 43/63\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6499 - accuracy: 0.6763\n",
            "Epoch 44/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6560 - accuracy: 0.6509\n",
            "Epoch 45/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6481 - accuracy: 0.6840\n",
            "Epoch 46/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6537 - accuracy: 0.7016\n",
            "Epoch 47/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6601 - accuracy: 0.6645\n",
            "Epoch 48/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6460 - accuracy: 0.6778\n",
            "Epoch 49/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6426 - accuracy: 0.6509\n",
            "Epoch 50/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6436 - accuracy: 0.6838\n",
            "Epoch 51/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6807 - accuracy: 0.5949\n",
            "Epoch 52/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6610 - accuracy: 0.6592\n",
            "Epoch 53/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6296 - accuracy: 0.6778\n",
            "Epoch 54/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6444 - accuracy: 0.6845\n",
            "Epoch 55/63\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6266 - accuracy: 0.6864\n",
            "Epoch 56/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6277 - accuracy: 0.7046\n",
            "Epoch 57/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6135 - accuracy: 0.6980\n",
            "Epoch 58/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6075 - accuracy: 0.7301\n",
            "Epoch 59/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6386 - accuracy: 0.6723\n",
            "Epoch 60/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5985 - accuracy: 0.7108\n",
            "Epoch 61/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6671 - accuracy: 0.6049\n",
            "Epoch 62/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6069 - accuracy: 0.7076\n",
            "Epoch 63/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6517 - accuracy: 0.6183\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fea20ed90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 3ms/step - loss: 0.6616 - accuracy: 0.6333\n",
            "Epoch 1/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 94.8257 - accuracy: 0.5200\n",
            "Epoch 2/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6960 - accuracy: 0.4166\n",
            "Epoch 3/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6953 - accuracy: 0.5580\n",
            "Epoch 4/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6923 - accuracy: 0.5358\n",
            "Epoch 5/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6895 - accuracy: 0.5044\n",
            "Epoch 6/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6923 - accuracy: 0.5393\n",
            "Epoch 7/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6920 - accuracy: 0.5338\n",
            "Epoch 8/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6907 - accuracy: 0.5745\n",
            "Epoch 9/63\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.5514\n",
            "Epoch 10/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6923 - accuracy: 0.5395\n",
            "Epoch 11/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6928 - accuracy: 0.4963\n",
            "Epoch 12/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6923 - accuracy: 0.5066\n",
            "Epoch 13/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6918 - accuracy: 0.5368\n",
            "Epoch 14/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6885 - accuracy: 0.5308\n",
            "Epoch 15/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6884 - accuracy: 0.5759\n",
            "Epoch 16/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6902 - accuracy: 0.5550\n",
            "Epoch 17/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6913 - accuracy: 0.5195\n",
            "Epoch 18/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6899 - accuracy: 0.5555\n",
            "Epoch 19/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6890 - accuracy: 0.5601\n",
            "Epoch 20/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6920 - accuracy: 0.4930\n",
            "Epoch 21/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6890 - accuracy: 0.5644\n",
            "Epoch 22/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6909 - accuracy: 0.5336\n",
            "Epoch 23/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6893 - accuracy: 0.5539\n",
            "Epoch 24/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6892 - accuracy: 0.5535\n",
            "Epoch 25/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6916 - accuracy: 0.5300\n",
            "Epoch 26/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6878 - accuracy: 0.5630\n",
            "Epoch 27/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6906 - accuracy: 0.5092\n",
            "Epoch 28/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6907 - accuracy: 0.5186\n",
            "Epoch 29/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6889 - accuracy: 0.5126\n",
            "Epoch 30/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6917 - accuracy: 0.5186\n",
            "Epoch 31/63\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6895 - accuracy: 0.5404\n",
            "Epoch 32/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.4958\n",
            "Epoch 33/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6902 - accuracy: 0.5236\n",
            "Epoch 34/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6896 - accuracy: 0.5504\n",
            "Epoch 35/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6911 - accuracy: 0.5282\n",
            "Epoch 36/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6873 - accuracy: 0.5826\n",
            "Epoch 37/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6917 - accuracy: 0.5033\n",
            "Epoch 38/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6895 - accuracy: 0.5490\n",
            "Epoch 39/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6897 - accuracy: 0.5280\n",
            "Epoch 40/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6856 - accuracy: 0.5887\n",
            "Epoch 41/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6892 - accuracy: 0.5503\n",
            "Epoch 42/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6840 - accuracy: 0.5698\n",
            "Epoch 43/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6921 - accuracy: 0.5164\n",
            "Epoch 44/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6894 - accuracy: 0.5428\n",
            "Epoch 45/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6885 - accuracy: 0.5332\n",
            "Epoch 46/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6904 - accuracy: 0.5330\n",
            "Epoch 47/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6881 - accuracy: 0.5503\n",
            "Epoch 48/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6912 - accuracy: 0.5269\n",
            "Epoch 49/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6858 - accuracy: 0.5674\n",
            "Epoch 50/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6940 - accuracy: 0.4849\n",
            "Epoch 51/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6859 - accuracy: 0.5709\n",
            "Epoch 52/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6902 - accuracy: 0.5347\n",
            "Epoch 53/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6882 - accuracy: 0.5500\n",
            "Epoch 54/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6915 - accuracy: 0.5215\n",
            "Epoch 55/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6846 - accuracy: 0.5687\n",
            "Epoch 56/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6904 - accuracy: 0.5358\n",
            "Epoch 57/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6893 - accuracy: 0.5339\n",
            "Epoch 58/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6895 - accuracy: 0.5445\n",
            "Epoch 59/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6846 - accuracy: 0.5746\n",
            "Epoch 60/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6911 - accuracy: 0.5196\n",
            "Epoch 61/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6885 - accuracy: 0.5418\n",
            "Epoch 62/63\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6872 - accuracy: 0.5486\n",
            "Epoch 63/63\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6847 - accuracy: 0.5781\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0ff26b78c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6873 - accuracy: 0.5667\n",
            "Epoch 1/63\n",
            "10/10 [==============================] - 1s 3ms/step - loss: 43.7627 - accuracy: 0.5162\n",
            "Epoch 2/63\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6952 - accuracy: 0.4867\n",
            "Epoch 3/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6904 - accuracy: 0.5236\n",
            "Epoch 4/63\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6899 - accuracy: 0.5834\n",
            "Epoch 5/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6890 - accuracy: 0.5477\n",
            "Epoch 6/63\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6885 - accuracy: 0.5331\n",
            "Epoch 7/63\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6728 - accuracy: 0.5405\n",
            "Epoch 8/63\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6527 - accuracy: 0.5240\n",
            "Epoch 9/63\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6711 - accuracy: 0.5526\n",
            "Epoch 10/63\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6534 - accuracy: 0.5334\n",
            "Epoch 11/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6491 - accuracy: 0.5542\n",
            "Epoch 12/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6422 - accuracy: 0.5907\n",
            "Epoch 13/63\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6894 - accuracy: 0.5470\n",
            "Epoch 14/63\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6479 - accuracy: 0.6626\n",
            "Epoch 15/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6409 - accuracy: 0.6508\n",
            "Epoch 16/63\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6691 - accuracy: 0.6799\n",
            "Epoch 17/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6537 - accuracy: 0.6442\n",
            "Epoch 18/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6373 - accuracy: 0.6656\n",
            "Epoch 19/63\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6491 - accuracy: 0.6487\n",
            "Epoch 20/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6419 - accuracy: 0.6792\n",
            "Epoch 21/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6750 - accuracy: 0.6212\n",
            "Epoch 22/63\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6541 - accuracy: 0.6662\n",
            "Epoch 23/63\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6358 - accuracy: 0.6827\n",
            "Epoch 24/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6483 - accuracy: 0.6320\n",
            "Epoch 25/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6374 - accuracy: 0.6437\n",
            "Epoch 26/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6275 - accuracy: 0.6540\n",
            "Epoch 27/63\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6567 - accuracy: 0.6694\n",
            "Epoch 28/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6234 - accuracy: 0.6383\n",
            "Epoch 29/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6332 - accuracy: 0.6358\n",
            "Epoch 30/63\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6323 - accuracy: 0.6860\n",
            "Epoch 31/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6433 - accuracy: 0.6379\n",
            "Epoch 32/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6576 - accuracy: 0.6038\n",
            "Epoch 33/63\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6424 - accuracy: 0.6876\n",
            "Epoch 34/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6171 - accuracy: 0.6562\n",
            "Epoch 35/63\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6308 - accuracy: 0.6962\n",
            "Epoch 36/63\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6292 - accuracy: 0.6372\n",
            "Epoch 37/63\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6140 - accuracy: 0.7186\n",
            "Epoch 38/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6347 - accuracy: 0.6384\n",
            "Epoch 39/63\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.6271 - accuracy: 0.6726\n",
            "Epoch 40/63\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6340 - accuracy: 0.6906\n",
            "Epoch 41/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6133 - accuracy: 0.6355\n",
            "Epoch 42/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6266 - accuracy: 0.6408\n",
            "Epoch 43/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6109 - accuracy: 0.6859\n",
            "Epoch 44/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6796 - accuracy: 0.5973\n",
            "Epoch 45/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6171 - accuracy: 0.6791\n",
            "Epoch 46/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6338 - accuracy: 0.6444\n",
            "Epoch 47/63\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6426 - accuracy: 0.6601\n",
            "Epoch 48/63\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6377 - accuracy: 0.6488\n",
            "Epoch 49/63\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6354 - accuracy: 0.6588\n",
            "Epoch 50/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6512 - accuracy: 0.6599\n",
            "Epoch 51/63\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6165 - accuracy: 0.6886\n",
            "Epoch 52/63\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6089 - accuracy: 0.7040\n",
            "Epoch 53/63\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6136 - accuracy: 0.6585\n",
            "Epoch 54/63\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6245 - accuracy: 0.6423\n",
            "Epoch 55/63\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6233 - accuracy: 0.6869\n",
            "Epoch 56/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6562 - accuracy: 0.6313\n",
            "Epoch 57/63\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6060 - accuracy: 0.6883\n",
            "Epoch 58/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6176 - accuracy: 0.7051\n",
            "Epoch 59/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6214 - accuracy: 0.6818\n",
            "Epoch 60/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6528 - accuracy: 0.6519\n",
            "Epoch 61/63\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6021 - accuracy: 0.6823\n",
            "Epoch 62/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6122 - accuracy: 0.6757\n",
            "Epoch 63/63\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5983 - accuracy: 0.6805\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vh_mr5NC188y",
        "outputId": "4d622ade-bb19-4275-b5a2-a6b153e7af4b"
      },
      "source": [
        "# your grid_result object should be able to run in this code \r\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\r\n",
        "means = grid_result.cv_results_['mean_test_score']\r\n",
        "stds = grid_result.cv_results_['std_test_score']\r\n",
        "params = grid_result.cv_results_['params']\r\n",
        "for mean, stdev, param in zip(means, stds, params):\r\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.5875409841537476 using {'batch_size': 32, 'epochs': 63}\n",
            "Means: 0.5113661110401153, Stdev: 0.0990179321183282 with: {'batch_size': 16, 'epochs': 33}\n",
            "Means: 0.544590163230896, Stdev: 0.04661634680777779 with: {'batch_size': 16, 'epochs': 63}\n",
            "Means: 0.5840983510017395, Stdev: 0.06891107794103736 with: {'batch_size': 32, 'epochs': 33}\n",
            "Means: 0.5875409841537476, Stdev: 0.07649290211206625 with: {'batch_size': 32, 'epochs': 63}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThV_a7w32SX5",
        "outputId": "f9cd5ef8-db6c-4641-eca5-d338215fe410"
      },
      "source": [
        "# Define my new search parameters\r\n",
        "param_grid = {\r\n",
        "    'batch_size': [32, 48, 64],\r\n",
        "    'epochs': [7, 39, 45]\r\n",
        "}\r\n",
        "\r\n",
        "# Run Grid Search \r\n",
        "gs = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\r\n",
        "grid_result = gs.fit(X, Y)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 24.2014 - accuracy: 0.5015\n",
            "Epoch 2/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6933 - accuracy: 0.4829\n",
            "Epoch 3/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6769 - accuracy: 0.5855\n",
            "Epoch 4/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6656 - accuracy: 0.6345\n",
            "Epoch 5/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6828 - accuracy: 0.5566\n",
            "Epoch 6/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6573 - accuracy: 0.6344\n",
            "Epoch 7/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6454 - accuracy: 0.6201\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fec3d9158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 0.6317 - accuracy: 0.6393\n",
            "Epoch 1/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 30.8664 - accuracy: 0.4908\n",
            "Epoch 2/7\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.6972 - accuracy: 0.5570\n",
            "Epoch 3/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6923 - accuracy: 0.5696\n",
            "Epoch 4/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6901 - accuracy: 0.5605\n",
            "Epoch 5/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6903 - accuracy: 0.5681\n",
            "Epoch 6/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6877 - accuracy: 0.5784\n",
            "Epoch 7/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6896 - accuracy: 0.5655\n",
            "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe5cb39d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6992 - accuracy: 0.4590\n",
            "Epoch 1/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 27.8413 - accuracy: 0.4677\n",
            "Epoch 2/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6954 - accuracy: 0.4115\n",
            "Epoch 3/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6878 - accuracy: 0.4974\n",
            "Epoch 4/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6914 - accuracy: 0.5242\n",
            "Epoch 5/7\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6901 - accuracy: 0.5746\n",
            "Epoch 6/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6895 - accuracy: 0.6313\n",
            "Epoch 7/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6858 - accuracy: 0.6130\n",
            "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0ff221d840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.6888 - accuracy: 0.5738\n",
            "Epoch 1/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 19.4706 - accuracy: 0.5456\n",
            "Epoch 2/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6675 - accuracy: 0.6045\n",
            "Epoch 3/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6278 - accuracy: 0.7265\n",
            "Epoch 4/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6384 - accuracy: 0.6737\n",
            "Epoch 5/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6423 - accuracy: 0.6778\n",
            "Epoch 6/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6242 - accuracy: 0.6755\n",
            "Epoch 7/7\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6711 - accuracy: 0.6287\n",
            "WARNING:tensorflow:8 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fea20e268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 0.6694 - accuracy: 0.6167\n",
            "Epoch 1/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 19.0585 - accuracy: 0.5451\n",
            "Epoch 2/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.9135 - accuracy: 0.4776\n",
            "Epoch 3/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6270 - accuracy: 0.6586\n",
            "Epoch 4/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6081 - accuracy: 0.6556\n",
            "Epoch 5/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6539 - accuracy: 0.6313\n",
            "Epoch 6/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6377 - accuracy: 0.6412\n",
            "Epoch 7/7\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6161 - accuracy: 0.6821\n",
            "WARNING:tensorflow:7 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe0fbcae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.5903 - accuracy: 0.7000\n",
            "Epoch 1/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 23.7230 - accuracy: 0.5107\n",
            "Epoch 2/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6927 - accuracy: 0.5308\n",
            "Epoch 3/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6925 - accuracy: 0.5360\n",
            "Epoch 4/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6900 - accuracy: 0.5870\n",
            "Epoch 5/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6909 - accuracy: 0.5864\n",
            "Epoch 6/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6871 - accuracy: 0.6237\n",
            "Epoch 7/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6868 - accuracy: 0.6049\n",
            "Epoch 8/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6804 - accuracy: 0.6129\n",
            "Epoch 9/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5626\n",
            "Epoch 10/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6872 - accuracy: 0.5882\n",
            "Epoch 11/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6882 - accuracy: 0.5811\n",
            "Epoch 12/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6845 - accuracy: 0.5682\n",
            "Epoch 13/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6836 - accuracy: 0.5880\n",
            "Epoch 14/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6805 - accuracy: 0.6062\n",
            "Epoch 15/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6808 - accuracy: 0.5937\n",
            "Epoch 16/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6824 - accuracy: 0.6022\n",
            "Epoch 17/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6866 - accuracy: 0.5506\n",
            "Epoch 18/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6755 - accuracy: 0.6186\n",
            "Epoch 19/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6783 - accuracy: 0.6282\n",
            "Epoch 20/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6780 - accuracy: 0.5747\n",
            "Epoch 21/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6828 - accuracy: 0.5820\n",
            "Epoch 22/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6796 - accuracy: 0.5927\n",
            "Epoch 23/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6829 - accuracy: 0.6075\n",
            "Epoch 24/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6745 - accuracy: 0.5922\n",
            "Epoch 25/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6778 - accuracy: 0.6086\n",
            "Epoch 26/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6733 - accuracy: 0.6076\n",
            "Epoch 27/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6770 - accuracy: 0.5828\n",
            "Epoch 28/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6808 - accuracy: 0.6108\n",
            "Epoch 29/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6712 - accuracy: 0.6386\n",
            "Epoch 30/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6728 - accuracy: 0.5591\n",
            "Epoch 31/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6740 - accuracy: 0.6018\n",
            "Epoch 32/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6524 - accuracy: 0.6543\n",
            "Epoch 33/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6776 - accuracy: 0.5999\n",
            "Epoch 34/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6470 - accuracy: 0.6517\n",
            "Epoch 35/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6733 - accuracy: 0.6093\n",
            "Epoch 36/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6696 - accuracy: 0.5968\n",
            "Epoch 37/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6641 - accuracy: 0.6152\n",
            "Epoch 38/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6700 - accuracy: 0.6309\n",
            "Epoch 39/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6848 - accuracy: 0.5576\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0ff101f400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6836 - accuracy: 0.5902\n",
            "Epoch 1/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 39.6710 - accuracy: 0.4859\n",
            "Epoch 2/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6947 - accuracy: 0.5566\n",
            "Epoch 3/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6933 - accuracy: 0.5836\n",
            "Epoch 4/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6922 - accuracy: 0.5522\n",
            "Epoch 5/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6910 - accuracy: 0.5852\n",
            "Epoch 6/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6918 - accuracy: 0.5478\n",
            "Epoch 7/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6922 - accuracy: 0.5312\n",
            "Epoch 8/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6903 - accuracy: 0.5745\n",
            "Epoch 9/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6912 - accuracy: 0.5492\n",
            "Epoch 10/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.5332\n",
            "Epoch 11/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6912 - accuracy: 0.5439\n",
            "Epoch 12/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6899 - accuracy: 0.5638\n",
            "Epoch 13/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6890 - accuracy: 0.5758\n",
            "Epoch 14/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6873 - accuracy: 0.5982\n",
            "Epoch 15/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6894 - accuracy: 0.5628\n",
            "Epoch 16/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6893 - accuracy: 0.5621\n",
            "Epoch 17/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6889 - accuracy: 0.5646\n",
            "Epoch 18/39\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6871 - accuracy: 0.5852\n",
            "Epoch 19/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6887 - accuracy: 0.5636\n",
            "Epoch 20/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6886 - accuracy: 0.5623\n",
            "Epoch 21/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6911 - accuracy: 0.5342\n",
            "Epoch 22/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6857 - accuracy: 0.5906\n",
            "Epoch 23/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6889 - accuracy: 0.5554\n",
            "Epoch 24/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6889 - accuracy: 0.5548\n",
            "Epoch 25/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6902 - accuracy: 0.5419\n",
            "Epoch 26/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6883 - accuracy: 0.5587\n",
            "Epoch 27/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6881 - accuracy: 0.5597\n",
            "Epoch 28/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6892 - accuracy: 0.5488\n",
            "Epoch 29/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6870 - accuracy: 0.5670\n",
            "Epoch 30/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6865 - accuracy: 0.5706\n",
            "Epoch 31/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6914 - accuracy: 0.5298\n",
            "Epoch 32/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6892 - accuracy: 0.5477\n",
            "Epoch 33/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6890 - accuracy: 0.5487\n",
            "Epoch 34/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6849 - accuracy: 0.5793\n",
            "Epoch 35/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6898 - accuracy: 0.5418\n",
            "Epoch 36/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6923 - accuracy: 0.5232\n",
            "Epoch 37/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6839 - accuracy: 0.5840\n",
            "Epoch 38/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6883 - accuracy: 0.5519\n",
            "Epoch 39/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6843 - accuracy: 0.5795\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0ff0fe3bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.7018 - accuracy: 0.4590\n",
            "Epoch 1/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 31.9482 - accuracy: 0.4981\n",
            "Epoch 2/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6899 - accuracy: 0.5337\n",
            "Epoch 3/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6898 - accuracy: 0.5558\n",
            "Epoch 4/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6864 - accuracy: 0.5191\n",
            "Epoch 5/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6859 - accuracy: 0.5545\n",
            "Epoch 6/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6896 - accuracy: 0.5281\n",
            "Epoch 7/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.4983\n",
            "Epoch 8/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6891 - accuracy: 0.5532\n",
            "Epoch 9/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6884 - accuracy: 0.5238\n",
            "Epoch 10/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6900 - accuracy: 0.4974\n",
            "Epoch 11/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6898 - accuracy: 0.5070\n",
            "Epoch 12/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6908 - accuracy: 0.5269\n",
            "Epoch 13/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6905 - accuracy: 0.5355\n",
            "Epoch 14/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6902 - accuracy: 0.5477\n",
            "Epoch 15/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6905 - accuracy: 0.5305\n",
            "Epoch 16/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.5170\n",
            "Epoch 17/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6909 - accuracy: 0.5161\n",
            "Epoch 18/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6923 - accuracy: 0.4889\n",
            "Epoch 19/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.4883\n",
            "Epoch 20/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6889 - accuracy: 0.5044\n",
            "Epoch 21/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6848 - accuracy: 0.5476\n",
            "Epoch 22/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6851 - accuracy: 0.5386\n",
            "Epoch 23/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6903 - accuracy: 0.5479\n",
            "Epoch 24/39\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6886 - accuracy: 0.5355\n",
            "Epoch 25/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6898 - accuracy: 0.5474\n",
            "Epoch 26/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6911 - accuracy: 0.4994\n",
            "Epoch 27/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6896 - accuracy: 0.5493\n",
            "Epoch 28/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6878 - accuracy: 0.5257\n",
            "Epoch 29/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6899 - accuracy: 0.5414\n",
            "Epoch 30/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6885 - accuracy: 0.5336\n",
            "Epoch 31/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6907 - accuracy: 0.5342\n",
            "Epoch 32/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6933 - accuracy: 0.4896\n",
            "Epoch 33/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6872 - accuracy: 0.5352\n",
            "Epoch 34/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6884 - accuracy: 0.5152\n",
            "Epoch 35/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6877 - accuracy: 0.5265\n",
            "Epoch 36/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6901 - accuracy: 0.5409\n",
            "Epoch 37/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6902 - accuracy: 0.5260\n",
            "Epoch 38/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6882 - accuracy: 0.5559\n",
            "Epoch 39/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6840 - accuracy: 0.5479\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0ff0fe3e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 0.6876 - accuracy: 0.5902\n",
            "Epoch 1/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 24.7553 - accuracy: 0.4706\n",
            "Epoch 2/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6914 - accuracy: 0.5627\n",
            "Epoch 3/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.5247\n",
            "Epoch 4/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6897 - accuracy: 0.5790\n",
            "Epoch 5/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6886 - accuracy: 0.5747\n",
            "Epoch 6/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6898 - accuracy: 0.5153\n",
            "Epoch 7/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6874 - accuracy: 0.5578\n",
            "Epoch 8/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6820 - accuracy: 0.5696\n",
            "Epoch 9/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6759 - accuracy: 0.6142\n",
            "Epoch 10/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6472 - accuracy: 0.6626\n",
            "Epoch 11/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6561 - accuracy: 0.5943\n",
            "Epoch 12/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6468 - accuracy: 0.7089\n",
            "Epoch 13/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6532 - accuracy: 0.6413\n",
            "Epoch 14/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6601 - accuracy: 0.6151\n",
            "Epoch 15/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6389 - accuracy: 0.6409\n",
            "Epoch 16/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6599 - accuracy: 0.6272\n",
            "Epoch 17/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6420 - accuracy: 0.6189\n",
            "Epoch 18/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6628 - accuracy: 0.6224\n",
            "Epoch 19/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6581 - accuracy: 0.6185\n",
            "Epoch 20/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6362 - accuracy: 0.6274\n",
            "Epoch 21/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6787 - accuracy: 0.6075\n",
            "Epoch 22/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6761 - accuracy: 0.5962\n",
            "Epoch 23/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6185 - accuracy: 0.6654\n",
            "Epoch 24/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6464 - accuracy: 0.6274\n",
            "Epoch 25/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6528 - accuracy: 0.5917\n",
            "Epoch 26/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6169 - accuracy: 0.6733\n",
            "Epoch 27/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6011 - accuracy: 0.6971\n",
            "Epoch 28/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6808 - accuracy: 0.5380\n",
            "Epoch 29/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6725 - accuracy: 0.5431\n",
            "Epoch 30/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6255 - accuracy: 0.6468\n",
            "Epoch 31/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6333 - accuracy: 0.6661\n",
            "Epoch 32/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6834 - accuracy: 0.5973\n",
            "Epoch 33/39\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6618 - accuracy: 0.6197\n",
            "Epoch 34/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6285 - accuracy: 0.6442\n",
            "Epoch 35/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6384 - accuracy: 0.6420\n",
            "Epoch 36/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6110 - accuracy: 0.6675\n",
            "Epoch 37/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6376 - accuracy: 0.6149\n",
            "Epoch 38/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6816 - accuracy: 0.6031\n",
            "Epoch 39/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6724 - accuracy: 0.5995\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe654b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 0.6907 - accuracy: 0.6167\n",
            "Epoch 1/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 33.9523 - accuracy: 0.5114\n",
            "Epoch 2/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6892 - accuracy: 0.4781\n",
            "Epoch 3/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6885 - accuracy: 0.5692\n",
            "Epoch 4/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6880 - accuracy: 0.5427\n",
            "Epoch 5/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6906 - accuracy: 0.5132\n",
            "Epoch 6/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6912 - accuracy: 0.5885\n",
            "Epoch 7/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6896 - accuracy: 0.5769\n",
            "Epoch 8/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6912 - accuracy: 0.5298\n",
            "Epoch 9/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6914 - accuracy: 0.5368\n",
            "Epoch 10/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6864 - accuracy: 0.5485\n",
            "Epoch 11/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6909 - accuracy: 0.5509\n",
            "Epoch 12/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6881 - accuracy: 0.5502\n",
            "Epoch 13/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6898 - accuracy: 0.5668\n",
            "Epoch 14/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6902 - accuracy: 0.5318\n",
            "Epoch 15/39\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6898 - accuracy: 0.5183\n",
            "Epoch 16/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6873 - accuracy: 0.5586\n",
            "Epoch 17/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6894 - accuracy: 0.5294\n",
            "Epoch 18/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6900 - accuracy: 0.5295\n",
            "Epoch 19/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6870 - accuracy: 0.5571\n",
            "Epoch 20/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6884 - accuracy: 0.5234\n",
            "Epoch 21/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6904 - accuracy: 0.5381\n",
            "Epoch 22/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6912 - accuracy: 0.5291\n",
            "Epoch 23/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6878 - accuracy: 0.5551\n",
            "Epoch 24/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6890 - accuracy: 0.5453\n",
            "Epoch 25/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6890 - accuracy: 0.5456\n",
            "Epoch 26/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6851 - accuracy: 0.5446\n",
            "Epoch 27/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.4949\n",
            "Epoch 28/39\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6894 - accuracy: 0.5442\n",
            "Epoch 29/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6884 - accuracy: 0.5210\n",
            "Epoch 30/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6913 - accuracy: 0.5284\n",
            "Epoch 31/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6917 - accuracy: 0.5224\n",
            "Epoch 32/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6928 - accuracy: 0.5044\n",
            "Epoch 33/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6900 - accuracy: 0.5334\n",
            "Epoch 34/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6845 - accuracy: 0.5462\n",
            "Epoch 35/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6865 - accuracy: 0.5483\n",
            "Epoch 36/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6876 - accuracy: 0.5566\n",
            "Epoch 37/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6860 - accuracy: 0.5672\n",
            "Epoch 38/39\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6895 - accuracy: 0.5427\n",
            "Epoch 39/39\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6886 - accuracy: 0.5410\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe2059048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.6888 - accuracy: 0.5667\n",
            "Epoch 1/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 16.9404 - accuracy: 0.5677\n",
            "Epoch 2/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6804 - accuracy: 0.5939\n",
            "Epoch 3/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6455 - accuracy: 0.6347\n",
            "Epoch 4/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6720 - accuracy: 0.5685\n",
            "Epoch 5/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6765 - accuracy: 0.6312\n",
            "Epoch 6/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6566 - accuracy: 0.6475\n",
            "Epoch 7/45\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6329 - accuracy: 0.6432\n",
            "Epoch 8/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6282 - accuracy: 0.6575\n",
            "Epoch 9/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7695 - accuracy: 0.5834\n",
            "Epoch 10/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6420 - accuracy: 0.6586\n",
            "Epoch 11/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6047 - accuracy: 0.6854\n",
            "Epoch 12/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5921 - accuracy: 0.6696\n",
            "Epoch 13/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7818 - accuracy: 0.5642\n",
            "Epoch 14/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6784 - accuracy: 0.5949\n",
            "Epoch 15/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6603 - accuracy: 0.6684\n",
            "Epoch 16/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6534 - accuracy: 0.6255\n",
            "Epoch 17/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6204 - accuracy: 0.6666\n",
            "Epoch 18/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6246 - accuracy: 0.5982\n",
            "Epoch 19/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6182 - accuracy: 0.6434\n",
            "Epoch 20/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6413 - accuracy: 0.6061\n",
            "Epoch 21/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6588 - accuracy: 0.6238\n",
            "Epoch 22/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6764 - accuracy: 0.6468\n",
            "Epoch 23/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6456 - accuracy: 0.5579\n",
            "Epoch 24/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6681 - accuracy: 0.6088\n",
            "Epoch 25/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6793 - accuracy: 0.6149\n",
            "Epoch 26/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6050 - accuracy: 0.6826\n",
            "Epoch 27/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6356 - accuracy: 0.5753\n",
            "Epoch 28/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6207 - accuracy: 0.6869\n",
            "Epoch 29/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5378\n",
            "Epoch 30/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6532 - accuracy: 0.5739\n",
            "Epoch 31/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6547 - accuracy: 0.5406\n",
            "Epoch 32/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6650 - accuracy: 0.5063\n",
            "Epoch 33/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6333 - accuracy: 0.5816\n",
            "Epoch 34/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6300 - accuracy: 0.5561\n",
            "Epoch 35/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6518 - accuracy: 0.5444\n",
            "Epoch 36/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6846 - accuracy: 0.6309\n",
            "Epoch 37/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6637 - accuracy: 0.6430\n",
            "Epoch 38/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6715 - accuracy: 0.6197\n",
            "Epoch 39/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.7292 - accuracy: 0.6008\n",
            "Epoch 40/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6629 - accuracy: 0.5894\n",
            "Epoch 41/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6332 - accuracy: 0.6422\n",
            "Epoch 42/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6292 - accuracy: 0.6519\n",
            "Epoch 43/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6349 - accuracy: 0.6200\n",
            "Epoch 44/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6060 - accuracy: 0.6518\n",
            "Epoch 45/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6701 - accuracy: 0.6248\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe4b0b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 0.6781 - accuracy: 0.6066\n",
            "Epoch 1/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 23.2215 - accuracy: 0.5841\n",
            "Epoch 2/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6864 - accuracy: 0.6344\n",
            "Epoch 3/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6845 - accuracy: 0.6579\n",
            "Epoch 4/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6860 - accuracy: 0.6349\n",
            "Epoch 5/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6811 - accuracy: 0.6395\n",
            "Epoch 6/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6793 - accuracy: 0.6618\n",
            "Epoch 7/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6725 - accuracy: 0.6330\n",
            "Epoch 8/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6786 - accuracy: 0.6658\n",
            "Epoch 9/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6722 - accuracy: 0.6501\n",
            "Epoch 10/45\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6745 - accuracy: 0.6900\n",
            "Epoch 11/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6644 - accuracy: 0.6614\n",
            "Epoch 12/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6643 - accuracy: 0.7129\n",
            "Epoch 13/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6745 - accuracy: 0.6159\n",
            "Epoch 14/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6652 - accuracy: 0.6747\n",
            "Epoch 15/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6726 - accuracy: 0.6367\n",
            "Epoch 16/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6683 - accuracy: 0.6568\n",
            "Epoch 17/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6723 - accuracy: 0.6242\n",
            "Epoch 18/45\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6627 - accuracy: 0.6584\n",
            "Epoch 19/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6558 - accuracy: 0.6360\n",
            "Epoch 20/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6607 - accuracy: 0.6648\n",
            "Epoch 21/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6507 - accuracy: 0.6776\n",
            "Epoch 22/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6496 - accuracy: 0.6910\n",
            "Epoch 23/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6381 - accuracy: 0.6218\n",
            "Epoch 24/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6736 - accuracy: 0.6090\n",
            "Epoch 25/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5900 - accuracy: 0.6874\n",
            "Epoch 26/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6251 - accuracy: 0.6626\n",
            "Epoch 27/45\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6323 - accuracy: 0.6738\n",
            "Epoch 28/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6347 - accuracy: 0.6462\n",
            "Epoch 29/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6351 - accuracy: 0.6691\n",
            "Epoch 30/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6329 - accuracy: 0.6263\n",
            "Epoch 31/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6016 - accuracy: 0.6585\n",
            "Epoch 32/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6200 - accuracy: 0.6530\n",
            "Epoch 33/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6139 - accuracy: 0.6739\n",
            "Epoch 34/45\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6626 - accuracy: 0.6227\n",
            "Epoch 35/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6559 - accuracy: 0.6124\n",
            "Epoch 36/45\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6167 - accuracy: 0.6702\n",
            "Epoch 37/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6292 - accuracy: 0.6172\n",
            "Epoch 38/45\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6043 - accuracy: 0.6871\n",
            "Epoch 39/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6161 - accuracy: 0.6558\n",
            "Epoch 40/45\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6075 - accuracy: 0.6891\n",
            "Epoch 41/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6071 - accuracy: 0.6862\n",
            "Epoch 42/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6216 - accuracy: 0.6649\n",
            "Epoch 43/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6258 - accuracy: 0.6620\n",
            "Epoch 44/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6281 - accuracy: 0.6778\n",
            "Epoch 45/45\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6046 - accuracy: 0.6909\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0feaa688c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6256 - accuracy: 0.6557\n",
            "Epoch 1/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 69.8680 - accuracy: 0.5166\n",
            "Epoch 2/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6860 - accuracy: 0.4745\n",
            "Epoch 3/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6785 - accuracy: 0.5190\n",
            "Epoch 4/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.7064 - accuracy: 0.5017\n",
            "Epoch 5/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6827 - accuracy: 0.5311\n",
            "Epoch 6/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7030 - accuracy: 0.5087\n",
            "Epoch 7/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6744 - accuracy: 0.5423\n",
            "Epoch 8/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6614 - accuracy: 0.6405\n",
            "Epoch 9/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6528 - accuracy: 0.6418\n",
            "Epoch 10/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6680 - accuracy: 0.5997\n",
            "Epoch 11/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6542 - accuracy: 0.6145\n",
            "Epoch 12/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6646 - accuracy: 0.5793\n",
            "Epoch 13/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6665 - accuracy: 0.6011\n",
            "Epoch 14/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6588 - accuracy: 0.5909\n",
            "Epoch 15/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6572 - accuracy: 0.6199\n",
            "Epoch 16/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6639 - accuracy: 0.6640\n",
            "Epoch 17/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6638 - accuracy: 0.5552\n",
            "Epoch 18/45\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6615 - accuracy: 0.6367\n",
            "Epoch 19/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6497 - accuracy: 0.6559\n",
            "Epoch 20/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6407 - accuracy: 0.6279\n",
            "Epoch 21/45\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6338 - accuracy: 0.6458\n",
            "Epoch 22/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6389 - accuracy: 0.6394\n",
            "Epoch 23/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6429 - accuracy: 0.6518\n",
            "Epoch 24/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6539 - accuracy: 0.6194\n",
            "Epoch 25/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6369 - accuracy: 0.6858\n",
            "Epoch 26/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6493 - accuracy: 0.6518\n",
            "Epoch 27/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6389 - accuracy: 0.6815\n",
            "Epoch 28/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6564 - accuracy: 0.5962\n",
            "Epoch 29/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6185 - accuracy: 0.6600\n",
            "Epoch 30/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6437 - accuracy: 0.6283\n",
            "Epoch 31/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6485 - accuracy: 0.6178\n",
            "Epoch 32/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6481 - accuracy: 0.6361\n",
            "Epoch 33/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6424 - accuracy: 0.6614\n",
            "Epoch 34/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6711 - accuracy: 0.6156\n",
            "Epoch 35/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7412 - accuracy: 0.4552\n",
            "Epoch 36/45\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6407 - accuracy: 0.6608\n",
            "Epoch 37/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5924 - accuracy: 0.6674\n",
            "Epoch 38/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6162 - accuracy: 0.6723\n",
            "Epoch 39/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6221 - accuracy: 0.6768\n",
            "Epoch 40/45\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6442 - accuracy: 0.6324\n",
            "Epoch 41/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6350 - accuracy: 0.7219\n",
            "Epoch 42/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6368 - accuracy: 0.6829\n",
            "Epoch 43/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6456 - accuracy: 0.6325\n",
            "Epoch 44/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6391 - accuracy: 0.6836\n",
            "Epoch 45/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6509 - accuracy: 0.5922\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0feaa680d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.6609 - accuracy: 0.5738\n",
            "Epoch 1/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 35.7675 - accuracy: 0.5067\n",
            "Epoch 2/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6443 - accuracy: 0.6358\n",
            "Epoch 3/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7140 - accuracy: 0.5794\n",
            "Epoch 4/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6114 - accuracy: 0.6769\n",
            "Epoch 5/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.8003 - accuracy: 0.5377\n",
            "Epoch 6/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6583 - accuracy: 0.6271\n",
            "Epoch 7/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6655 - accuracy: 0.5908\n",
            "Epoch 8/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6071 - accuracy: 0.6944\n",
            "Epoch 9/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6144 - accuracy: 0.6733\n",
            "Epoch 10/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6350 - accuracy: 0.6425\n",
            "Epoch 11/45\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7055 - accuracy: 0.6152\n",
            "Epoch 12/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6149 - accuracy: 0.6466\n",
            "Epoch 13/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5833 - accuracy: 0.7349\n",
            "Epoch 14/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5976 - accuracy: 0.6981\n",
            "Epoch 15/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5995 - accuracy: 0.7185\n",
            "Epoch 16/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6067 - accuracy: 0.6600\n",
            "Epoch 17/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5908 - accuracy: 0.6913\n",
            "Epoch 18/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5780 - accuracy: 0.6927\n",
            "Epoch 19/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5758 - accuracy: 0.7230\n",
            "Epoch 20/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5977 - accuracy: 0.6542\n",
            "Epoch 21/45\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6252 - accuracy: 0.6482\n",
            "Epoch 22/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6050 - accuracy: 0.6892\n",
            "Epoch 23/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5862 - accuracy: 0.6839\n",
            "Epoch 24/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6035 - accuracy: 0.6708\n",
            "Epoch 25/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5863 - accuracy: 0.6799\n",
            "Epoch 26/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5957 - accuracy: 0.6871\n",
            "Epoch 27/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5858 - accuracy: 0.6883\n",
            "Epoch 28/45\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6090 - accuracy: 0.6787\n",
            "Epoch 29/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5660 - accuracy: 0.7221\n",
            "Epoch 30/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6705 - accuracy: 0.6101\n",
            "Epoch 31/45\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5957 - accuracy: 0.7010\n",
            "Epoch 32/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5721 - accuracy: 0.7407\n",
            "Epoch 33/45\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6041 - accuracy: 0.6655\n",
            "Epoch 34/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6075 - accuracy: 0.6741\n",
            "Epoch 35/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6201 - accuracy: 0.6788\n",
            "Epoch 36/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5853 - accuracy: 0.6763\n",
            "Epoch 37/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5809 - accuracy: 0.6949\n",
            "Epoch 38/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6227 - accuracy: 0.6662\n",
            "Epoch 39/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6007 - accuracy: 0.6479\n",
            "Epoch 40/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6638 - accuracy: 0.6086\n",
            "Epoch 41/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6595 - accuracy: 0.6421\n",
            "Epoch 42/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5805 - accuracy: 0.6949\n",
            "Epoch 43/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6274 - accuracy: 0.6413\n",
            "Epoch 44/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5654 - accuracy: 0.7295\n",
            "Epoch 45/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6098 - accuracy: 0.7035\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fecc691e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 0.6739 - accuracy: 0.5833\n",
            "Epoch 1/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 84.0967 - accuracy: 0.4515\n",
            "Epoch 2/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6796 - accuracy: 0.5294\n",
            "Epoch 3/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6802 - accuracy: 0.5819\n",
            "Epoch 4/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6837 - accuracy: 0.5348\n",
            "Epoch 5/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6843 - accuracy: 0.5191\n",
            "Epoch 6/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6819 - accuracy: 0.5295\n",
            "Epoch 7/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6844 - accuracy: 0.5814\n",
            "Epoch 8/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6434 - accuracy: 0.6587\n",
            "Epoch 9/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6384 - accuracy: 0.6064\n",
            "Epoch 10/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6680 - accuracy: 0.6125\n",
            "Epoch 11/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6593 - accuracy: 0.6118\n",
            "Epoch 12/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6766 - accuracy: 0.6334\n",
            "Epoch 13/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6573 - accuracy: 0.6387\n",
            "Epoch 14/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6778 - accuracy: 0.5640\n",
            "Epoch 15/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6675 - accuracy: 0.5857\n",
            "Epoch 16/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6696 - accuracy: 0.5956\n",
            "Epoch 17/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6434 - accuracy: 0.6306\n",
            "Epoch 18/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6522 - accuracy: 0.6246\n",
            "Epoch 19/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6315 - accuracy: 0.6135\n",
            "Epoch 20/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6588 - accuracy: 0.6096\n",
            "Epoch 21/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6714 - accuracy: 0.6194\n",
            "Epoch 22/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6437 - accuracy: 0.6356\n",
            "Epoch 23/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6405 - accuracy: 0.6128\n",
            "Epoch 24/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6562 - accuracy: 0.6600\n",
            "Epoch 25/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6174\n",
            "Epoch 26/45\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6347 - accuracy: 0.6062\n",
            "Epoch 27/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6201 - accuracy: 0.6248\n",
            "Epoch 28/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6483 - accuracy: 0.6082\n",
            "Epoch 29/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6221 - accuracy: 0.6728\n",
            "Epoch 30/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6429 - accuracy: 0.6238\n",
            "Epoch 31/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6264 - accuracy: 0.6396\n",
            "Epoch 32/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.6312\n",
            "Epoch 33/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6218 - accuracy: 0.6840\n",
            "Epoch 34/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6534 - accuracy: 0.6298\n",
            "Epoch 35/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6691 - accuracy: 0.6471\n",
            "Epoch 36/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5990 - accuracy: 0.6604\n",
            "Epoch 37/45\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6426 - accuracy: 0.6181\n",
            "Epoch 38/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6770 - accuracy: 0.6545\n",
            "Epoch 39/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6640 - accuracy: 0.6345\n",
            "Epoch 40/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6928 - accuracy: 0.5114\n",
            "Epoch 41/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6919 - accuracy: 0.5387\n",
            "Epoch 42/45\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6900 - accuracy: 0.5734\n",
            "Epoch 43/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6844 - accuracy: 0.6046\n",
            "Epoch 44/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6569 - accuracy: 0.6428\n",
            "Epoch 45/45\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6549\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fec3d9620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 0.6835 - accuracy: 0.6833\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 6.6245 - accuracy: 0.4452\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6807 - accuracy: 0.5390\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6826 - accuracy: 0.6308\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.7014 - accuracy: 0.5924\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6697 - accuracy: 0.6557\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6876 - accuracy: 0.5749\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6810 - accuracy: 0.6155\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe5c50598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.6747 - accuracy: 0.6230\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 41.7368 - accuracy: 0.4999\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.7206 - accuracy: 0.5205\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.7251 - accuracy: 0.5683\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.7586 - accuracy: 0.5687\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.7201 - accuracy: 0.4619\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6852 - accuracy: 0.5352\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6657 - accuracy: 0.5929\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe5cb3bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6213 - accuracy: 0.5410\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 52.6509 - accuracy: 0.5013\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6999 - accuracy: 0.4700\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6949 - accuracy: 0.4611\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6943 - accuracy: 0.4503\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6935 - accuracy: 0.5184\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.4642\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6902 - accuracy: 0.5448\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe4b45bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.6931 - accuracy: 0.5902\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 119.2529 - accuracy: 0.4900\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 1.4296 - accuracy: 0.5282\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.7897 - accuracy: 0.4690\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6838 - accuracy: 0.5940\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6835 - accuracy: 0.5390\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6686 - accuracy: 0.5690\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6904 - accuracy: 0.5267\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0ff0faeb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 0.6813 - accuracy: 0.5667\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 39.4240 - accuracy: 0.4869\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6995 - accuracy: 0.5430\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.7065 - accuracy: 0.5037\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.7531 - accuracy: 0.5331\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6967 - accuracy: 0.5317\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6975 - accuracy: 0.5112\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6982 - accuracy: 0.3878\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fecc81d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.7060 - accuracy: 0.4333\n",
            "Epoch 1/39\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 39.1869 - accuracy: 0.5075\n",
            "Epoch 2/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4630\n",
            "Epoch 3/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6934 - accuracy: 0.4428\n",
            "Epoch 4/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6933 - accuracy: 0.4476\n",
            "Epoch 5/39\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.4768\n",
            "Epoch 6/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5331\n",
            "Epoch 7/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5539\n",
            "Epoch 8/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5003\n",
            "Epoch 9/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6926 - accuracy: 0.5510\n",
            "Epoch 10/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5001\n",
            "Epoch 11/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6927 - accuracy: 0.5295\n",
            "Epoch 12/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6928 - accuracy: 0.5281\n",
            "Epoch 13/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6926 - accuracy: 0.5418\n",
            "Epoch 14/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6921 - accuracy: 0.5709\n",
            "Epoch 15/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6925 - accuracy: 0.5559\n",
            "Epoch 16/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6923 - accuracy: 0.5624\n",
            "Epoch 17/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6930 - accuracy: 0.5100\n",
            "Epoch 18/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6924 - accuracy: 0.5439\n",
            "Epoch 19/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6923 - accuracy: 0.5593\n",
            "Epoch 20/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5050\n",
            "Epoch 21/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6923 - accuracy: 0.5368\n",
            "Epoch 22/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.5523\n",
            "Epoch 23/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6925 - accuracy: 0.5229\n",
            "Epoch 24/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6926 - accuracy: 0.5204\n",
            "Epoch 25/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6908 - accuracy: 0.5673\n",
            "Epoch 26/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6920 - accuracy: 0.5342\n",
            "Epoch 27/39\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.6907 - accuracy: 0.5610\n",
            "Epoch 28/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.5110\n",
            "Epoch 29/39\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.6912 - accuracy: 0.5526\n",
            "Epoch 30/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6903 - accuracy: 0.5658\n",
            "Epoch 31/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6916 - accuracy: 0.5372\n",
            "Epoch 32/39\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6926 - accuracy: 0.5171\n",
            "Epoch 33/39\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6904 - accuracy: 0.5601\n",
            "Epoch 34/39\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6915 - accuracy: 0.5378\n",
            "Epoch 35/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6908 - accuracy: 0.5500\n",
            "Epoch 36/39\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6929 - accuracy: 0.5109\n",
            "Epoch 37/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6918 - accuracy: 0.5293\n",
            "Epoch 38/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6902 - accuracy: 0.5524\n",
            "Epoch 39/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6926 - accuracy: 0.5165\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0feef367b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 0.6887 - accuracy: 0.5738\n",
            "Epoch 1/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 27.2853 - accuracy: 0.5288\n",
            "Epoch 2/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6937 - accuracy: 0.4572\n",
            "Epoch 3/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5577\n",
            "Epoch 4/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6917 - accuracy: 0.6290\n",
            "Epoch 5/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6915 - accuracy: 0.5867\n",
            "Epoch 6/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6922 - accuracy: 0.5434\n",
            "Epoch 7/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6913 - accuracy: 0.5625\n",
            "Epoch 8/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6907 - accuracy: 0.5864\n",
            "Epoch 9/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6922 - accuracy: 0.5286\n",
            "Epoch 10/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6901 - accuracy: 0.5721\n",
            "Epoch 11/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6902 - accuracy: 0.5655\n",
            "Epoch 12/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6904 - accuracy: 0.5488\n",
            "Epoch 13/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6902 - accuracy: 0.5438\n",
            "Epoch 14/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6912 - accuracy: 0.5397\n",
            "Epoch 15/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6908 - accuracy: 0.5436\n",
            "Epoch 16/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6877 - accuracy: 0.5938\n",
            "Epoch 17/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6893 - accuracy: 0.5629\n",
            "Epoch 18/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6902 - accuracy: 0.5489\n",
            "Epoch 19/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6896 - accuracy: 0.5546\n",
            "Epoch 20/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6879 - accuracy: 0.5772\n",
            "Epoch 21/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6889 - accuracy: 0.5611\n",
            "Epoch 22/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6898 - accuracy: 0.5485\n",
            "Epoch 23/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6886 - accuracy: 0.5607\n",
            "Epoch 24/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6881 - accuracy: 0.5650\n",
            "Epoch 25/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6889 - accuracy: 0.5572\n",
            "Epoch 26/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6903 - accuracy: 0.5426\n",
            "Epoch 27/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6892 - accuracy: 0.5535\n",
            "Epoch 28/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6847 - accuracy: 0.5989\n",
            "Epoch 29/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5769\n",
            "Epoch 30/39\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5241\n",
            "Epoch 31/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6886 - accuracy: 0.5564\n",
            "Epoch 32/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6897 - accuracy: 0.5460\n",
            "Epoch 33/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6849 - accuracy: 0.5902\n",
            "Epoch 34/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6875 - accuracy: 0.5660\n",
            "Epoch 35/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6876 - accuracy: 0.5654\n",
            "Epoch 36/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6901 - accuracy: 0.5420\n",
            "Epoch 37/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6864 - accuracy: 0.5731\n",
            "Epoch 38/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6877 - accuracy: 0.5616\n",
            "Epoch 39/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6904 - accuracy: 0.5380\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fed53e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.7001 - accuracy: 0.4590\n",
            "Epoch 1/39\n",
            "6/6 [==============================] - 1s 3ms/step - loss: 53.6706 - accuracy: 0.5261\n",
            "Epoch 2/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.7173 - accuracy: 0.4751\n",
            "Epoch 3/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6754 - accuracy: 0.5776\n",
            "Epoch 4/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6700 - accuracy: 0.5703\n",
            "Epoch 5/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6625 - accuracy: 0.6044\n",
            "Epoch 6/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6661 - accuracy: 0.6341\n",
            "Epoch 7/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6496 - accuracy: 0.5908\n",
            "Epoch 8/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6532 - accuracy: 0.5940\n",
            "Epoch 9/39\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6560 - accuracy: 0.5921\n",
            "Epoch 10/39\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6435 - accuracy: 0.6539\n",
            "Epoch 11/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6769 - accuracy: 0.5887\n",
            "Epoch 12/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6493 - accuracy: 0.6398\n",
            "Epoch 13/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6425 - accuracy: 0.6341\n",
            "Epoch 14/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6511 - accuracy: 0.6461\n",
            "Epoch 15/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6331 - accuracy: 0.6499\n",
            "Epoch 16/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6889 - accuracy: 0.5926\n",
            "Epoch 17/39\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6972 - accuracy: 0.6343\n",
            "Epoch 18/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.8579 - accuracy: 0.5703\n",
            "Epoch 19/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6307 - accuracy: 0.6742\n",
            "Epoch 20/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6450 - accuracy: 0.6660\n",
            "Epoch 21/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6360 - accuracy: 0.6440\n",
            "Epoch 22/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6513 - accuracy: 0.6137\n",
            "Epoch 23/39\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.7012 - accuracy: 0.6102\n",
            "Epoch 24/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6721 - accuracy: 0.5816\n",
            "Epoch 25/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6341 - accuracy: 0.6852\n",
            "Epoch 26/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6323 - accuracy: 0.6602\n",
            "Epoch 27/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6642 - accuracy: 0.5761\n",
            "Epoch 28/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6469 - accuracy: 0.6067\n",
            "Epoch 29/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.7745 - accuracy: 0.6244\n",
            "Epoch 30/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6442 - accuracy: 0.6632\n",
            "Epoch 31/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6108 - accuracy: 0.6761\n",
            "Epoch 32/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6461 - accuracy: 0.6339\n",
            "Epoch 33/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6863 - accuracy: 0.6347\n",
            "Epoch 34/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6106 - accuracy: 0.6838\n",
            "Epoch 35/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6500 - accuracy: 0.6503\n",
            "Epoch 36/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6175 - accuracy: 0.6589\n",
            "Epoch 37/39\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.6531 - accuracy: 0.6391\n",
            "Epoch 38/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6208 - accuracy: 0.6787\n",
            "Epoch 39/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6126 - accuracy: 0.6934\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe90a1048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.6443 - accuracy: 0.5738\n",
            "Epoch 1/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 51.3548 - accuracy: 0.4028\n",
            "Epoch 2/39\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6906 - accuracy: 0.5511\n",
            "Epoch 3/39\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6834 - accuracy: 0.6258\n",
            "Epoch 4/39\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6755 - accuracy: 0.6575\n",
            "Epoch 5/39\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.6696 - accuracy: 0.5874\n",
            "Epoch 6/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6531 - accuracy: 0.6137\n",
            "Epoch 7/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6663 - accuracy: 0.6455\n",
            "Epoch 8/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6486 - accuracy: 0.6263\n",
            "Epoch 9/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6438 - accuracy: 0.6194\n",
            "Epoch 10/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6673 - accuracy: 0.6520\n",
            "Epoch 11/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6572 - accuracy: 0.6282\n",
            "Epoch 12/39\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.7709 - accuracy: 0.5860\n",
            "Epoch 13/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6598 - accuracy: 0.6316\n",
            "Epoch 14/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6491 - accuracy: 0.6260\n",
            "Epoch 15/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6722 - accuracy: 0.6489\n",
            "Epoch 16/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6625 - accuracy: 0.6210\n",
            "Epoch 17/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6472 - accuracy: 0.6157\n",
            "Epoch 18/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6168 - accuracy: 0.6332\n",
            "Epoch 19/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6021 - accuracy: 0.6711\n",
            "Epoch 20/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6153 - accuracy: 0.6714\n",
            "Epoch 21/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6835 - accuracy: 0.6084\n",
            "Epoch 22/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6880 - accuracy: 0.4803\n",
            "Epoch 23/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6884 - accuracy: 0.4426\n",
            "Epoch 24/39\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6866 - accuracy: 0.4715\n",
            "Epoch 25/39\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6869 - accuracy: 0.4419\n",
            "Epoch 26/39\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6870 - accuracy: 0.4305\n",
            "Epoch 27/39\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6831 - accuracy: 0.4235\n",
            "Epoch 28/39\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6929 - accuracy: 0.4726\n",
            "Epoch 29/39\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6922 - accuracy: 0.5820\n",
            "Epoch 30/39\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6913 - accuracy: 0.6078\n",
            "Epoch 31/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6903 - accuracy: 0.6140\n",
            "Epoch 32/39\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6879 - accuracy: 0.6162\n",
            "Epoch 33/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6836 - accuracy: 0.6153\n",
            "Epoch 34/39\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6824 - accuracy: 0.6170\n",
            "Epoch 35/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6754 - accuracy: 0.6421\n",
            "Epoch 36/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6777 - accuracy: 0.6575\n",
            "Epoch 37/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6764 - accuracy: 0.6293\n",
            "Epoch 38/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6753 - accuracy: 0.5959\n",
            "Epoch 39/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6906 - accuracy: 0.5741\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe5cb3ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.6952 - accuracy: 0.5167\n",
            "Epoch 1/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 67.7419 - accuracy: 0.5164\n",
            "Epoch 2/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.7178 - accuracy: 0.5058\n",
            "Epoch 3/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6935 - accuracy: 0.4082\n",
            "Epoch 4/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6847 - accuracy: 0.4551\n",
            "Epoch 5/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6955 - accuracy: 0.4566\n",
            "Epoch 6/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6797 - accuracy: 0.4667\n",
            "Epoch 7/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6713 - accuracy: 0.5611\n",
            "Epoch 8/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6846 - accuracy: 0.5815\n",
            "Epoch 9/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6744 - accuracy: 0.6587\n",
            "Epoch 10/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6642 - accuracy: 0.6745\n",
            "Epoch 11/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6485 - accuracy: 0.5921\n",
            "Epoch 12/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.8370 - accuracy: 0.5646\n",
            "Epoch 13/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6666 - accuracy: 0.6181\n",
            "Epoch 14/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6638 - accuracy: 0.6090\n",
            "Epoch 15/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6611 - accuracy: 0.6525\n",
            "Epoch 16/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6626 - accuracy: 0.6567\n",
            "Epoch 17/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.7321 - accuracy: 0.6473\n",
            "Epoch 18/39\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.6558 - accuracy: 0.6288\n",
            "Epoch 19/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6938 - accuracy: 0.6160\n",
            "Epoch 20/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6612 - accuracy: 0.6125\n",
            "Epoch 21/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6739 - accuracy: 0.6141\n",
            "Epoch 22/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6588 - accuracy: 0.6068\n",
            "Epoch 23/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6428 - accuracy: 0.6386\n",
            "Epoch 24/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6384 - accuracy: 0.6408\n",
            "Epoch 25/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6523 - accuracy: 0.6127\n",
            "Epoch 26/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6822 - accuracy: 0.5187\n",
            "Epoch 27/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6748 - accuracy: 0.5293\n",
            "Epoch 28/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6703 - accuracy: 0.5449\n",
            "Epoch 29/39\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6473 - accuracy: 0.6338\n",
            "Epoch 30/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6173 - accuracy: 0.6407\n",
            "Epoch 31/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6387 - accuracy: 0.6442\n",
            "Epoch 32/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.8300 - accuracy: 0.5382\n",
            "Epoch 33/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6812 - accuracy: 0.5551\n",
            "Epoch 34/39\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6644 - accuracy: 0.6118\n",
            "Epoch 35/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6565 - accuracy: 0.6137\n",
            "Epoch 36/39\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6591 - accuracy: 0.6047\n",
            "Epoch 37/39\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.6619 - accuracy: 0.6054\n",
            "Epoch 38/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6287 - accuracy: 0.6305\n",
            "Epoch 39/39\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6558 - accuracy: 0.5678\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe7f16048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6769 - accuracy: 0.6000\n",
            "Epoch 1/45\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 65.2779 - accuracy: 0.5224\n",
            "Epoch 2/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.7041 - accuracy: 0.4853\n",
            "Epoch 3/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 1.0079 - accuracy: 0.4779\n",
            "Epoch 4/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6954 - accuracy: 0.5020\n",
            "Epoch 5/45\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5352\n",
            "Epoch 6/45\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.7378 - accuracy: 0.5424\n",
            "Epoch 7/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.7181 - accuracy: 0.5441\n",
            "Epoch 8/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6943 - accuracy: 0.4571\n",
            "Epoch 9/45\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6924 - accuracy: 0.5266\n",
            "Epoch 10/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5418\n",
            "Epoch 11/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6893 - accuracy: 0.5641\n",
            "Epoch 12/45\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.6920 - accuracy: 0.5406\n",
            "Epoch 13/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6909 - accuracy: 0.5458\n",
            "Epoch 14/45\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6941 - accuracy: 0.5293\n",
            "Epoch 15/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6924 - accuracy: 0.5294\n",
            "Epoch 16/45\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6914 - accuracy: 0.5489\n",
            "Epoch 17/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5664\n",
            "Epoch 18/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.5842\n",
            "Epoch 19/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6902 - accuracy: 0.5779\n",
            "Epoch 20/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6921 - accuracy: 0.5422\n",
            "Epoch 21/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6920 - accuracy: 0.5269\n",
            "Epoch 22/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6916 - accuracy: 0.5221\n",
            "Epoch 23/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6905 - accuracy: 0.5616\n",
            "Epoch 24/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6896 - accuracy: 0.5851\n",
            "Epoch 25/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6923 - accuracy: 0.5328\n",
            "Epoch 26/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6916 - accuracy: 0.5460\n",
            "Epoch 27/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6882 - accuracy: 0.5783\n",
            "Epoch 28/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6880 - accuracy: 0.5799\n",
            "Epoch 29/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6899 - accuracy: 0.5779\n",
            "Epoch 30/45\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6910 - accuracy: 0.5540\n",
            "Epoch 31/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6886 - accuracy: 0.5073\n",
            "Epoch 32/45\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6874 - accuracy: 0.4764\n",
            "Epoch 33/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6840 - accuracy: 0.5897\n",
            "Epoch 34/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6749 - accuracy: 0.6663\n",
            "Epoch 35/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6627 - accuracy: 0.6571\n",
            "Epoch 36/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6712 - accuracy: 0.5882\n",
            "Epoch 37/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.7745 - accuracy: 0.5093\n",
            "Epoch 38/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6401 - accuracy: 0.6908\n",
            "Epoch 39/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6441 - accuracy: 0.6553\n",
            "Epoch 40/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6751 - accuracy: 0.5589\n",
            "Epoch 41/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6588 - accuracy: 0.6685\n",
            "Epoch 42/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6454 - accuracy: 0.6064\n",
            "Epoch 43/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.7862 - accuracy: 0.4840\n",
            "Epoch 44/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6900 - accuracy: 0.4959\n",
            "Epoch 45/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6896 - accuracy: 0.5042\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe7f73e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 0.6660 - accuracy: 0.6557\n",
            "Epoch 1/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 100.3059 - accuracy: 0.5131\n",
            "Epoch 2/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6534 - accuracy: 0.5286\n",
            "Epoch 3/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6988 - accuracy: 0.5693\n",
            "Epoch 4/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6825 - accuracy: 0.5488\n",
            "Epoch 5/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.7072 - accuracy: 0.5907\n",
            "Epoch 6/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6894 - accuracy: 0.5316\n",
            "Epoch 7/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6865 - accuracy: 0.5448\n",
            "Epoch 8/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5599\n",
            "Epoch 9/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6886 - accuracy: 0.5444\n",
            "Epoch 10/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5638\n",
            "Epoch 11/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6843 - accuracy: 0.5717\n",
            "Epoch 12/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6832 - accuracy: 0.5903\n",
            "Epoch 13/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6822 - accuracy: 0.5601\n",
            "Epoch 14/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6611 - accuracy: 0.5923\n",
            "Epoch 15/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.7032 - accuracy: 0.5744\n",
            "Epoch 16/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 1.0481 - accuracy: 0.5721\n",
            "Epoch 17/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6896 - accuracy: 0.5688\n",
            "Epoch 18/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6849 - accuracy: 0.5760\n",
            "Epoch 19/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6896 - accuracy: 0.5550\n",
            "Epoch 20/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6849 - accuracy: 0.5624\n",
            "Epoch 21/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6881 - accuracy: 0.5661\n",
            "Epoch 22/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6880 - accuracy: 0.5539\n",
            "Epoch 23/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6848 - accuracy: 0.5828\n",
            "Epoch 24/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6868 - accuracy: 0.5655\n",
            "Epoch 25/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6835 - accuracy: 0.5616\n",
            "Epoch 26/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6869 - accuracy: 0.5600\n",
            "Epoch 27/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6830 - accuracy: 0.5930\n",
            "Epoch 28/45\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6838 - accuracy: 0.5621\n",
            "Epoch 29/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6866 - accuracy: 0.5574\n",
            "Epoch 30/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6819 - accuracy: 0.5705\n",
            "Epoch 31/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6815 - accuracy: 0.5596\n",
            "Epoch 32/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6800 - accuracy: 0.5728\n",
            "Epoch 33/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6776 - accuracy: 0.5653\n",
            "Epoch 34/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6709 - accuracy: 0.5344\n",
            "Epoch 35/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6428 - accuracy: 0.5651\n",
            "Epoch 36/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6127 - accuracy: 0.5778\n",
            "Epoch 37/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6681 - accuracy: 0.5220\n",
            "Epoch 38/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6336 - accuracy: 0.5789\n",
            "Epoch 39/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6736 - accuracy: 0.5492\n",
            "Epoch 40/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6556 - accuracy: 0.5748\n",
            "Epoch 41/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6690 - accuracy: 0.5287\n",
            "Epoch 42/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6944 - accuracy: 0.5075\n",
            "Epoch 43/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6863 - accuracy: 0.5785\n",
            "Epoch 44/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6839 - accuracy: 0.5547\n",
            "Epoch 45/45\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6692 - accuracy: 0.5931\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe7f73730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.4590\n",
            "Epoch 1/45\n",
            "6/6 [==============================] - 1s 5ms/step - loss: 32.9288 - accuracy: 0.4954\n",
            "Epoch 2/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6864 - accuracy: 0.5364\n",
            "Epoch 3/45\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6882 - accuracy: 0.5698\n",
            "Epoch 4/45\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6867 - accuracy: 0.5118\n",
            "Epoch 5/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6823 - accuracy: 0.5498\n",
            "Epoch 6/45\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.6792 - accuracy: 0.5247\n",
            "Epoch 7/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4722\n",
            "Epoch 8/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5426\n",
            "Epoch 9/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6933 - accuracy: 0.4439\n",
            "Epoch 10/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4820\n",
            "Epoch 11/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5130\n",
            "Epoch 12/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5287\n",
            "Epoch 13/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5605\n",
            "Epoch 14/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6928 - accuracy: 0.5380\n",
            "Epoch 15/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5085\n",
            "Epoch 16/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6927 - accuracy: 0.5334\n",
            "Epoch 17/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5320\n",
            "Epoch 18/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5033\n",
            "Epoch 19/45\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.6921 - accuracy: 0.5598\n",
            "Epoch 20/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6926 - accuracy: 0.5286\n",
            "Epoch 21/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6921 - accuracy: 0.5543\n",
            "Epoch 22/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6922 - accuracy: 0.5402\n",
            "Epoch 23/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6923 - accuracy: 0.5345\n",
            "Epoch 24/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6933 - accuracy: 0.4980\n",
            "Epoch 25/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6926 - accuracy: 0.5204\n",
            "Epoch 26/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6922 - accuracy: 0.5295\n",
            "Epoch 27/45\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.6927 - accuracy: 0.5151\n",
            "Epoch 28/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.5322\n",
            "Epoch 29/45\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5063\n",
            "Epoch 30/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5106\n",
            "Epoch 31/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5284\n",
            "Epoch 32/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6928 - accuracy: 0.5139\n",
            "Epoch 33/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6924 - accuracy: 0.5214\n",
            "Epoch 34/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6920 - accuracy: 0.5282\n",
            "Epoch 35/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6923 - accuracy: 0.5225\n",
            "Epoch 36/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6913 - accuracy: 0.5361\n",
            "Epoch 37/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6903 - accuracy: 0.5546\n",
            "Epoch 38/45\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.6899 - accuracy: 0.5602\n",
            "Epoch 39/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6921 - accuracy: 0.5251\n",
            "Epoch 40/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6929 - accuracy: 0.5109\n",
            "Epoch 41/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6906 - accuracy: 0.5477\n",
            "Epoch 42/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6928 - accuracy: 0.5138\n",
            "Epoch 43/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6926 - accuracy: 0.5177\n",
            "Epoch 44/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6936 - accuracy: 0.5010\n",
            "Epoch 45/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6915 - accuracy: 0.5335\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe7f73158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 0.6873 - accuracy: 0.5902\n",
            "Epoch 1/45\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 31.9960 - accuracy: 0.4422\n",
            "Epoch 2/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6979 - accuracy: 0.4876\n",
            "Epoch 3/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6891 - accuracy: 0.5188\n",
            "Epoch 4/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6877 - accuracy: 0.5501\n",
            "Epoch 5/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6902 - accuracy: 0.5337\n",
            "Epoch 6/45\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6843 - accuracy: 0.5769\n",
            "Epoch 7/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6788 - accuracy: 0.5885\n",
            "Epoch 8/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6896 - accuracy: 0.5665\n",
            "Epoch 9/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6891 - accuracy: 0.5512\n",
            "Epoch 10/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6951 - accuracy: 0.5434\n",
            "Epoch 11/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.7001 - accuracy: 0.5456\n",
            "Epoch 12/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.7454 - accuracy: 0.5620\n",
            "Epoch 13/45\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6732 - accuracy: 0.5545\n",
            "Epoch 14/45\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6637 - accuracy: 0.5950\n",
            "Epoch 15/45\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.7034 - accuracy: 0.5174\n",
            "Epoch 16/45\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.6668 - accuracy: 0.5593\n",
            "Epoch 17/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6816 - accuracy: 0.5885\n",
            "Epoch 18/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6851 - accuracy: 0.5522\n",
            "Epoch 19/45\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.8268 - accuracy: 0.5229\n",
            "Epoch 20/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6772 - accuracy: 0.5517\n",
            "Epoch 21/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6334 - accuracy: 0.6156\n",
            "Epoch 22/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.7194 - accuracy: 0.6082\n",
            "Epoch 23/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6351 - accuracy: 0.6122\n",
            "Epoch 24/45\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.7863 - accuracy: 0.5487\n",
            "Epoch 25/45\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.6793 - accuracy: 0.5596\n",
            "Epoch 26/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6751 - accuracy: 0.5829\n",
            "Epoch 27/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6748 - accuracy: 0.5600\n",
            "Epoch 28/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6892 - accuracy: 0.5377\n",
            "Epoch 29/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6605 - accuracy: 0.5804\n",
            "Epoch 30/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 2.0012 - accuracy: 0.4890\n",
            "Epoch 31/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6925 - accuracy: 0.5507\n",
            "Epoch 32/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6924 - accuracy: 0.5514\n",
            "Epoch 33/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6994 - accuracy: 0.5508\n",
            "Epoch 34/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6756 - accuracy: 0.5642\n",
            "Epoch 35/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6743 - accuracy: 0.5575\n",
            "Epoch 36/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6851 - accuracy: 0.5240\n",
            "Epoch 37/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6801 - accuracy: 0.5825\n",
            "Epoch 38/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6693 - accuracy: 0.5554\n",
            "Epoch 39/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6825 - accuracy: 0.5470\n",
            "Epoch 40/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6763 - accuracy: 0.5548\n",
            "Epoch 41/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6810 - accuracy: 0.5467\n",
            "Epoch 42/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6836 - accuracy: 0.5842\n",
            "Epoch 43/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6803 - accuracy: 0.5394\n",
            "Epoch 44/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6863 - accuracy: 0.5178\n",
            "Epoch 45/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6678 - accuracy: 0.5575\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0feb3478c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6734 - accuracy: 0.5333\n",
            "Epoch 1/45\n",
            "6/6 [==============================] - 1s 4ms/step - loss: 20.1144 - accuracy: 0.5020\n",
            "Epoch 2/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6926 - accuracy: 0.5488\n",
            "Epoch 3/45\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.6925 - accuracy: 0.5417\n",
            "Epoch 4/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6917 - accuracy: 0.5671\n",
            "Epoch 5/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6924 - accuracy: 0.5313\n",
            "Epoch 6/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6910 - accuracy: 0.5706\n",
            "Epoch 7/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6925 - accuracy: 0.5217\n",
            "Epoch 8/45\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.6920 - accuracy: 0.5356\n",
            "Epoch 9/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6906 - accuracy: 0.5682\n",
            "Epoch 10/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6905 - accuracy: 0.5700\n",
            "Epoch 11/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.5387\n",
            "Epoch 12/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6921 - accuracy: 0.5319\n",
            "Epoch 13/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6916 - accuracy: 0.5434\n",
            "Epoch 14/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6914 - accuracy: 0.5499\n",
            "Epoch 15/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6914 - accuracy: 0.5493\n",
            "Epoch 16/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6909 - accuracy: 0.5586\n",
            "Epoch 17/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6898 - accuracy: 0.5756\n",
            "Epoch 18/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6904 - accuracy: 0.5608\n",
            "Epoch 19/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6917 - accuracy: 0.5351\n",
            "Epoch 20/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6908 - accuracy: 0.5500\n",
            "Epoch 21/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6904 - accuracy: 0.5561\n",
            "Epoch 22/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6917 - accuracy: 0.5329\n",
            "Epoch 23/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6938 - accuracy: 0.4973\n",
            "Epoch 24/45\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.6908 - accuracy: 0.5442\n",
            "Epoch 25/45\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.6903 - accuracy: 0.5529\n",
            "Epoch 26/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5249\n",
            "Epoch 27/45\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.6924 - accuracy: 0.5195\n",
            "Epoch 28/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6913 - accuracy: 0.5341\n",
            "Epoch 29/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6904 - accuracy: 0.5442\n",
            "Epoch 30/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6893 - accuracy: 0.5579\n",
            "Epoch 31/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6937 - accuracy: 0.5041\n",
            "Epoch 32/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6913 - accuracy: 0.5330\n",
            "Epoch 33/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6893 - accuracy: 0.5563\n",
            "Epoch 34/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6917 - accuracy: 0.5278\n",
            "Epoch 35/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5899\n",
            "Epoch 36/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6895 - accuracy: 0.5546\n",
            "Epoch 37/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6894 - accuracy: 0.5561\n",
            "Epoch 38/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6904 - accuracy: 0.5437\n",
            "Epoch 39/45\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.6924 - accuracy: 0.5192\n",
            "Epoch 40/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6913 - accuracy: 0.5312\n",
            "Epoch 41/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6920 - accuracy: 0.5237\n",
            "Epoch 42/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6883 - accuracy: 0.5650\n",
            "Epoch 43/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6918 - accuracy: 0.5262\n",
            "Epoch 44/45\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6890 - accuracy: 0.5567\n",
            "Epoch 45/45\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.6938 - accuracy: 0.5040\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe87f3400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6883 - accuracy: 0.5667\n",
            "Epoch 1/7\n",
            "4/4 [==============================] - 1s 5ms/step - loss: 98.0659 - accuracy: 0.5303\n",
            "Epoch 2/7\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.8214 - accuracy: 0.5503\n",
            "Epoch 3/7\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6792 - accuracy: 0.5858\n",
            "Epoch 4/7\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6800 - accuracy: 0.5894\n",
            "Epoch 5/7\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7167 - accuracy: 0.5227\n",
            "Epoch 6/7\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6773 - accuracy: 0.6133\n",
            "Epoch 7/7\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6681 - accuracy: 0.5759\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0ff0f78510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.6488 - accuracy: 0.6885\n",
            "Epoch 1/7\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 120.1671 - accuracy: 0.4409\n",
            "Epoch 2/7\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 6.6756 - accuracy: 0.5655\n",
            "Epoch 3/7\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7287 - accuracy: 0.5447\n",
            "Epoch 4/7\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7030 - accuracy: 0.5780\n",
            "Epoch 5/7\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7023 - accuracy: 0.5713\n",
            "Epoch 6/7\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6941 - accuracy: 0.5855\n",
            "Epoch 7/7\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6947 - accuracy: 0.5756\n",
            "WARNING:tensorflow:7 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe205d7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.7066 - accuracy: 0.4754\n",
            "Epoch 1/7\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 99.0552 - accuracy: 0.4564\n",
            "Epoch 2/7\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6960 - accuracy: 0.4816\n",
            "Epoch 3/7\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6959 - accuracy: 0.4482\n",
            "Epoch 4/7\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6941 - accuracy: 0.4442\n",
            "Epoch 5/7\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6933 - accuracy: 0.5226\n",
            "Epoch 6/7\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6944 - accuracy: 0.5242\n",
            "Epoch 7/7\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6943 - accuracy: 0.5268\n",
            "WARNING:tensorflow:7 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fecc8a400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.6927 - accuracy: 0.5902\n",
            "Epoch 1/7\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 22.3217 - accuracy: 0.5142\n",
            "Epoch 2/7\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 4.7616 - accuracy: 0.5567\n",
            "Epoch 3/7\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6448 - accuracy: 0.6592\n",
            "Epoch 4/7\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6660 - accuracy: 0.5989\n",
            "Epoch 5/7\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6266 - accuracy: 0.6777\n",
            "Epoch 6/7\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6399 - accuracy: 0.6373\n",
            "Epoch 7/7\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6419 - accuracy: 0.6357\n",
            "WARNING:tensorflow:8 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0ffddd77b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.7943 - accuracy: 0.5333\n",
            "Epoch 1/7\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 33.7584 - accuracy: 0.4749\n",
            "Epoch 2/7\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7127 - accuracy: 0.4781\n",
            "Epoch 3/7\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6949 - accuracy: 0.5143\n",
            "Epoch 4/7\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6935 - accuracy: 0.5227\n",
            "Epoch 5/7\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6936 - accuracy: 0.5566\n",
            "Epoch 6/7\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6934 - accuracy: 0.5045\n",
            "Epoch 7/7\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6934 - accuracy: 0.5156\n",
            "WARNING:tensorflow:8 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe0fbc0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.6941 - accuracy: 0.5667\n",
            "Epoch 1/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 93.9517 - accuracy: 0.4616\n",
            "Epoch 2/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.9099 - accuracy: 0.5050\n",
            "Epoch 3/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7067 - accuracy: 0.6156\n",
            "Epoch 4/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6757 - accuracy: 0.6326\n",
            "Epoch 5/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6726 - accuracy: 0.6434\n",
            "Epoch 6/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6464 - accuracy: 0.6869\n",
            "Epoch 7/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6656 - accuracy: 0.6612\n",
            "Epoch 8/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6612 - accuracy: 0.7172\n",
            "Epoch 9/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6556 - accuracy: 0.6840\n",
            "Epoch 10/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6546 - accuracy: 0.6571\n",
            "Epoch 11/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6590 - accuracy: 0.6560\n",
            "Epoch 12/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6497 - accuracy: 0.6951\n",
            "Epoch 13/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6574 - accuracy: 0.6787\n",
            "Epoch 14/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6447 - accuracy: 0.7132\n",
            "Epoch 15/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6610 - accuracy: 0.6665\n",
            "Epoch 16/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6485 - accuracy: 0.6963\n",
            "Epoch 17/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6513 - accuracy: 0.6757\n",
            "Epoch 18/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6488 - accuracy: 0.6813\n",
            "Epoch 19/39\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6519 - accuracy: 0.6627\n",
            "Epoch 20/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6557 - accuracy: 0.6483\n",
            "Epoch 21/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6439 - accuracy: 0.6797\n",
            "Epoch 22/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6409 - accuracy: 0.7181\n",
            "Epoch 23/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6334 - accuracy: 0.7023\n",
            "Epoch 24/39\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6617 - accuracy: 0.6662\n",
            "Epoch 25/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6247 - accuracy: 0.6819\n",
            "Epoch 26/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6492 - accuracy: 0.6809\n",
            "Epoch 27/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6343 - accuracy: 0.6678\n",
            "Epoch 28/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6399 - accuracy: 0.6849\n",
            "Epoch 29/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6525 - accuracy: 0.6636\n",
            "Epoch 30/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6453 - accuracy: 0.6682\n",
            "Epoch 31/39\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6495 - accuracy: 0.6606\n",
            "Epoch 32/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6291 - accuracy: 0.7071\n",
            "Epoch 33/39\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6347 - accuracy: 0.7123\n",
            "Epoch 34/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6453 - accuracy: 0.7053\n",
            "Epoch 35/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6276 - accuracy: 0.6890\n",
            "Epoch 36/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6437 - accuracy: 0.6874\n",
            "Epoch 37/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6625 - accuracy: 0.6692\n",
            "Epoch 38/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6287 - accuracy: 0.7148\n",
            "Epoch 39/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6236 - accuracy: 0.7116\n",
            "WARNING:tensorflow:9 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe7f72598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.6704 - accuracy: 0.6557\n",
            "Epoch 1/39\n",
            "4/4 [==============================] - 1s 2ms/step - loss: 104.6211 - accuracy: 0.4690\n",
            "Epoch 2/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.7111 - accuracy: 0.5941\n",
            "Epoch 3/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.7019 - accuracy: 0.5810\n",
            "Epoch 4/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6799 - accuracy: 0.5833\n",
            "Epoch 5/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6911 - accuracy: 0.5573\n",
            "Epoch 6/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6703 - accuracy: 0.6198\n",
            "Epoch 7/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6916 - accuracy: 0.5516\n",
            "Epoch 8/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7170 - accuracy: 0.5853\n",
            "Epoch 9/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6911 - accuracy: 0.4846\n",
            "Epoch 10/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6844 - accuracy: 0.5525\n",
            "Epoch 11/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6926 - accuracy: 0.5442\n",
            "Epoch 12/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6885 - accuracy: 0.5603\n",
            "Epoch 13/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6842 - accuracy: 0.5535\n",
            "Epoch 14/39\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.6789 - accuracy: 0.5572\n",
            "Epoch 15/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6725 - accuracy: 0.5666\n",
            "Epoch 16/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6720 - accuracy: 0.5733\n",
            "Epoch 17/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6776 - accuracy: 0.5577\n",
            "Epoch 18/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6729 - accuracy: 0.5650\n",
            "Epoch 19/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6786 - accuracy: 0.5525\n",
            "Epoch 20/39\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6797 - accuracy: 0.5504\n",
            "Epoch 21/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6735 - accuracy: 0.5718\n",
            "Epoch 22/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6728 - accuracy: 0.5389\n",
            "Epoch 23/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6673 - accuracy: 0.5561\n",
            "Epoch 24/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6687 - accuracy: 0.5738\n",
            "Epoch 25/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6723 - accuracy: 0.5530\n",
            "Epoch 26/39\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6698 - accuracy: 0.5666\n",
            "Epoch 27/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6578 - accuracy: 0.5863\n",
            "Epoch 28/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6592 - accuracy: 0.5952\n",
            "Epoch 29/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6807 - accuracy: 0.5630\n",
            "Epoch 30/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6763 - accuracy: 0.5535\n",
            "Epoch 31/39\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6649 - accuracy: 0.5777\n",
            "Epoch 32/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6695 - accuracy: 0.5841\n",
            "Epoch 33/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6695 - accuracy: 0.5869\n",
            "Epoch 34/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6672 - accuracy: 0.5755\n",
            "Epoch 35/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6607 - accuracy: 0.6104\n",
            "Epoch 36/39\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6738 - accuracy: 0.5594\n",
            "Epoch 37/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6599 - accuracy: 0.5745\n",
            "Epoch 38/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6632 - accuracy: 0.5985\n",
            "Epoch 39/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6555 - accuracy: 0.6128\n",
            "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fec3d9e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.6892 - accuracy: 0.6557\n",
            "Epoch 1/39\n",
            "4/4 [==============================] - 1s 3ms/step - loss: 37.1518 - accuracy: 0.4953\n",
            "Epoch 2/39\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7103 - accuracy: 0.6191\n",
            "Epoch 3/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6867 - accuracy: 0.5487\n",
            "Epoch 4/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6820 - accuracy: 0.5671\n",
            "Epoch 5/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6770 - accuracy: 0.5847\n",
            "Epoch 6/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6788 - accuracy: 0.5395\n",
            "Epoch 7/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6685 - accuracy: 0.5624\n",
            "Epoch 8/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6694 - accuracy: 0.5424\n",
            "Epoch 9/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6579 - accuracy: 0.5828\n",
            "Epoch 10/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6660 - accuracy: 0.5566\n",
            "Epoch 11/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6680 - accuracy: 0.6359\n",
            "Epoch 12/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6330 - accuracy: 0.6738\n",
            "Epoch 13/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6654 - accuracy: 0.5798\n",
            "Epoch 14/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6707 - accuracy: 0.5874\n",
            "Epoch 15/39\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6412 - accuracy: 0.6367\n",
            "Epoch 16/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6707 - accuracy: 0.5981\n",
            "Epoch 17/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6315 - accuracy: 0.6432\n",
            "Epoch 18/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6385 - accuracy: 0.6386\n",
            "Epoch 19/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6545 - accuracy: 0.6266\n",
            "Epoch 20/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6566 - accuracy: 0.6319\n",
            "Epoch 21/39\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6482 - accuracy: 0.6312\n",
            "Epoch 22/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6482 - accuracy: 0.6476\n",
            "Epoch 23/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6573 - accuracy: 0.5769\n",
            "Epoch 24/39\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6555 - accuracy: 0.6402\n",
            "Epoch 25/39\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6225 - accuracy: 0.6799\n",
            "Epoch 26/39\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6377 - accuracy: 0.6392\n",
            "Epoch 27/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6257 - accuracy: 0.6695\n",
            "Epoch 28/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6490 - accuracy: 0.5992\n",
            "Epoch 29/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6398 - accuracy: 0.6018\n",
            "Epoch 30/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6347 - accuracy: 0.6730\n",
            "Epoch 31/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6236 - accuracy: 0.6718\n",
            "Epoch 32/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6424 - accuracy: 0.6021\n",
            "Epoch 33/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6366 - accuracy: 0.6417\n",
            "Epoch 34/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6066 - accuracy: 0.6645\n",
            "Epoch 35/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6330 - accuracy: 0.6554\n",
            "Epoch 36/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6448 - accuracy: 0.6344\n",
            "Epoch 37/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6501 - accuracy: 0.6679\n",
            "Epoch 38/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7138 - accuracy: 0.5634\n",
            "Epoch 39/39\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6262 - accuracy: 0.6290\n",
            "WARNING:tensorflow:10 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fec425d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.6552 - accuracy: 0.5246\n",
            "Epoch 1/39\n",
            "4/4 [==============================] - 1s 4ms/step - loss: 228.7378 - accuracy: 0.4720\n",
            "Epoch 2/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.7520 - accuracy: 0.6052\n",
            "Epoch 3/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6950 - accuracy: 0.5377\n",
            "Epoch 4/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6939 - accuracy: 0.5799\n",
            "Epoch 5/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5356\n",
            "Epoch 6/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6929 - accuracy: 0.5382\n",
            "Epoch 7/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6922 - accuracy: 0.5580\n",
            "Epoch 8/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6918 - accuracy: 0.5658\n",
            "Epoch 9/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6917 - accuracy: 0.5627\n",
            "Epoch 10/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6923 - accuracy: 0.5356\n",
            "Epoch 11/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6915 - accuracy: 0.5606\n",
            "Epoch 12/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6912 - accuracy: 0.5658\n",
            "Epoch 13/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6921 - accuracy: 0.5346\n",
            "Epoch 14/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6906 - accuracy: 0.5778\n",
            "Epoch 15/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6909 - accuracy: 0.5674\n",
            "Epoch 16/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6925 - accuracy: 0.5226\n",
            "Epoch 17/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6915 - accuracy: 0.5460\n",
            "Epoch 18/39\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6927 - accuracy: 0.5153\n",
            "Epoch 19/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6910 - accuracy: 0.5533\n",
            "Epoch 20/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6914 - accuracy: 0.5434\n",
            "Epoch 21/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6908 - accuracy: 0.5533\n",
            "Epoch 22/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6916 - accuracy: 0.5372\n",
            "Epoch 23/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6910 - accuracy: 0.5481\n",
            "Epoch 24/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6921 - accuracy: 0.5262\n",
            "Epoch 25/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6928 - accuracy: 0.5132\n",
            "Epoch 26/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6906 - accuracy: 0.5507\n",
            "Epoch 27/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6912 - accuracy: 0.5403\n",
            "Epoch 28/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6922 - accuracy: 0.5226\n",
            "Epoch 29/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6908 - accuracy: 0.5445\n",
            "Epoch 30/39\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6899 - accuracy: 0.5590\n",
            "Epoch 31/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6920 - accuracy: 0.5252\n",
            "Epoch 32/39\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.5075\n",
            "Epoch 33/39\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6904 - accuracy: 0.5476\n",
            "Epoch 34/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6891 - accuracy: 0.5648\n",
            "Epoch 35/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6893 - accuracy: 0.5622\n",
            "Epoch 36/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6902 - accuracy: 0.5491\n",
            "Epoch 37/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6892 - accuracy: 0.5611\n",
            "Epoch 38/39\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.6892 - accuracy: 0.5611\n",
            "Epoch 39/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6888 - accuracy: 0.5658\n",
            "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe5cb3268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.8575 - accuracy: 0.5167\n",
            "Epoch 1/39\n",
            "4/4 [==============================] - 1s 4ms/step - loss: 91.3057 - accuracy: 0.4403\n",
            "Epoch 2/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.7099 - accuracy: 0.4249\n",
            "Epoch 3/39\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6695 - accuracy: 0.5386\n",
            "Epoch 4/39\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6519 - accuracy: 0.5537\n",
            "Epoch 5/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6618 - accuracy: 0.5568\n",
            "Epoch 6/39\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6632 - accuracy: 0.5287\n",
            "Epoch 7/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6633 - accuracy: 0.5412\n",
            "Epoch 8/39\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6443 - accuracy: 0.6204\n",
            "Epoch 9/39\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6651 - accuracy: 0.6319\n",
            "Epoch 10/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6536 - accuracy: 0.6222\n",
            "Epoch 11/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6571 - accuracy: 0.5978\n",
            "Epoch 12/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6604 - accuracy: 0.6334\n",
            "Epoch 13/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6365 - accuracy: 0.6826\n",
            "Epoch 14/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6550 - accuracy: 0.6426\n",
            "Epoch 15/39\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6499 - accuracy: 0.6335\n",
            "Epoch 16/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6548 - accuracy: 0.6502\n",
            "Epoch 17/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6761 - accuracy: 0.6363\n",
            "Epoch 18/39\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6689 - accuracy: 0.6477\n",
            "Epoch 19/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6540 - accuracy: 0.6520\n",
            "Epoch 20/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6441 - accuracy: 0.6773\n",
            "Epoch 21/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6547 - accuracy: 0.6419\n",
            "Epoch 22/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6380 - accuracy: 0.6685\n",
            "Epoch 23/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6439 - accuracy: 0.6279\n",
            "Epoch 24/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6482 - accuracy: 0.6579\n",
            "Epoch 25/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6563 - accuracy: 0.6228\n",
            "Epoch 26/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6368 - accuracy: 0.6980\n",
            "Epoch 27/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6563 - accuracy: 0.6336\n",
            "Epoch 28/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6661 - accuracy: 0.6178\n",
            "Epoch 29/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6371 - accuracy: 0.6692\n",
            "Epoch 30/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6414 - accuracy: 0.6564\n",
            "Epoch 31/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6352 - accuracy: 0.6659\n",
            "Epoch 32/39\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6416 - accuracy: 0.6549\n",
            "Epoch 33/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6418 - accuracy: 0.6648\n",
            "Epoch 34/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6603 - accuracy: 0.6451\n",
            "Epoch 35/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6286 - accuracy: 0.6734\n",
            "Epoch 36/39\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6536 - accuracy: 0.6474\n",
            "Epoch 37/39\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6214 - accuracy: 0.6904\n",
            "Epoch 38/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6500 - accuracy: 0.6382\n",
            "Epoch 39/39\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6325 - accuracy: 0.6424\n",
            "WARNING:tensorflow:11 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe90a1510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.6128 - accuracy: 0.6000\n",
            "Epoch 1/45\n",
            "4/4 [==============================] - 1s 4ms/step - loss: 37.4887 - accuracy: 0.5084\n",
            "Epoch 2/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6751 - accuracy: 0.5639\n",
            "Epoch 3/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6561 - accuracy: 0.6361\n",
            "Epoch 4/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6468 - accuracy: 0.6644\n",
            "Epoch 5/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6598 - accuracy: 0.6191\n",
            "Epoch 6/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6267 - accuracy: 0.6479\n",
            "Epoch 7/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6135 - accuracy: 0.6442\n",
            "Epoch 8/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7266 - accuracy: 0.6136\n",
            "Epoch 9/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6552 - accuracy: 0.6081\n",
            "Epoch 10/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6261 - accuracy: 0.6426\n",
            "Epoch 11/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6165 - accuracy: 0.6701\n",
            "Epoch 12/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5933 - accuracy: 0.6759\n",
            "Epoch 13/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6155 - accuracy: 0.5990\n",
            "Epoch 14/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5961 - accuracy: 0.6812\n",
            "Epoch 15/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5978 - accuracy: 0.6582\n",
            "Epoch 16/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5928 - accuracy: 0.6728\n",
            "Epoch 17/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6102 - accuracy: 0.6588\n",
            "Epoch 18/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5703 - accuracy: 0.7007\n",
            "Epoch 19/45\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5686 - accuracy: 0.7022\n",
            "Epoch 20/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6443 - accuracy: 0.6523\n",
            "Epoch 21/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6849 - accuracy: 0.5542\n",
            "Epoch 22/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5866 - accuracy: 0.6843\n",
            "Epoch 23/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6316 - accuracy: 0.5978\n",
            "Epoch 24/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6491 - accuracy: 0.6575\n",
            "Epoch 25/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6056 - accuracy: 0.6523\n",
            "Epoch 26/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5981 - accuracy: 0.6955\n",
            "Epoch 27/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5927 - accuracy: 0.6684\n",
            "Epoch 28/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6126 - accuracy: 0.6488\n",
            "Epoch 29/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6110 - accuracy: 0.6391\n",
            "Epoch 30/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6337 - accuracy: 0.6258\n",
            "Epoch 31/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6117 - accuracy: 0.6876\n",
            "Epoch 32/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5903 - accuracy: 0.6817\n",
            "Epoch 33/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6043 - accuracy: 0.6302\n",
            "Epoch 34/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5930 - accuracy: 0.6804\n",
            "Epoch 35/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6020 - accuracy: 0.6618\n",
            "Epoch 36/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5615 - accuracy: 0.6850\n",
            "Epoch 37/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5819 - accuracy: 0.6805\n",
            "Epoch 38/45\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5982 - accuracy: 0.6575\n",
            "Epoch 39/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5733 - accuracy: 0.7011\n",
            "Epoch 40/45\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5927 - accuracy: 0.6350\n",
            "Epoch 41/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7008 - accuracy: 0.5922\n",
            "Epoch 42/45\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5635 - accuracy: 0.6416\n",
            "Epoch 43/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5891 - accuracy: 0.6391\n",
            "Epoch 44/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6507 - accuracy: 0.6515\n",
            "Epoch 45/45\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5903 - accuracy: 0.6380\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe31961e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.5713 - accuracy: 0.6721\n",
            "Epoch 1/45\n",
            "4/4 [==============================] - 1s 2ms/step - loss: 82.3961 - accuracy: 0.5068\n",
            "Epoch 2/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6886 - accuracy: 0.5315\n",
            "Epoch 3/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6720 - accuracy: 0.5309\n",
            "Epoch 4/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6730 - accuracy: 0.5312\n",
            "Epoch 5/45\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6831 - accuracy: 0.5261\n",
            "Epoch 6/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7085 - accuracy: 0.5084\n",
            "Epoch 7/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6805 - accuracy: 0.5712\n",
            "Epoch 8/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6739 - accuracy: 0.5817\n",
            "Epoch 9/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6858 - accuracy: 0.5728\n",
            "Epoch 10/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6868 - accuracy: 0.5452\n",
            "Epoch 11/45\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6723 - accuracy: 0.5629\n",
            "Epoch 12/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6766 - accuracy: 0.5619\n",
            "Epoch 13/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6695 - accuracy: 0.5676\n",
            "Epoch 14/45\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.6461 - accuracy: 0.5400\n",
            "Epoch 15/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6625 - accuracy: 0.5676\n",
            "Epoch 16/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6580 - accuracy: 0.5556\n",
            "Epoch 17/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6617 - accuracy: 0.5639\n",
            "Epoch 18/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6528 - accuracy: 0.5624\n",
            "Epoch 19/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6621 - accuracy: 0.5900\n",
            "Epoch 20/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6511 - accuracy: 0.5408\n",
            "Epoch 21/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6259 - accuracy: 0.5778\n",
            "Epoch 22/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6417 - accuracy: 0.5879\n",
            "Epoch 23/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6346 - accuracy: 0.6353\n",
            "Epoch 24/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6270 - accuracy: 0.6344\n",
            "Epoch 25/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6420 - accuracy: 0.6107\n",
            "Epoch 26/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6445 - accuracy: 0.6022\n",
            "Epoch 27/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6244 - accuracy: 0.6446\n",
            "Epoch 28/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6383 - accuracy: 0.6150\n",
            "Epoch 29/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6514 - accuracy: 0.6235\n",
            "Epoch 30/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6460 - accuracy: 0.5551\n",
            "Epoch 31/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6241 - accuracy: 0.6319\n",
            "Epoch 32/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6409 - accuracy: 0.6110\n",
            "Epoch 33/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6389 - accuracy: 0.6446\n",
            "Epoch 34/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6452 - accuracy: 0.6131\n",
            "Epoch 35/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6105 - accuracy: 0.6228\n",
            "Epoch 36/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6458 - accuracy: 0.6476\n",
            "Epoch 37/45\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6227 - accuracy: 0.6308\n",
            "Epoch 38/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6458 - accuracy: 0.6403\n",
            "Epoch 39/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6293 - accuracy: 0.6204\n",
            "Epoch 40/45\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6354 - accuracy: 0.6391\n",
            "Epoch 41/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6235 - accuracy: 0.6064\n",
            "Epoch 42/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6272 - accuracy: 0.6545\n",
            "Epoch 43/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6398 - accuracy: 0.6297\n",
            "Epoch 44/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6202 - accuracy: 0.6207\n",
            "Epoch 45/45\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6396 - accuracy: 0.6371\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fe90a17b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.6734 - accuracy: 0.6393\n",
            "Epoch 1/45\n",
            "4/4 [==============================] - 1s 3ms/step - loss: 42.7810 - accuracy: 0.5070\n",
            "Epoch 2/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.8446 - accuracy: 0.5227\n",
            "Epoch 3/45\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6979 - accuracy: 0.4530\n",
            "Epoch 4/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6927 - accuracy: 0.4726\n",
            "Epoch 5/45\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6883 - accuracy: 0.5067\n",
            "Epoch 6/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6809 - accuracy: 0.5706\n",
            "Epoch 7/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6579 - accuracy: 0.6420\n",
            "Epoch 8/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6614 - accuracy: 0.6306\n",
            "Epoch 9/45\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6697 - accuracy: 0.5358\n",
            "Epoch 10/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6540 - accuracy: 0.5757\n",
            "Epoch 11/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6525 - accuracy: 0.6314\n",
            "Epoch 12/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6369 - accuracy: 0.6291\n",
            "Epoch 13/45\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6398 - accuracy: 0.6098\n",
            "Epoch 14/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6288 - accuracy: 0.6390\n",
            "Epoch 15/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6191 - accuracy: 0.6589\n",
            "Epoch 16/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6428 - accuracy: 0.6250\n",
            "Epoch 17/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6464 - accuracy: 0.6294\n",
            "Epoch 18/45\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6572 - accuracy: 0.5887\n",
            "Epoch 19/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.7298 - accuracy: 0.5469\n",
            "Epoch 20/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6112 - accuracy: 0.6466\n",
            "Epoch 21/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6353 - accuracy: 0.6256\n",
            "Epoch 22/45\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6173 - accuracy: 0.6872\n",
            "Epoch 23/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6156 - accuracy: 0.6596\n",
            "Epoch 24/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7126 - accuracy: 0.5125\n",
            "Epoch 25/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5967 - accuracy: 0.6790\n",
            "Epoch 26/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5914 - accuracy: 0.7025\n",
            "Epoch 27/45\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.5922 - accuracy: 0.7031\n",
            "Epoch 28/45\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6074 - accuracy: 0.6609\n",
            "Epoch 29/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6826 - accuracy: 0.5591\n",
            "Epoch 30/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5888 - accuracy: 0.6946\n",
            "Epoch 31/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6106 - accuracy: 0.6465\n",
            "Epoch 32/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5968 - accuracy: 0.6896\n",
            "Epoch 33/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6513 - accuracy: 0.6159\n",
            "Epoch 34/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6306 - accuracy: 0.6408\n",
            "Epoch 35/45\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5922 - accuracy: 0.6718\n",
            "Epoch 36/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6747 - accuracy: 0.5955\n",
            "Epoch 37/45\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6157 - accuracy: 0.6540\n",
            "Epoch 38/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6498 - accuracy: 0.6084\n",
            "Epoch 39/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6450 - accuracy: 0.6372\n",
            "Epoch 40/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6244 - accuracy: 0.6264\n",
            "Epoch 41/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5948 - accuracy: 0.6602\n",
            "Epoch 42/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6007 - accuracy: 0.6919\n",
            "Epoch 43/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6086 - accuracy: 0.6751\n",
            "Epoch 44/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5851 - accuracy: 0.6716\n",
            "Epoch 45/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5908 - accuracy: 0.6622\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0fee66b510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.5982 - accuracy: 0.6393\n",
            "Epoch 1/45\n",
            "4/4 [==============================] - 1s 6ms/step - loss: 78.7679 - accuracy: 0.5109\n",
            "Epoch 2/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6925 - accuracy: 0.4493\n",
            "Epoch 3/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.4686\n",
            "Epoch 4/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6925 - accuracy: 0.5654\n",
            "Epoch 5/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6928 - accuracy: 0.5508\n",
            "Epoch 6/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6927 - accuracy: 0.5367\n",
            "Epoch 7/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6914 - accuracy: 0.5643\n",
            "Epoch 8/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.5508\n",
            "Epoch 9/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6923 - accuracy: 0.5534\n",
            "Epoch 10/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6907 - accuracy: 0.5732\n",
            "Epoch 11/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6916 - accuracy: 0.5706\n",
            "Epoch 12/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6903 - accuracy: 0.5768\n",
            "Epoch 13/45\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6911 - accuracy: 0.5596\n",
            "Epoch 14/45\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6912 - accuracy: 0.5695\n",
            "Epoch 15/45\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6917 - accuracy: 0.5185\n",
            "Epoch 16/45\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6917 - accuracy: 0.5497\n",
            "Epoch 17/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6918 - accuracy: 0.5414\n",
            "Epoch 18/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6910 - accuracy: 0.5680\n",
            "Epoch 19/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6888 - accuracy: 0.5826\n",
            "Epoch 20/45\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6904 - accuracy: 0.5661\n",
            "Epoch 21/45\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6806 - accuracy: 0.6300\n",
            "Epoch 22/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6689 - accuracy: 0.6620\n",
            "Epoch 23/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6558 - accuracy: 0.6634\n",
            "Epoch 24/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6727 - accuracy: 0.6329\n",
            "Epoch 25/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6654 - accuracy: 0.6493\n",
            "Epoch 26/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6431 - accuracy: 0.6402\n",
            "Epoch 27/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6705 - accuracy: 0.6467\n",
            "Epoch 28/45\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7050 - accuracy: 0.6577\n",
            "Epoch 29/45\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.6897 - accuracy: 0.5658\n",
            "Epoch 30/45\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6884 - accuracy: 0.5856\n",
            "Epoch 31/45\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6899 - accuracy: 0.5611\n",
            "Epoch 32/45\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6923 - accuracy: 0.5221\n",
            "Epoch 33/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6928 - accuracy: 0.5137\n",
            "Epoch 34/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6905 - accuracy: 0.5486\n",
            "Epoch 35/45\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6906 - accuracy: 0.5471\n",
            "Epoch 36/45\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6910 - accuracy: 0.5403\n",
            "Epoch 37/45\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6907 - accuracy: 0.5439\n",
            "Epoch 38/45\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6918 - accuracy: 0.5278\n",
            "Epoch 39/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6912 - accuracy: 0.5366\n",
            "Epoch 40/45\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6900 - accuracy: 0.5523\n",
            "Epoch 41/45\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6902 - accuracy: 0.5497\n",
            "Epoch 42/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6883 - accuracy: 0.5741\n",
            "Epoch 43/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6897 - accuracy: 0.5554\n",
            "Epoch 44/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6908 - accuracy: 0.5403\n",
            "Epoch 45/45\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6882 - accuracy: 0.5731\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0febb91620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.7391 - accuracy: 0.5167\n",
            "Epoch 1/45\n",
            "4/4 [==============================] - 1s 5ms/step - loss: 93.5498 - accuracy: 0.4756\n",
            "Epoch 2/45\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.2702 - accuracy: 0.4421\n",
            "Epoch 3/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6875 - accuracy: 0.4501\n",
            "Epoch 4/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6724 - accuracy: 0.5151\n",
            "Epoch 5/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6701 - accuracy: 0.6152\n",
            "Epoch 6/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6554 - accuracy: 0.6399\n",
            "Epoch 7/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6577 - accuracy: 0.6220\n",
            "Epoch 8/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6700 - accuracy: 0.6530\n",
            "Epoch 9/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6644 - accuracy: 0.6191\n",
            "Epoch 10/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6696 - accuracy: 0.5964\n",
            "Epoch 11/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6665 - accuracy: 0.6237\n",
            "Epoch 12/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6521 - accuracy: 0.6707\n",
            "Epoch 13/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6713 - accuracy: 0.6543\n",
            "Epoch 14/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6528 - accuracy: 0.6800\n",
            "Epoch 15/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6391 - accuracy: 0.6446\n",
            "Epoch 16/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6515 - accuracy: 0.6924\n",
            "Epoch 17/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6889 - accuracy: 0.6449\n",
            "Epoch 18/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6699 - accuracy: 0.6269\n",
            "Epoch 19/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6566 - accuracy: 0.6925\n",
            "Epoch 20/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6635 - accuracy: 0.6224\n",
            "Epoch 21/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6470 - accuracy: 0.7080\n",
            "Epoch 22/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6309 - accuracy: 0.6352\n",
            "Epoch 23/45\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.8515 - accuracy: 0.5484\n",
            "Epoch 24/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6629 - accuracy: 0.6775\n",
            "Epoch 25/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6583 - accuracy: 0.6566\n",
            "Epoch 26/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6573 - accuracy: 0.6594\n",
            "Epoch 27/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6589 - accuracy: 0.6474\n",
            "Epoch 28/45\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6496 - accuracy: 0.6467\n",
            "Epoch 29/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6594 - accuracy: 0.6798\n",
            "Epoch 30/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6575 - accuracy: 0.6520\n",
            "Epoch 31/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6511 - accuracy: 0.7126\n",
            "Epoch 32/45\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.6531 - accuracy: 0.6770\n",
            "Epoch 33/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6489 - accuracy: 0.6623\n",
            "Epoch 34/45\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6475 - accuracy: 0.6794\n",
            "Epoch 35/45\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6590 - accuracy: 0.6536\n",
            "Epoch 36/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6607 - accuracy: 0.6297\n",
            "Epoch 37/45\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6519 - accuracy: 0.6735\n",
            "Epoch 38/45\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6530 - accuracy: 0.6788\n",
            "Epoch 39/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6677 - accuracy: 0.6594\n",
            "Epoch 40/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6770 - accuracy: 0.5856\n",
            "Epoch 41/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6515 - accuracy: 0.6694\n",
            "Epoch 42/45\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.6484 - accuracy: 0.6835\n",
            "Epoch 43/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6423 - accuracy: 0.6739\n",
            "Epoch 44/45\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.6368 - accuracy: 0.6891\n",
            "Epoch 45/45\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6434 - accuracy: 0.7212\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f0febb917b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.6436 - accuracy: 0.6833\n",
            "Epoch 1/45\n",
            "5/5 [==============================] - 1s 4ms/step - loss: 44.0048 - accuracy: 0.5049\n",
            "Epoch 2/45\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6977 - accuracy: 0.6077\n",
            "Epoch 3/45\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.7210 - accuracy: 0.5302\n",
            "Epoch 4/45\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6546 - accuracy: 0.6452\n",
            "Epoch 5/45\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6613 - accuracy: 0.6061\n",
            "Epoch 6/45\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6712 - accuracy: 0.6007\n",
            "Epoch 7/45\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6465 - accuracy: 0.6325\n",
            "Epoch 8/45\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6548 - accuracy: 0.6322\n",
            "Epoch 9/45\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6374 - accuracy: 0.6280\n",
            "Epoch 10/45\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6592 - accuracy: 0.5945\n",
            "Epoch 11/45\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6375 - accuracy: 0.6266\n",
            "Epoch 12/45\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6286 - accuracy: 0.6443\n",
            "Epoch 13/45\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6689 - accuracy: 0.5896\n",
            "Epoch 14/45\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6404 - accuracy: 0.6269\n",
            "Epoch 15/45\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6435 - accuracy: 0.6190\n",
            "Epoch 16/45\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.6403 - accuracy: 0.6211\n",
            "Epoch 17/45\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6528 - accuracy: 0.6085\n",
            "Epoch 18/45\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6335 - accuracy: 0.6421\n",
            "Epoch 19/45\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6585 - accuracy: 0.6113\n",
            "Epoch 20/45\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6699 - accuracy: 0.6161\n",
            "Epoch 21/45\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6286 - accuracy: 0.6361\n",
            "Epoch 22/45\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.6389 - accuracy: 0.6167\n",
            "Epoch 23/45\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6761 - accuracy: 0.6173\n",
            "Epoch 24/45\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.6598 - accuracy: 0.5800\n",
            "Epoch 25/45\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6296 - accuracy: 0.6588\n",
            "Epoch 26/45\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6260 - accuracy: 0.6247\n",
            "Epoch 27/45\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6194 - accuracy: 0.6642\n",
            "Epoch 28/45\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.6212 - accuracy: 0.6383\n",
            "Epoch 29/45\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.5989 - accuracy: 0.6774\n",
            "Epoch 30/45\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6527 - accuracy: 0.6327\n",
            "Epoch 31/45\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6117 - accuracy: 0.6561\n",
            "Epoch 32/45\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.5853 - accuracy: 0.7046\n",
            "Epoch 33/45\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6058 - accuracy: 0.6711\n",
            "Epoch 34/45\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6515 - accuracy: 0.6099\n",
            "Epoch 35/45\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6149 - accuracy: 0.6429\n",
            "Epoch 36/45\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6437 - accuracy: 0.6327\n",
            "Epoch 37/45\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6084 - accuracy: 0.6703\n",
            "Epoch 38/45\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6043 - accuracy: 0.6712\n",
            "Epoch 39/45\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.6212 - accuracy: 0.6397\n",
            "Epoch 40/45\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6179 - accuracy: 0.6551\n",
            "Epoch 41/45\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6236 - accuracy: 0.6255\n",
            "Epoch 42/45\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6001 - accuracy: 0.6538\n",
            "Epoch 43/45\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.5966 - accuracy: 0.6621\n",
            "Epoch 44/45\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.5922 - accuracy: 0.6545\n",
            "Epoch 45/45\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.5965 - accuracy: 0.6634\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN893dnh3ZxH",
        "outputId": "db59fdec-1410-48d0-b758-8913c5524c99"
      },
      "source": [
        "# your grid_result object should be able to run in this code \r\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\r\n",
        "means = grid_result.cv_results_['mean_test_score']\r\n",
        "stds = grid_result.cv_results_['std_test_score']\r\n",
        "params = grid_result.cv_results_['params']\r\n",
        "for mean, stdev, param in zip(means, stds, params):\r\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.6301639318466187 using {'batch_size': 64, 'epochs': 45}\n",
            "Means: 0.5977595567703247, Stdev: 0.08045497950983599 with: {'batch_size': 32, 'epochs': 7}\n",
            "Means: 0.5645355224609375, Stdev: 0.05508197498699996 with: {'batch_size': 32, 'epochs': 39}\n",
            "Means: 0.6205464363098144, Stdev: 0.042306316281750085 with: {'batch_size': 32, 'epochs': 45}\n",
            "Means: 0.5508196711540222, Stdev: 0.06465567933546881 with: {'batch_size': 48, 'epochs': 7}\n",
            "Means: 0.5446447968482971, Stdev: 0.05074605804559433 with: {'batch_size': 48, 'epochs': 39}\n",
            "Means: 0.5609836101531982, Stdev: 0.06484439531573678 with: {'batch_size': 48, 'epochs': 45}\n",
            "Means: 0.5708196818828583, Stdev: 0.0703621486263459 with: {'batch_size': 64, 'epochs': 7}\n",
            "Means: 0.590546441078186, Stdev: 0.06065909327171317 with: {'batch_size': 64, 'epochs': 39}\n",
            "Means: 0.6301639318466187, Stdev: 0.05939431095283233 with: {'batch_size': 64, 'epochs': 45}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7_QMFzK3ctJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
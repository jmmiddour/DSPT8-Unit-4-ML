{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unit_4_Sprint_1_Natural_Language_Processing_Study_Guide.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP7Vt8lDCIgxOEDbGD4VUNE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "u4-s1-nlp",
      "display_name": "U4-S1-NLP"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bundickm/Study-Guides/blob/master/Unit_4_Sprint_1_Natural_Language_Processing_Study_Guide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enodNfbMIxzN",
        "colab_type": "text"
      },
      "source": [
        "This study guide should reinforce and provide practice for all of the concepts you have seen in the past week. There are a mix of written questions and coding exercises, both are equally important to prepare you for the sprint challenge as well as to be able to speak on these topics comfortably in interviews and on the job.\n",
        "\n",
        "If you get stuck or are unsure of something remember the 20 minute rule. If that doesn't help, then research a solution with google and stackoverflow. Only once you have exausted these methods should you turn to your Team Lead - they won't be there on your SC or during an interview. That being said, don't hesitate to ask for help if you truly are stuck.\n",
        "\n",
        "Have fun studying!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjVNoILlDD83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMQBp_ddC9CX",
        "colab_type": "code",
        "outputId": "688a6986-7a3c-4a90-9a7d-c93f36cf7bb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/bundickm/Study-Guides/master/data/cannabis.csv')\n",
        "print('Shape:', df.shape, '\\n')\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (2351, 6) \n\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Strain    Type  Rating                                     Effects  \\\n",
              "0          100-Og  hybrid     4.0  Creative,Energetic,Tingly,Euphoric,Relaxed   \n",
              "1  98-White-Widow  hybrid     4.7    Relaxed,Aroused,Creative,Happy,Energetic   \n",
              "2            1024  sativa     4.4   Uplifted,Happy,Relaxed,Energetic,Creative   \n",
              "3        13-Dawgs  hybrid     4.2     Tingly,Creative,Hungry,Relaxed,Uplifted   \n",
              "4        24K-Gold  hybrid     4.6   Happy,Relaxed,Euphoric,Uplifted,Talkative   \n",
              "\n",
              "                      Flavor  \\\n",
              "0        Earthy,Sweet,Citrus   \n",
              "1      Flowery,Violet,Diesel   \n",
              "2    Spicy/Herbal,Sage,Woody   \n",
              "3  Apricot,Citrus,Grapefruit   \n",
              "4       Citrus,Earthy,Orange   \n",
              "\n",
              "                                         Description  \n",
              "0  $100 OG is a 50/50 hybrid strain that packs a ...  \n",
              "1  The ‘98 Aloha White Widow is an especially pot...  \n",
              "2  1024 is a sativa-dominant hybrid bred in Spain...  \n",
              "3  13 Dawgs is a hybrid of G13 and Chemdawg genet...  \n",
              "4  Also known as Kosher Tangie, 24k Gold is a 60%...  "
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Strain</th>\n      <th>Type</th>\n      <th>Rating</th>\n      <th>Effects</th>\n      <th>Flavor</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100-Og</td>\n      <td>hybrid</td>\n      <td>4.0</td>\n      <td>Creative,Energetic,Tingly,Euphoric,Relaxed</td>\n      <td>Earthy,Sweet,Citrus</td>\n      <td>$100 OG is a 50/50 hybrid strain that packs a ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>98-White-Widow</td>\n      <td>hybrid</td>\n      <td>4.7</td>\n      <td>Relaxed,Aroused,Creative,Happy,Energetic</td>\n      <td>Flowery,Violet,Diesel</td>\n      <td>The ‘98 Aloha White Widow is an especially pot...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1024</td>\n      <td>sativa</td>\n      <td>4.4</td>\n      <td>Uplifted,Happy,Relaxed,Energetic,Creative</td>\n      <td>Spicy/Herbal,Sage,Woody</td>\n      <td>1024 is a sativa-dominant hybrid bred in Spain...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13-Dawgs</td>\n      <td>hybrid</td>\n      <td>4.2</td>\n      <td>Tingly,Creative,Hungry,Relaxed,Uplifted</td>\n      <td>Apricot,Citrus,Grapefruit</td>\n      <td>13 Dawgs is a hybrid of G13 and Chemdawg genet...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>24K-Gold</td>\n      <td>hybrid</td>\n      <td>4.6</td>\n      <td>Happy,Relaxed,Euphoric,Uplifted,Talkative</td>\n      <td>Citrus,Earthy,Orange</td>\n      <td>Also known as Kosher Tangie, 24k Gold is a 60%...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "type(df['Description'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbpf-sf-DjRi",
        "colab_type": "text"
      },
      "source": [
        "# Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8pe3aGUJUkI",
        "colab_type": "text"
      },
      "source": [
        "## Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3GHPBZ4I3h5",
        "colab_type": "text"
      },
      "source": [
        "Define the following terms in your own words, do not simply copy and paste a definition found elsewhere but reword it to be understandable and memorable to you. *Double click the markdown to add your definitions.*\n",
        "<br/><br/>\n",
        "\n",
        "- **Natural Language Processing**: The processing done by a computer using the natural spoken and/or written human language.  \n",
        "\n",
        "- **Token**: Broke down text into individual words and turns it into a list of all words in a specific body of text.\n",
        "\n",
        "- **Corpus**: The collection of all the data, such as the dataset or dataframe you are working with.\n",
        "\n",
        "- **Stopwords**: The most common words in a language. Words in this list will be removed from the text. Can also create your own stop words list or use the default list provided with each library.\n",
        "\n",
        "- **Statistical Trimming**: Trimming the words based on the percentage they appear in the dataset.\n",
        "\n",
        "- **Stemming**: Removes the prefixes and suffixes from words, such as \"-ing\", \"-ed\", \"-s\", \"-es\", \"un-\", etc. This can actually cause problems because there are some words it will not transform properly.\n",
        "\n",
        "- **Lemmatization**: Will bring words down to their root forms by looking at verbs, adjectives, nouns, etc to get the root form of the word(s). Looks at words that are related to a root form and converts it to the root word, such as \"am\" and \"are\" are related to \"be\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbXlWbA3JWuU",
        "colab_type": "text"
      },
      "source": [
        "## Questions of Understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjm-Ab4sJaOs",
        "colab_type": "text"
      },
      "source": [
        "1. What are at least 4 common cleaning tasks you need to do when creating tokens?\n",
        "    1. Normalize the case of words (i.e. `.lower()` or `.upper()`)\n",
        "    2. Remove punctuation\n",
        "    3. Remove special characters\n",
        "    4. Remove whitespace\n",
        "\n",
        "2. Why is it important to apply custom stopwords to our dataset in addition to the ones that come in a library like spaCy?\n",
        "    \n",
        "    ```\n",
        "    To reduce the dimensionalty of the data.  \n",
        "    This is a great tool for words that appear frequently and do not add any value to the data if kept.\n",
        "    ```\n",
        "\n",
        "3. Explain the tradeoffs between statistical trimming, stemming, and lemmatizing.\n",
        "    \n",
        "    ```\n",
        "    In statistical trimming you are removing words based on the frequency in which they appear in the entire dataset based on how many documents they are in.  \n",
        "    While stemming and lemmatizing reduces the number of unique values by combining words that are similar.  \n",
        "    Depending on which you use, stemmming will remove the prefixes and suffixes to get the root word, and lemmatizing will use parts of grammar to get the root word, such as noun, verb, adjective, etc.  \n",
        "    Which one you use will be case specific to the problem at hand and the data you have.\n",
        "    ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn3Z587YMwnE",
        "colab_type": "text"
      },
      "source": [
        "## Practice Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii_LYpeoDrTp",
        "colab_type": "text"
      },
      "source": [
        "Write a function to tokenize the `Description` column. Make sure to include the following:\n",
        "- Return the tokens in an iterable structure\n",
        "- Normalize the case\n",
        "- Remove non-alphanumeric characters such as punctuation, whitespace, unicode, etc.\n",
        "- Apply stopwords and make sure to add stopwords specific to this dataset\n",
        "- Lemmatize the tokens before returning them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NLP libraries\n",
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_lg')\n",
        "tokenizer = Tokenizer(nlp.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgt9aT-TDFcq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lemmatize(text):\n",
        "    '''\n",
        "    This function will return a lemmatized object for the text imputted\n",
        "    '''\n",
        "    doc = nlp(text)\n",
        "\n",
        "    lemmas = []\n",
        "    for token in doc:\n",
        "        if (token.is_stop == False) & (token.is_punct == False):\n",
        "            lemmas.append(token.lemma_.lower())\n",
        "\n",
        "    return lemmas"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pX8ZRE7fFqcw",
        "colab_type": "text"
      },
      "source": [
        "Apply your function to `Description` and save the resulting tokens in a new column, `Tokens`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9pQJYfZFxd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Tokens'] = df['Description'].apply(lemmatize)\n",
        "df['Tokens'].head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'float' has no len()",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-13-0b57103cefdb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Tokens'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Tokens'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4198\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4199\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4200\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4202\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;32m<ipython-input-12-57e9da40b342>\u001b[0m in \u001b[0;36mlemmatize\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mwill\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlemmatized\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtext\u001b[0m \u001b[0mimputted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     '''\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mlemmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;31m#call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \"\"\"\n\u001b[1;32m--> 423\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m             raise ValueError(\n\u001b[0;32m    425\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: object of type 'float' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4x9xuBVF4Nh",
        "colab_type": "text"
      },
      "source": [
        "Use the function below to create a `word_count` dataframe based off the `df['Tokens']` column you created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zu2dfbcGz2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count(docs):\n",
        "        word_counts = Counter()\n",
        "        appears_in = Counter()\n",
        "        total_docs = len(docs)\n",
        "\n",
        "        for doc in docs:\n",
        "            word_counts.update(doc)\n",
        "            appears_in.update(set(doc))\n",
        "\n",
        "        temp = zip(word_counts.keys(), word_counts.values())\n",
        "        wc = pd.DataFrame(temp, columns = ['word', 'count'])\n",
        "\n",
        "        wc['rank'] = wc['count'].rank(method='first', ascending=False)\n",
        "        total = wc['count'].sum()\n",
        "\n",
        "        wc['pct_total'] = wc['count'].apply(lambda x: x / total)\n",
        "        \n",
        "        wc = wc.sort_values(by='rank')\n",
        "        wc['cul_pct_total'] = wc['pct_total'].cumsum()\n",
        "\n",
        "        t2 = zip(appears_in.keys(), appears_in.values())\n",
        "        ac = pd.DataFrame(t2, columns=['word', 'appears_in'])\n",
        "        wc = ac.merge(wc, on='word')\n",
        "\n",
        "        wc['appears_in_pct'] = wc['appears_in'].apply(lambda x: x / total_docs)\n",
        "        \n",
        "        return wc.sort_values(by='rank')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94lL-w_uGzzm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPPxVzGIHUF8",
        "colab_type": "text"
      },
      "source": [
        "Run the line of code below, and then explain how to interpret the graph.\n",
        "\n",
        "```\n",
        "Your Answer Here\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWqbuy68Ib0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.lineplot(x='rank', y='cul_pct_total', data=word_count);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-_e03NrMjIO",
        "colab_type": "text"
      },
      "source": [
        "# Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tQRlWI7UM4ah"
      },
      "source": [
        "## Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "djODVPGjM4ao"
      },
      "source": [
        "Define the following terms in your own words, do not simply copy and paste a definition found elsewhere but reword it to be understandable and memorable to you. *Double click the markdown to add your definitions.*\n",
        "<br/><br/>\n",
        "\n",
        "- **Vectorization**: Change text (words) into numerical data to be able to pass into a model to make predictions.\n",
        "\n",
        "- **Document Term Matrix (DTM)**: `Your Answer Here`\n",
        "\n",
        "- **Latent Semantic Analysis**: `Your Answer Here`\n",
        "\n",
        "- **Term Frequency - Inverse Document Frequency (TF-IDF)**: `Your Answer Here`\n",
        "\n",
        "- **Word Embedding**: `Your Answer Here`\n",
        "\n",
        "- **N-Gram**: `Your Answer Here`\n",
        "\n",
        "- **Skip-Gram**: `Your Answer Here`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lOsi6xE4M-cS"
      },
      "source": [
        "## Questions of Understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3_Atsw1bM-cY"
      },
      "source": [
        "1. Why do we need to vectorize our documents?\n",
        "    ```\n",
        "    Need to change words to numerical data in order to be able to pass them into models.\n",
        "    ```\n",
        "\n",
        "2. How is TF-IDF different from simple word frequency? Why do we use TF-IDF over word frequency?\n",
        "    ```\n",
        "    Your Answer Here\n",
        "    ```\n",
        "\n",
        "3. Why might we choose a word embedding approach over a bag-of-words approach when it comes to vectorization?\n",
        "    ```\n",
        "    Your Answer Here\n",
        "    ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SogHDgfhMTsc",
        "colab_type": "text"
      },
      "source": [
        "## Practice Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7QrjSwIMYzB",
        "colab_type": "text"
      },
      "source": [
        "Use the dataframe `df` above to complete the following."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BTQbHxIMeQN",
        "colab_type": "text"
      },
      "source": [
        "Vectorize the `Tokens` column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ka0AywjNMBMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B26eq5wKMrF4",
        "colab_type": "text"
      },
      "source": [
        "Build a Nearest Neighbors model from your dataframe and then find the 5 nearest neighbors to the strain \"100-OG\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcwURJatMp7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvGLfBxDW6D7",
        "colab_type": "text"
      },
      "source": [
        "You will be putting together a classification model below, but before you do you'll need a baseline. Run the line of code below and then find the normalized value counts for the `Rating` column in `df`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zsEPQgRZKmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Rating'] = df['Rating'].round().astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCPof-7VZOMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hboaEX03Z_w5",
        "colab_type": "text"
      },
      "source": [
        "What is the baseline accuracy?\n",
        "```\n",
        "Your Answer Here\n",
        "```\n",
        "\n",
        "Visualize the rating counts from above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PGmJSMqZxo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwPg1cpShKNA",
        "colab_type": "text"
      },
      "source": [
        "Use your vectorized tokens in the `df` dataframe to train a classification model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awu-ujvvhips",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGhSLJ5Fhlg9",
        "colab_type": "text"
      },
      "source": [
        "Predict the score of the fake strain description below.\n",
        "\n",
        "```\n",
        "'Afgooey, also known as Afgoo, is a potent indica strain that is believed to descend from an Afghani indica and Maui Haze. \n",
        "Its sativa parent may lend Afgoo some uplifting, creative qualities, but this strain undoubtedly takes after its indica \n",
        "parent as it primarily delivers relaxing, sleepy effects alongside its earthy pine flavor. Growers hoping to cultivate Afgoo \n",
        "may have a better chance of success indoors, but this indica can also thrive in Mediterranean climates outdoors.'\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAHaMGjBiG-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGnLTUL8ik4V",
        "colab_type": "text"
      },
      "source": [
        "# Topic Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rfXxSZSDk-Sh"
      },
      "source": [
        "## Questions of Understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hlcEfmnyk-St"
      },
      "source": [
        "1. What is Latent Dirichlet Allocation? What is another name for LDA in NLP?\n",
        "    ```\n",
        "    LDA is a \"generative probabilistic model\" of a collection of documents (composites) made up of words and/or phrases (parts). Another name for LDA in NLP is \"Topic Modeling\"\n",
        "    ```\n",
        "\n",
        "2. How do interpret the results of a topic modeling output?\n",
        "    ```\n",
        "    The imput to a LDA is a DTM. The output will return 2 matrices, one will be a topic term matrix, and the other will be a document topic matrix. The topic term matrix will show you the probability of a word/term being in a topic. The document topic matrix will show the percentage that a document contains words from each topic.\n",
        "    ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lAf8cmNFl_n5"
      },
      "source": [
        "## Practice Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIeP8NyHmAU8",
        "colab_type": "text"
      },
      "source": [
        "Find the top 5 topics of the `Description` column using LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-8zDKA_mAba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADcqbM9FmiVg",
        "colab_type": "text"
      },
      "source": [
        "In a short paragraph, explain how to interpret the first topic your model came up with. If your topic words are difficult to interpret, explain how you could clean up the descriptions to improve your topics\n",
        "\n",
        "```\n",
        "Your Answer Here\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suchG0sEm8lU",
        "colab_type": "text"
      },
      "source": [
        "Use `pyLDAvis` to create a visualization to help you interpret your topic modeling results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f5LbisKnRPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HafoLqwHnR5M",
        "colab_type": "text"
      },
      "source": [
        "Explain how to interpret the results of `pyLDAvis`\n",
        "\n",
        "```\n",
        "Your Answer Here\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANxVUGU2nYsB",
        "colab_type": "text"
      },
      "source": [
        "Create at least 1 more visualization to help you interpret the results of your topic modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEsF_ZMIm7mC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}